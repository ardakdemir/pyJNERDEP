{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DLA_Submission_Notebook",
      "provenance": [],
      "collapsed_sections": [
        "KNfInACJnmeb"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ferqbuAljMJd"
      },
      "source": [
        "# NOTEBOOK For Running the ML Models\n",
        "\n",
        "**Subword Contextual Embeddings for Languages with Rich Morphology**\n",
        "\n",
        "Arda Akdemir, Tetsuo Shibuya, Tunga Gungor\n",
        "\n",
        "\n",
        "\n",
        "This notebok is prepared to make it easy to re-run the experiments and train new models related to the paper titled, \"Subword Contextual Embeddings for Languages with Rich Morphology\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXlQqbkzPPlz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dadc3350-1577-4a00-9b4e-4da344c68eb3"
      },
      "source": [
        "## Mount drive to run the source code on Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEpaMS2OQNF6",
        "outputId": "35ccf6aa-e2f1-4973-b63a-56c462afafd3"
      },
      "source": [
        "## Move cwd to source code\n",
        "path_to_source_code = \"drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP\"\n",
        "%cd $path_to_source_code"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yv5nxi1o3_-D",
        "outputId": "b1e75c87-9c16-4e2b-b9e7-7d23d3d030a2"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/dlaPaper_sourceCode/pyJNERDEP\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2qVg1jVi1_t"
      },
      "source": [
        "***Datasets*** All datasets are available under:\n",
        "\n",
        "https://drive.google.com/drive/folders/1AHVHB1t0_9-0oMjSMTnnMiOjywr7bavh?usp=sharing\n",
        "\n",
        "\n",
        "By default all datasets are expected to be under \"../../datasets\" folder, relative to the source code. \n",
        "Please place the dataset accordingly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EI6bi3fLRHiI",
        "outputId": "201e8b6d-4dfd-49a4-a5bf-6e6fc19488c2"
      },
      "source": [
        "# Install requirements\n",
        "req_path=\"/content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt\"\n",
        "!pip3 install -r $req_path;"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: absl-py==0.11.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 1)) (0.11.0)\n",
            "Requirement already satisfied: appnope==0.1.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 2)) (0.1.0)\n",
            "Requirement already satisfied: asn1crypto==1.0.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 3)) (1.0.1)\n",
            "Requirement already satisfied: astor==0.8.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 4)) (0.8.1)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 5)) (1.6.3)\n",
            "Requirement already satisfied: attrs==19.1.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 6)) (19.1.0)\n",
            "Requirement already satisfied: backcall==0.1.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 7)) (0.1.0)\n",
            "Requirement already satisfied: beautifulsoup4==4.8.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 8)) (4.8.0)\n",
            "Requirement already satisfied: bleach==3.1.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 9)) (3.1.0)\n",
            "Requirement already satisfied: boto3==1.9.249 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 10)) (1.9.249)\n",
            "Requirement already satisfied: botocore==1.12.249 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 11)) (1.12.249)\n",
            "Requirement already satisfied: cachetools==4.0.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 12)) (4.0.0)\n",
            "Requirement already satisfied: certifi==2019.9.11 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 13)) (2019.9.11)\n",
            "Requirement already satisfied: cffi==1.12.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 14)) (1.12.3)\n",
            "Requirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 15)) (3.0.4)\n",
            "Requirement already satisfied: chart-studio==1.0.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 16)) (1.0.0)\n",
            "Requirement already satisfied: Click==7.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 17)) (7.0)\n",
            "Requirement already satisfied: cloudpickle==1.2.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 18)) (1.2.2)\n",
            "Requirement already satisfied: cryptography==2.7 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 19)) (2.7)\n",
            "Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 20)) (0.10.0)\n",
            "Requirement already satisfied: dask==2.3.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 21)) (2.3.0)\n",
            "Requirement already satisfied: entrypoints==0.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 22)) (0.3)\n",
            "Requirement already satisfied: fasttext==0.9.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 23)) (0.9.2)\n",
            "Requirement already satisfied: gensim==3.8.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 24)) (3.8.3)\n",
            "Requirement already satisfied: h5py==2.10.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 25)) (2.10.0)\n",
            "Requirement already satisfied: joblib==0.13.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 26)) (0.13.2)\n",
            "Requirement already satisfied: lightgbm==2.2.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 27)) (2.2.1)\n",
            "Requirement already satisfied: Markdown==3.1.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 28)) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe==1.1.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 29)) (1.1.1)\n",
            "Requirement already satisfied: matplotlib==3.1.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 30)) (3.1.1)\n",
            "Requirement already satisfied: mistune==0.8.4 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 31)) (0.8.4)\n",
            "Requirement already satisfied: networkx==2.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 32)) (2.3)\n",
            "Requirement already satisfied: numpy==1.19.5 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 33)) (1.19.5)\n",
            "Requirement already satisfied: oauthlib==3.1.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 34)) (3.1.0)\n",
            "Requirement already satisfied: olefile==0.46 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 35)) (0.46)\n",
            "Requirement already satisfied: opt-einsum==3.3.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 36)) (3.3.0)\n",
            "Requirement already satisfied: packaging==20.8 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 37)) (20.8)\n",
            "Requirement already satisfied: parso==0.5.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 38)) (0.5.1)\n",
            "Requirement already satisfied: pexpect==4.7.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 39)) (4.7.0)\n",
            "Requirement already satisfied: pickleshare==0.7.5 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 40)) (0.7.5)\n",
            "Requirement already satisfied: pillow==6.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 41)) (6.1.0)\n",
            "Requirement already satisfied: plotly==4.1.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 42)) (4.1.1)\n",
            "Requirement already satisfied: psutil==5.6.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 43)) (5.6.3)\n",
            "Requirement already satisfied: ptyprocess==0.6.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 44)) (0.6.0)\n",
            "Requirement already satisfied: scikit-image==0.15.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 45)) (0.15.0)\n",
            "Requirement already satisfied: scikit-learn==0.21.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 46)) (0.21.2)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 47)) (1.4.1)\n",
            "Requirement already satisfied: seaborn==0.9.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 48)) (0.9.0)\n",
            "Requirement already satisfied: Send2Trash==1.5.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 49)) (1.5.0)\n",
            "Requirement already satisfied: sentencepiece==0.1.83 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 50)) (0.1.83)\n",
            "Requirement already satisfied: seqeval==0.0.12 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 51)) (0.0.12)\n",
            "Requirement already satisfied: six==1.15.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 52)) (1.15.0)\n",
            "Requirement already satisfied: tensorboard==2.4.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 53)) (2.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit==1.7.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 54)) (1.7.0)\n",
            "Requirement already satisfied: tensorboardX==2.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 55)) (2.0)\n",
            "Requirement already satisfied: tensorflow==2.1.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 56)) (2.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator==2.4.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 57)) (2.4.0)\n",
            "Requirement already satisfied: tensorflow-gpu==2.4.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 58)) (2.4.0)\n",
            "Requirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 59)) (1.1.0)\n",
            "Requirement already satisfied: terminado==0.8.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 60)) (0.8.2)\n",
            "Requirement already satisfied: testpath==0.4.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 61)) (0.4.2)\n",
            "Requirement already satisfied: tokenization==1.0.7 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 62)) (1.0.7)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 63)) (0.9.4)\n",
            "Requirement already satisfied: toolz==0.10.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 64)) (0.10.0)\n",
            "Requirement already satisfied: torch==1.2.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 65)) (1.2.0)\n",
            "Requirement already satisfied: torchvision==0.4.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 66)) (0.4.0)\n",
            "Requirement already satisfied: tqdm==4.36.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 67)) (4.36.1)\n",
            "Requirement already satisfied: traitlets==4.3.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 68)) (4.3.2)\n",
            "Requirement already satisfied: transformers==4.1.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 69)) (4.1.1)\n",
            "Requirement already satisfied: typing-extensions==3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 70)) (3.7.4.3)\n",
            "Requirement already satisfied: Unidecode==1.1.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 71)) (1.1.1)\n",
            "Requirement already satisfied: update==0.0.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 72)) (0.0.1)\n",
            "Requirement already satisfied: urllib3==1.25.6 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 73)) (1.25.6)\n",
            "Requirement already satisfied: wcwidth==0.1.7 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 74)) (0.1.7)\n",
            "Requirement already satisfied: webencodings==0.5.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 75)) (0.5.1)\n",
            "Requirement already satisfied: Werkzeug==0.16.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 76)) (0.16.1)\n",
            "Requirement already satisfied: widgetsnbextension==3.5.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 77)) (3.5.1)\n",
            "Requirement already satisfied: wrapt==1.12.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 78)) (1.12.1)\n",
            "Requirement already satisfied: zipdir in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 79)) (0.1.2)\n",
            "Requirement already satisfied: fugashi in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 80)) (1.1.0)\n",
            "Requirement already satisfied: ipadic in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 81)) (1.0.0)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 82)) (3.6.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse==1.6.3->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 5)) (0.36.2)\n",
            "Requirement already satisfied: soupsieve>=1.2 in /usr/local/lib/python3.7/dist-packages (from beautifulsoup4==4.8.0->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 8)) (2.2.1)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from boto3==1.9.249->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 10)) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3==1.9.249->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 10)) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.7/dist-packages (from botocore==1.12.249->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 11)) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.7/dist-packages (from botocore==1.12.249->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 11)) (0.15.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi==1.12.3->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 14)) (2.20)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from chart-studio==1.0.0->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 16)) (1.3.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from chart-studio==1.0.0->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 16)) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 23)) (54.2.0)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 23)) (2.6.2)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.3->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 24)) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.1->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 30)) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.1->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 30)) (1.3.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx==2.3->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 32)) (4.4.2)\n",
            "Requirement already satisfied: imageio>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.15.0->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 45)) (2.4.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.15.0->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 45)) (1.1.1)\n",
            "Requirement already satisfied: pandas>=0.15.2 in /usr/local/lib/python3.7/dist-packages (from seaborn==0.9.0->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 48)) (1.1.5)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.7/dist-packages (from seqeval==0.0.12->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 51)) (2.4.3)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.4.0->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 53)) (3.12.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.4.0->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 53)) (1.28.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.4.0->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 53)) (1.32.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.4.0->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 53)) (0.4.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 56)) (0.2.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 56)) (1.0.8)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 56)) (0.2.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 56)) (1.1.2)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.4.0->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 58)) (1.12)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.7/dist-packages (from terminado==0.8.2->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 60)) (5.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from tokenization==1.0.7->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 62)) (2019.12.20)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets==4.3.2->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 68)) (0.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.1.1->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 69)) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.1.1->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 69)) (0.0.43)\n",
            "Requirement already satisfied: style==1.1.0 in /usr/local/lib/python3.7/dist-packages (from update==0.0.1->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 72)) (1.1.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension==3.5.1->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 77)) (5.3.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->chart-studio==1.0.0->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 16)) (2.10)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.15.2->seaborn==0.9.0->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 48)) (2018.9)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from Keras>=2.2.4->seqeval==0.0.12->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 51)) (3.13)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.4.0->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 53)) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.4.0->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 53)) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.4.0->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 53)) (1.3.0)\n",
            "Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension==3.5.1->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 77)) (4.7.1)\n",
            "Requirement already satisfied: jupyter-client>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension==3.5.1->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 77)) (5.3.5)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension==3.5.1->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 77)) (5.1.2)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension==3.5.1->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 77)) (4.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension==3.5.1->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 77)) (2.11.3)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension==3.5.1->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 77)) (5.6.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard==2.4.0->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 53)) (0.4.8)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=5.2.0->notebook>=4.4.1->widgetsnbextension==3.5.1->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 77)) (22.0.3)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension==3.5.1->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 77)) (2.6.0)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->notebook>=4.4.1->widgetsnbextension==3.5.1->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 77)) (5.5.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension==3.5.1->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 77)) (2.6.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension==3.5.1->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 77)) (1.4.3)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension==3.5.1->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 77)) (0.7.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook>=4.4.1->widgetsnbextension==3.5.1->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 77)) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook>=4.4.1->widgetsnbextension==3.5.1->-r /content/drive/MyDrive/dlaPaper_sourceCode/pyJNERDEP/docker_req.txt (line 77)) (1.0.18)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNfInACJnmeb"
      },
      "source": [
        "## Download Data Dependencies\n",
        "\n",
        "Below are the code scripts for downloading the pretrained models to replicate the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABiObpcXuYLP"
      },
      "source": [
        "import subprocess\n",
        "import sys\n",
        "import gdown\n",
        "import zipdir\n",
        "from zipfile import ZipFile\n",
        "import os\n",
        "\n",
        "id_model_map = {\"SA\": {\"twitter_turkish\": \"10mOwZGp4-NTo9K_bJkE2KlWa4HUudW07\",\n",
        "                       \"movie_turkish\": \"1IoQhYijlWVlK0vHnVUn0e1ohwahePO7R\",\n",
        "                       \"movie_english\": \"1t2XgkbfxGPjvThEg-wkO_ejTOlikIzv7\"},\n",
        "                \"NER\": {\"bert\": \"1M9-JWPL535IIDUSoDNDpMRzTex8tBbN7\",\n",
        "                        \"mbert\": \"1GCcU5hP86CDnH3Jhb8MBE9bHW9zl3vbb\",\n",
        "                        \"bert_en\": \"1KswPuDKxWOl-3qBQ_BESwBpd0vMzN0zn\",\n",
        "                        \"fastext\": \"1JWHSHDmTxsZoYwkc6_K8Wz76JSUmJVhA\",\n",
        "                        \"random_init\": \"117tWTA18lC6iOd31Maypg8jpNMQhPptf\",\n",
        "                        \"word2vec\": \"1E5jGGlhbevjSg-oprf_e0vU2y89_zhHJ\"\n",
        "                        },\n",
        "                \"DEP\": {\"bert\": \"1qdWkuwPkyKMBKKHMgTB2sAw_SmdqJ99m\",\n",
        "                        \"mbert\": \"1p4VJg_hQhC1yIiabJbPOuuDwj8jfeOlP\",\n",
        "                        \"bert_en\": \"1NLxgynxoDVb0QoN2NB8z9bf6OrfVNUU7\",\n",
        "                        \"fastext\": \"1reGe2vMsGU-xbXV6dDACzvn8ViRhJKof\",\n",
        "                        \"random_init\": \"1_AqzeDVlsTUlnhduQtU7tE0VyaZTIADj\",\n",
        "                        \"word2vec\": \"1gF7ujIvRdmwvIKIdGw9bKBoKgstX1Mws\"\n",
        "                        }\n",
        "                }\n",
        "\n",
        "names = {\"SA\": \"Sentiment Analysis\",\n",
        "         \"NER\": \"Named Entity Recognition\",\n",
        "         \"DEP\": \"Dependency Parsing\",\n",
        "         \"FLAT_DEP\": \"Multi-task Learning DEP\",\n",
        "         \"FLAT_NER\": \"Multi-task Learning NER\"}\n",
        "\n",
        "word2vec_driveIds = {\"jp\": \"1dYISBXsgK3yR6mw-LRGfjGrcN3aVme2q\",\n",
        "                     \"tr\": \"14WH-amhKXn4ayqi2lugUSIoS7b8q0U9H\",\n",
        "                     \"hu\": \"1dmEC0-7Zkmc4p9OmTIw3JKMJ7aNyGYIt\",\n",
        "                     \"en\": \"1avdWgjq138lrfJnIZVRpa9EaLJrU4HVj\",\n",
        "                     \"fi\": \"1wtqAc4FZ6wl4w4_kSozWbDjgUUV22Fjr\",\n",
        "                     \"cs\": \"1ibFwJ6B01Kpm6k6qdI1cJ64wLyaJy-s3\"}\n",
        "\n",
        "word2vec_dict = {\"jp\": \"../word_vecs/jp/jp.bin\",\n",
        "                 \"tr\": \"../word_vecs/tr/tr.bin\",\n",
        "                 \"hu\": \"../word_vecs/hu/hu.bin\",\n",
        "                 \"en\": \"../word_vecs/en/en.txt\",\n",
        "                 \"fi\": \"../word_vecs/fi/fi.bin\",\n",
        "                 \"cs\": \"../word_vecs/cs/cs.txt\"}\n",
        "                 \n",
        "def drive_download_w2v(lang, save_path):\n",
        "    print(\"\\nDownloading word2Vec model for {} to {}\".format(lang,save_path))\n",
        "    id = word2vec_driveIds[lang]\n",
        "    link = download_link_generator(id)\n",
        "    gdown.download(link, save_path, quiet=False)\n",
        "    return save_path\n",
        "\n",
        "def unzip(src, dest):\n",
        "    with ZipFile(src, 'r') as zipObj:\n",
        "        # Extract all the contents of zip file in different directory\n",
        "        zipObj.extractall(dest)\n",
        "        print('File is unzipped in {} folder'.format(dest))\n",
        "\n",
        "\n",
        "def download_link_generator(id):\n",
        "    return \"https://drive.google.com/uc?id={}\".format(id)\n",
        "\n",
        "\n",
        "def load_download_models(model_type, word_type, save_folder=None):\n",
        "    id = id_model_map[model_type][word_type]\n",
        "    print(\"\\n===Downloading trained {} models to replicate the result===\\n\".format(names[model_type]))\n",
        "    link = download_link_generator(id)\n",
        "    dest = \"../{}_{}_models.zip\".format(model_type, word_type)\n",
        "    unzip_path = \"../{}\".format(os.path.split(dest)[-1].split(\".\")[0]) if not save_folder else save_folder\n",
        "    if not os.path.exists(unzip_path):\n",
        "        print(\"{} not found. Downloading trained models for {} {}\".format(unzip_path, model_type, word_type))\n",
        "        gdown.download(link, dest, quiet=False)\n",
        "        unzip(dest, unzip_path)\n",
        "        print(\"Trained models are stored in {}\".format(unzip_path))\n",
        "        return unzip_path\n",
        "    else:\n",
        "        print(\"Models for {} {} are already downloaded.\".format(model_type, word_type))\n",
        "        return unzip_path\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siQz5ToxiIsq"
      },
      "source": [
        "# Run Sentiment Analysis Experiments\n",
        "\n",
        "Below bash script allows getting results using a single CLI call.\n",
        "Steps: \n",
        "\n",
        "- Download the trained models\n",
        "- Run for each language/model combination\n",
        "- Store results in denoted folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYHbQ1NoL_Dk"
      },
      "source": [
        "save_dir = \"../sa_experiment_results_3103\" # folder to store the SA experiment results"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uumEWcr1oDWd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58bca0d7-3cbe-4dfb-d098-130362c071bf"
      },
      "source": [
        "import os\n",
        "for comb in [[\"movie\",\"en\",\"english\"],[\"movie\",\"tr\",\"turkish\"],[\"twitter\",\"tr\",\"turkish\"]]:\n",
        "  for word_type in [\"bert\",\"mbert\",\"bert_en\", \"fastext\", \"word2vec\", \"random_init\"]:\n",
        "    d,l,lang = comb\n",
        "    model_folder = os.path.join(\"../\",\"SA_{}_{}_models\".format(lang,d))\n",
        "    !bash \"experiment_scripts/run_sa_models.sh\" $l $lang $d $word_type $model_folder  $save_dir"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading trained Sentiment Analysis models to: ../SA_english_movie_models\n",
            "\n",
            "===Downloading trained Sentiment Analysis models to replicate the result===\n",
            "\n",
            "Models for SA movie_english are already downloaded.\n",
            "Content of the save model folder: ['movie_en_bert_best_sa_model_weights.pkh', 'movie_en_mbert_best_sa_model_weights.pkh', 'movie_en_bert_en_best_sa_model_weights.pkh', 'movie_en_word2vec_best_sa_model_weights.pkh', 'movie_en_fastext_best_sa_model_weights.pkh', 'movie_en_random_init_best_sa_model_weights.pkh']\n",
            "Running for  sa  english  bert\n",
            "2021-03-31 13:39:26.293838: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Downloading trained Sentiment Analysis models to: ../SA_english_movie_models\n",
            "\n",
            "===Downloading trained Sentiment Analysis models to replicate the result===\n",
            "\n",
            "Models for SA movie_english are already downloaded.\n",
            "Content of the save model folder: ['movie_en_bert_best_sa_model_weights.pkh', 'movie_en_mbert_best_sa_model_weights.pkh', 'movie_en_bert_en_best_sa_model_weights.pkh', 'movie_en_word2vec_best_sa_model_weights.pkh', 'movie_en_fastext_best_sa_model_weights.pkh', 'movie_en_random_init_best_sa_model_weights.pkh']\n",
            "Running for  sa  english  mbert\n",
            "2021-03-31 13:39:30.488960: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Using trained models to predict...\n",
            "Running for  en mbert \n",
            "Initializing tokenizer with bert-base-multilingual-cased for en\n",
            "{'train': '../../datasets/sa_movie_english-train.json', 'dev': '../../datasets/sa_movie_english-dev.json', 'test': '../../datasets/sa_movie_english-test.json'}\n",
            "Reading from ../../datasets/sa_movie_english-train.json \n",
            "Dataset size : 8529 \n",
            "Grouping with batch size: 200\n",
            "Reading from ../../datasets/sa_movie_english-dev.json \n",
            "Dataset size : 1067 \n",
            "Grouping with batch size: 200\n",
            "Reading from ../../datasets/sa_movie_english-test.json \n",
            "Dataset size : 1065 \n",
            "Grouping with batch size: 200\n",
            "Training vocab: {'p': 0, 'n': 1}\n",
            "train number of batches: 871\n",
            "dev number of batches: 112\n",
            "test number of batches: 112\n",
            "Initializing mbert model\n",
            "200 parameter groups will be loaded in total\n",
            "Evaluation: 100% 112/112 [00:02<00:00, 39.17it/s]\n",
            "\n",
            "\n",
            "===Sentiment Analysis Test results for en mbert movie=== \n",
            " Acc:\t0.8037558685446009\n",
            "F1\t0.8155339805825245\n",
            "Loss\t71.50816587917507\n",
            "\n",
            "\n",
            "\n",
            " Storing experiment log to ../sa_experiment_results_3103/sa_experiment_log_movie_en_mbert.json...\n",
            "Downloading trained Sentiment Analysis models to: ../SA_english_movie_models\n",
            "\n",
            "===Downloading trained Sentiment Analysis models to replicate the result===\n",
            "\n",
            "Models for SA movie_english are already downloaded.\n",
            "Content of the save model folder: ['movie_en_bert_best_sa_model_weights.pkh', 'movie_en_mbert_best_sa_model_weights.pkh', 'movie_en_bert_en_best_sa_model_weights.pkh', 'movie_en_word2vec_best_sa_model_weights.pkh', 'movie_en_fastext_best_sa_model_weights.pkh', 'movie_en_random_init_best_sa_model_weights.pkh']\n",
            "Running for  sa  english  bert_en\n",
            "2021-03-31 13:39:59.968406: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Using trained models to predict...\n",
            "Running for  en bert_en \n",
            "Initializing tokenizer with bert-base-cased for en\n",
            "{'train': '../../datasets/sa_movie_english-train.json', 'dev': '../../datasets/sa_movie_english-dev.json', 'test': '../../datasets/sa_movie_english-test.json'}\n",
            "Reading from ../../datasets/sa_movie_english-train.json \n",
            "Dataset size : 8529 \n",
            "Grouping with batch size: 200\n",
            "Reading from ../../datasets/sa_movie_english-dev.json \n",
            "Dataset size : 1067 \n",
            "Grouping with batch size: 200\n",
            "Reading from ../../datasets/sa_movie_english-test.json \n",
            "Dataset size : 1065 \n",
            "Grouping with batch size: 200\n",
            "Training vocab: {'p': 0, 'n': 1}\n",
            "train number of batches: 871\n",
            "dev number of batches: 112\n",
            "test number of batches: 112\n",
            "Initializing bert_en model\n",
            "200 parameter groups will be loaded in total\n",
            "Evaluation: 100% 112/112 [00:02<00:00, 39.48it/s]\n",
            "\n",
            "\n",
            "===Sentiment Analysis Test results for en bert_en movie=== \n",
            " Acc:\t0.8328638497652582\n",
            "F1\t0.8454861111111112\n",
            "Loss\t69.6209606109187\n",
            "\n",
            "\n",
            "\n",
            " Storing experiment log to ../sa_experiment_results_3103/sa_experiment_log_movie_en_bert_en.json...\n",
            "Downloading trained Sentiment Analysis models to: ../SA_english_movie_models\n",
            "\n",
            "===Downloading trained Sentiment Analysis models to replicate the result===\n",
            "\n",
            "Models for SA movie_english are already downloaded.\n",
            "Content of the save model folder: ['movie_en_bert_best_sa_model_weights.pkh', 'movie_en_mbert_best_sa_model_weights.pkh', 'movie_en_bert_en_best_sa_model_weights.pkh', 'movie_en_word2vec_best_sa_model_weights.pkh', 'movie_en_fastext_best_sa_model_weights.pkh', 'movie_en_random_init_best_sa_model_weights.pkh']\n",
            "Running for  sa  english  fastext\n",
            "2021-03-31 13:40:13.849626: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Using trained models to predict...\n",
            "Running for  en fastext \n",
            "Initializing tokenizer with bert-base-cased for en\n",
            "{'train': '../../datasets/sa_movie_english-train.json', 'dev': '../../datasets/sa_movie_english-dev.json', 'test': '../../datasets/sa_movie_english-test.json'}\n",
            "Reading from ../../datasets/sa_movie_english-train.json \n",
            "Dataset size : 8529 \n",
            "Grouping with batch size: 200\n",
            "Reading from ../../datasets/sa_movie_english-dev.json \n",
            "Dataset size : 1067 \n",
            "Grouping with batch size: 200\n",
            "Reading from ../../datasets/sa_movie_english-test.json \n",
            "Dataset size : 1065 \n",
            "Grouping with batch size: 200\n",
            "Training vocab: {'p': 0, 'n': 1}\n",
            "train number of batches: 871\n",
            "dev number of batches: 112\n",
            "test number of batches: 112\n",
            "27 parameter groups will be loaded in total\n",
            "Evaluation: 100% 112/112 [00:00<00:00, 153.95it/s]\n",
            "\n",
            "\n",
            "===Sentiment Analysis Test results for en fastext movie=== \n",
            " Acc:\t0.7464788732394366\n",
            "F1\t0.763157894736842\n",
            "Loss\t71.8554719351232\n",
            "\n",
            "\n",
            "\n",
            " Storing experiment log to ../sa_experiment_results_3103/sa_experiment_log_movie_en_fastext.json...\n",
            "Downloading trained Sentiment Analysis models to: ../SA_english_movie_models\n",
            "\n",
            "===Downloading trained Sentiment Analysis models to replicate the result===\n",
            "\n",
            "Models for SA movie_english are already downloaded.\n",
            "Content of the save model folder: ['movie_en_bert_best_sa_model_weights.pkh', 'movie_en_mbert_best_sa_model_weights.pkh', 'movie_en_bert_en_best_sa_model_weights.pkh', 'movie_en_word2vec_best_sa_model_weights.pkh', 'movie_en_fastext_best_sa_model_weights.pkh', 'movie_en_random_init_best_sa_model_weights.pkh']\n",
            "Running for  sa  english  word2vec\n",
            "2021-03-31 13:40:22.697899: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Using trained models to predict...\n",
            "Running for  en word2vec \n",
            "Initializing tokenizer with bert-base-cased for en\n",
            "{'train': '../../datasets/sa_movie_english-train.json', 'dev': '../../datasets/sa_movie_english-dev.json', 'test': '../../datasets/sa_movie_english-test.json'}\n",
            "Reading from ../../datasets/sa_movie_english-train.json \n",
            "Dataset size : 8529 \n",
            "Grouping with batch size: 200\n",
            "Reading from ../../datasets/sa_movie_english-dev.json \n",
            "Dataset size : 1067 \n",
            "Grouping with batch size: 200\n",
            "Reading from ../../datasets/sa_movie_english-test.json \n",
            "Dataset size : 1065 \n",
            "Grouping with batch size: 200\n",
            "Training vocab: {'p': 0, 'n': 1}\n",
            "train number of batches: 871\n",
            "dev number of batches: 112\n",
            "test number of batches: 112\n",
            "27 parameter groups will be loaded in total\n",
            "Evaluation: 100% 112/112 [00:00<00:00, 150.30it/s]\n",
            "\n",
            "\n",
            "===Sentiment Analysis Test results for en word2vec movie=== \n",
            " Acc:\t0.7605633802816901\n",
            "F1\t0.7741364038972542\n",
            "Loss\t102.95425589755177\n",
            "\n",
            "\n",
            "\n",
            " Storing experiment log to ../sa_experiment_results_3103/sa_experiment_log_movie_en_word2vec.json...\n",
            "Downloading trained Sentiment Analysis models to: ../SA_english_movie_models\n",
            "\n",
            "===Downloading trained Sentiment Analysis models to replicate the result===\n",
            "\n",
            "Models for SA movie_english are already downloaded.\n",
            "Content of the save model folder: ['movie_en_bert_best_sa_model_weights.pkh', 'movie_en_mbert_best_sa_model_weights.pkh', 'movie_en_bert_en_best_sa_model_weights.pkh', 'movie_en_word2vec_best_sa_model_weights.pkh', 'movie_en_fastext_best_sa_model_weights.pkh', 'movie_en_random_init_best_sa_model_weights.pkh']\n",
            "Running for  sa  english  random_init\n",
            "2021-03-31 13:40:31.347029: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Using trained models to predict...\n",
            "Running for  en random_init \n",
            "Initializing tokenizer with bert-base-cased for en\n",
            "{'train': '../../datasets/sa_movie_english-train.json', 'dev': '../../datasets/sa_movie_english-dev.json', 'test': '../../datasets/sa_movie_english-test.json'}\n",
            "Reading from ../../datasets/sa_movie_english-train.json \n",
            "Dataset size : 8529 \n",
            "Grouping with batch size: 200\n",
            "Reading from ../../datasets/sa_movie_english-dev.json \n",
            "Dataset size : 1067 \n",
            "Grouping with batch size: 200\n",
            "Reading from ../../datasets/sa_movie_english-test.json \n",
            "Dataset size : 1065 \n",
            "Grouping with batch size: 200\n",
            "Training vocab: {'p': 0, 'n': 1}\n",
            "train number of batches: 871\n",
            "dev number of batches: 112\n",
            "test number of batches: 112\n",
            "27 parameter groups will be loaded in total\n",
            "Evaluation: 100% 112/112 [00:00<00:00, 148.76it/s]\n",
            "\n",
            "\n",
            "===Sentiment Analysis Test results for en random_init movie=== \n",
            " Acc:\t0.7446009389671362\n",
            "F1\t0.7675213675213675\n",
            "Loss\t75.9813923239708\n",
            "\n",
            "\n",
            "\n",
            " Storing experiment log to ../sa_experiment_results_3103/sa_experiment_log_movie_en_random_init.json...\n",
            "Downloading trained Sentiment Analysis models to: ../SA_turkish_movie_models\n",
            "\n",
            "===Downloading trained Sentiment Analysis models to replicate the result===\n",
            "\n",
            "Models for SA movie_turkish are already downloaded.\n",
            "Content of the save model folder: ['movie_tr_bert_en_best_sa_model_weights.pkh', 'movie_tr_word2vec_best_sa_model_weights.pkh', 'movie_tr_mbert_best_sa_model_weights.pkh', 'movie_tr_fastext_best_sa_model_weights.pkh', 'movie_tr_random_init_best_sa_model_weights.pkh', 'movie_tr_bert_best_sa_model_weights.pkh']\n",
            "Running for  sa  turkish  bert\n",
            "2021-03-31 13:40:39.964914: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Using trained models to predict...\n",
            "Running for  tr bert \n",
            "Initializing tokenizer with dbmdz/bert-base-turkish-cased for tr\n",
            "{'train': '../../datasets/sa_movie_turkish-train.json', 'dev': '../../datasets/sa_movie_turkish-dev.json', 'test': '../../datasets/sa_movie_turkish-test.json'}\n",
            "Reading from ../../datasets/sa_movie_turkish-train.json \n",
            "Dataset size : 16196 \n",
            "Grouping with batch size: 200\n",
            "Reading from ../../datasets/sa_movie_turkish-dev.json \n",
            "Dataset size : 2025 \n",
            "Grouping with batch size: 200\n",
            "Reading from ../../datasets/sa_movie_turkish-test.json \n",
            "Dataset size : 2023 \n",
            "Grouping with batch size: 200\n",
            "Training vocab: {'p': 0, 'n': 1}\n",
            "train number of batches: 2341\n",
            "dev number of batches: 294\n",
            "test number of batches: 302\n",
            "Initializing lang specific bert model\n",
            "200 parameter groups will be loaded in total\n",
            "Evaluation:   0% 0/302 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (637 > 512). Running this sequence through the model will result in indexing errors\n",
            "Evaluation: 100% 302/302 [00:10<00:00, 27.56it/s]\n",
            "\n",
            "\n",
            "===Sentiment Analysis Test results for tr bert movie=== \n",
            " Acc:\t0.9303015323776569\n",
            "F1\t0.8950111690245718\n",
            "Loss\t104.37743412226507\n",
            "\n",
            "\n",
            "\n",
            " Storing experiment log to ../sa_experiment_results_3103/sa_experiment_log_movie_tr_bert.json...\n",
            "Downloading trained Sentiment Analysis models to: ../SA_turkish_movie_models\n",
            "\n",
            "===Downloading trained Sentiment Analysis models to replicate the result===\n",
            "\n",
            "Models for SA movie_turkish are already downloaded.\n",
            "Content of the save model folder: ['movie_tr_bert_en_best_sa_model_weights.pkh', 'movie_tr_word2vec_best_sa_model_weights.pkh', 'movie_tr_mbert_best_sa_model_weights.pkh', 'movie_tr_fastext_best_sa_model_weights.pkh', 'movie_tr_random_init_best_sa_model_weights.pkh', 'movie_tr_bert_best_sa_model_weights.pkh']\n",
            "Running for  sa  turkish  mbert\n",
            "2021-03-31 13:41:02.734629: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Using trained models to predict...\n",
            "Running for  tr mbert \n",
            "Initializing tokenizer with bert-base-multilingual-cased for tr\n",
            "{'train': '../../datasets/sa_movie_turkish-train.json', 'dev': '../../datasets/sa_movie_turkish-dev.json', 'test': '../../datasets/sa_movie_turkish-test.json'}\n",
            "Reading from ../../datasets/sa_movie_turkish-train.json \n",
            "Dataset size : 16196 \n",
            "Grouping with batch size: 200\n",
            "Reading from ../../datasets/sa_movie_turkish-dev.json \n",
            "Dataset size : 2025 \n",
            "Grouping with batch size: 200\n",
            "Reading from ../../datasets/sa_movie_turkish-test.json \n",
            "Dataset size : 2023 \n",
            "Grouping with batch size: 200\n",
            "Training vocab: {'p': 0, 'n': 1}\n",
            "train number of batches: 2341\n",
            "dev number of batches: 294\n",
            "test number of batches: 302\n",
            "Initializing mbert model\n",
            "200 parameter groups will be loaded in total\n",
            "Evaluation:   0% 0/302 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1017 > 512). Running this sequence through the model will result in indexing errors\n",
            "Evaluation: 100% 302/302 [00:14<00:00, 20.42it/s]\n",
            "\n",
            "\n",
            "===Sentiment Analysis Test results for tr mbert movie=== \n",
            " Acc:\t0.819080573405833\n",
            "F1\t0.7404255319148937\n",
            "Loss\t126.27808630745858\n",
            "\n",
            "\n",
            "\n",
            " Storing experiment log to ../sa_experiment_results_3103/sa_experiment_log_movie_tr_mbert.json...\n",
            "Downloading trained Sentiment Analysis models to: ../SA_turkish_movie_models\n",
            "\n",
            "===Downloading trained Sentiment Analysis models to replicate the result===\n",
            "\n",
            "Models for SA movie_turkish are already downloaded.\n",
            "Content of the save model folder: ['movie_tr_bert_en_best_sa_model_weights.pkh', 'movie_tr_word2vec_best_sa_model_weights.pkh', 'movie_tr_mbert_best_sa_model_weights.pkh', 'movie_tr_fastext_best_sa_model_weights.pkh', 'movie_tr_random_init_best_sa_model_weights.pkh', 'movie_tr_bert_best_sa_model_weights.pkh']\n",
            "Running for  sa  turkish  bert_en\n",
            "2021-03-31 13:41:46.111135: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Using trained models to predict...\n",
            "Running for  tr bert_en \n",
            "Initializing tokenizer with bert-base-cased for tr\n",
            "{'train': '../../datasets/sa_movie_turkish-train.json', 'dev': '../../datasets/sa_movie_turkish-dev.json', 'test': '../../datasets/sa_movie_turkish-test.json'}\n",
            "Reading from ../../datasets/sa_movie_turkish-train.json \n",
            "Dataset size : 16196 \n",
            "Grouping with batch size: 200\n",
            "Reading from ../../datasets/sa_movie_turkish-dev.json \n",
            "Dataset size : 2025 \n",
            "Grouping with batch size: 200\n",
            "Reading from ../../datasets/sa_movie_turkish-test.json \n",
            "Dataset size : 2023 \n",
            "Grouping with batch size: 200\n",
            "Training vocab: {'p': 0, 'n': 1}\n",
            "train number of batches: 2341\n",
            "dev number of batches: 294\n",
            "test number of batches: 302\n",
            "Initializing bert_en model\n",
            "200 parameter groups will be loaded in total\n",
            "Evaluation:   0% 0/302 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1640 > 512). Running this sequence through the model will result in indexing errors\n",
            "Evaluation: 100% 302/302 [00:21<00:00, 14.20it/s]\n",
            "\n",
            "\n",
            "===Sentiment Analysis Test results for tr bert_en movie=== \n",
            " Acc:\t0.7582797825012358\n",
            "F1\t0.709101725163593\n",
            "Loss\t150.91385757853277\n",
            "\n",
            "\n",
            "\n",
            " Storing experiment log to ../sa_experiment_results_3103/sa_experiment_log_movie_tr_bert_en.json...\n",
            "Downloading trained Sentiment Analysis models to: ../SA_turkish_movie_models\n",
            "\n",
            "===Downloading trained Sentiment Analysis models to replicate the result===\n",
            "\n",
            "Models for SA movie_turkish are already downloaded.\n",
            "Content of the save model folder: ['movie_tr_bert_en_best_sa_model_weights.pkh', 'movie_tr_word2vec_best_sa_model_weights.pkh', 'movie_tr_mbert_best_sa_model_weights.pkh', 'movie_tr_fastext_best_sa_model_weights.pkh', 'movie_tr_random_init_best_sa_model_weights.pkh', 'movie_tr_bert_best_sa_model_weights.pkh']\n",
            "Running for  sa  turkish  fastext\n",
            "2021-03-31 13:42:31.118740: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Using trained models to predict...\n",
            "Running for  tr fastext \n",
            "Initializing tokenizer with dbmdz/bert-base-turkish-cased for tr\n",
            "{'train': '../../datasets/sa_movie_turkish-train.json', 'dev': '../../datasets/sa_movie_turkish-dev.json', 'test': '../../datasets/sa_movie_turkish-test.json'}\n",
            "Reading from ../../datasets/sa_movie_turkish-train.json \n",
            "Dataset size : 16196 \n",
            "Grouping with batch size: 200\n",
            "Reading from ../../datasets/sa_movie_turkish-dev.json \n",
            "Dataset size : 2025 \n",
            "Grouping with batch size: 200\n",
            "Reading from ../../datasets/sa_movie_turkish-test.json \n",
            "Dataset size : 2023 \n",
            "Grouping with batch size: 200\n",
            "Training vocab: {'p': 0, 'n': 1}\n",
            "train number of batches: 2341\n",
            "dev number of batches: 294\n",
            "test number of batches: 302\n",
            "27 parameter groups will be loaded in total\n",
            "Evaluation:   0% 0/302 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (637 > 512). Running this sequence through the model will result in indexing errors\n",
            "Evaluation: 100% 302/302 [00:02<00:00, 109.99it/s]\n",
            "\n",
            "\n",
            "===Sentiment Analysis Test results for tr fastext movie=== \n",
            " Acc:\t0.8645575877409788\n",
            "F1\t0.8051209103840683\n",
            "Loss\t110.59772686382712\n",
            "\n",
            "\n",
            "\n",
            " Storing experiment log to ../sa_experiment_results_3103/sa_experiment_log_movie_tr_fastext.json...\n",
            "Downloading trained Sentiment Analysis models to: ../SA_turkish_movie_models\n",
            "\n",
            "===Downloading trained Sentiment Analysis models to replicate the result===\n",
            "\n",
            "Models for SA movie_turkish are already downloaded.\n",
            "Content of the save model folder: ['movie_tr_bert_en_best_sa_model_weights.pkh', 'movie_tr_word2vec_best_sa_model_weights.pkh', 'movie_tr_mbert_best_sa_model_weights.pkh', 'movie_tr_fastext_best_sa_model_weights.pkh', 'movie_tr_random_init_best_sa_model_weights.pkh', 'movie_tr_bert_best_sa_model_weights.pkh']\n",
            "Running for  sa  turkish  word2vec\n",
            "2021-03-31 13:42:47.157532: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Using trained models to predict...\n",
            "Running for  tr word2vec \n",
            "Initializing tokenizer with dbmdz/bert-base-turkish-cased for tr\n",
            "{'train': '../../datasets/sa_movie_turkish-train.json', 'dev': '../../datasets/sa_movie_turkish-dev.json', 'test': '../../datasets/sa_movie_turkish-test.json'}\n",
            "Reading from ../../datasets/sa_movie_turkish-train.json \n",
            "Dataset size : 16196 \n",
            "Grouping with batch size: 200\n",
            "Reading from ../../datasets/sa_movie_turkish-dev.json \n",
            "Dataset size : 2025 \n",
            "Grouping with batch size: 200\n",
            "Reading from ../../datasets/sa_movie_turkish-test.json \n",
            "Dataset size : 2023 \n",
            "Grouping with batch size: 200\n",
            "Training vocab: {'p': 0, 'n': 1}\n",
            "train number of batches: 2341\n",
            "dev number of batches: 294\n",
            "test number of batches: 302\n",
            "27 parameter groups will be loaded in total\n",
            "Evaluation:   0% 0/302 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (637 > 512). Running this sequence through the model will result in indexing errors\n",
            "Evaluation: 100% 302/302 [00:02<00:00, 108.83it/s]\n",
            "\n",
            "\n",
            "===Sentiment Analysis Test results for tr word2vec movie=== \n",
            " Acc:\t0.8764211566979733\n",
            "F1\t0.8145400593471811\n",
            "Loss\t90.96745012002066\n",
            "\n",
            "\n",
            "\n",
            " Storing experiment log to ../sa_experiment_results_3103/sa_experiment_log_movie_tr_word2vec.json...\n",
            "Downloading trained Sentiment Analysis models to: ../SA_turkish_movie_models\n",
            "\n",
            "===Downloading trained Sentiment Analysis models to replicate the result===\n",
            "\n",
            "Models for SA movie_turkish are already downloaded.\n",
            "Content of the save model folder: ['movie_tr_bert_en_best_sa_model_weights.pkh', 'movie_tr_word2vec_best_sa_model_weights.pkh', 'movie_tr_mbert_best_sa_model_weights.pkh', 'movie_tr_fastext_best_sa_model_weights.pkh', 'movie_tr_random_init_best_sa_model_weights.pkh', 'movie_tr_bert_best_sa_model_weights.pkh']\n",
            "Running for  sa  turkish  random_init\n",
            "2021-03-31 13:43:02.616418: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Using trained models to predict...\n",
            "Running for  tr random_init \n",
            "Initializing tokenizer with dbmdz/bert-base-turkish-cased for tr\n",
            "{'train': '../../datasets/sa_movie_turkish-train.json', 'dev': '../../datasets/sa_movie_turkish-dev.json', 'test': '../../datasets/sa_movie_turkish-test.json'}\n",
            "Reading from ../../datasets/sa_movie_turkish-train.json \n",
            "Dataset size : 16196 \n",
            "Grouping with batch size: 200\n",
            "Reading from ../../datasets/sa_movie_turkish-dev.json \n",
            "Dataset size : 2025 \n",
            "Grouping with batch size: 200\n",
            "Reading from ../../datasets/sa_movie_turkish-test.json \n",
            "Dataset size : 2023 \n",
            "Grouping with batch size: 200\n",
            "Training vocab: {'p': 0, 'n': 1}\n",
            "train number of batches: 2341\n",
            "dev number of batches: 294\n",
            "test number of batches: 302\n",
            "27 parameter groups will be loaded in total\n",
            "Evaluation:   0% 0/302 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (637 > 512). Running this sequence through the model will result in indexing errors\n",
            "Evaluation: 100% 302/302 [00:02<00:00, 108.89it/s]\n",
            "\n",
            "\n",
            "===Sentiment Analysis Test results for tr random_init movie=== \n",
            " Acc:\t0.870489372219476\n",
            "F1\t0.8035982008995503\n",
            "Loss\t97.8883149906469\n",
            "\n",
            "\n",
            "\n",
            " Storing experiment log to ../sa_experiment_results_3103/sa_experiment_log_movie_tr_random_init.json...\n",
            "Downloading trained Sentiment Analysis models to: ../SA_turkish_twitter_models\n",
            "\n",
            "===Downloading trained Sentiment Analysis models to replicate the result===\n",
            "\n",
            "Models for SA twitter_turkish are already downloaded.\n",
            "Content of the save model folder: ['twitter_tr_word2vec_best_sa_model_weights.pkh', 'twitter_tr_bert_best_sa_model_weights.pkh', 'twitter_tr_bert_en_best_sa_model_weights.pkh', 'twitter_tr_fastext_best_sa_model_weights.pkh', 'twitter_tr_mbert_best_sa_model_weights.pkh', 'twitter_tr_random_init_best_sa_model_weights.pkh']\n",
            "Running for  sa  turkish  bert\n",
            "2021-03-31 13:43:22.126425: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Using trained models to predict...\n",
            "Running for  tr bert \n",
            "Initializing tokenizer with dbmdz/bert-base-turkish-cased for tr\n",
            "{'train': '../../datasets/sa_twitter_turkish-train.json', 'dev': '../../datasets/sa_twitter_turkish-dev.json', 'test': '../../datasets/sa_twitter_turkish-test.json'}\n",
            "Reading from ../../datasets/sa_twitter_turkish-train.json \n",
            "Dataset size : 1030 \n",
            "Grouping with batch size: 200\n",
            "Reading from ../../datasets/sa_twitter_turkish-dev.json \n",
            "Dataset size : 172 \n",
            "Grouping with batch size: 200\n",
            "Reading from ../../datasets/sa_twitter_turkish-test.json \n",
            "Dataset size : 514 \n",
            "Grouping with batch size: 200\n",
            "Training vocab: {'n': 0, 'p': 1}\n",
            "train number of batches: 58\n",
            "dev number of batches: 10\n",
            "test number of batches: 29\n",
            "Initializing lang specific bert model\n",
            "200 parameter groups will be loaded in total\n",
            "Evaluation: 100% 29/29 [00:01<00:00, 19.05it/s]\n",
            "\n",
            "\n",
            "===Sentiment Analysis Test results for tr bert twitter=== \n",
            " Acc:\t0.8657587548638133\n",
            "F1\t0.8490153172866521\n",
            "Loss\t11.652670178562403\n",
            "\n",
            "\n",
            "\n",
            " Storing experiment log to ../sa_experiment_results_3103/sa_experiment_log_twitter_tr_bert.json...\n",
            "Downloading trained Sentiment Analysis models to: ../SA_turkish_twitter_models\n",
            "\n",
            "===Downloading trained Sentiment Analysis models to replicate the result===\n",
            "\n",
            "Models for SA twitter_turkish are already downloaded.\n",
            "Content of the save model folder: ['twitter_tr_word2vec_best_sa_model_weights.pkh', 'twitter_tr_bert_best_sa_model_weights.pkh', 'twitter_tr_bert_en_best_sa_model_weights.pkh', 'twitter_tr_fastext_best_sa_model_weights.pkh', 'twitter_tr_mbert_best_sa_model_weights.pkh', 'twitter_tr_random_init_best_sa_model_weights.pkh']\n",
            "Running for  sa  turkish  mbert\n",
            "2021-03-31 13:43:45.783370: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Using trained models to predict...\n",
            "Running for  tr mbert \n",
            "Initializing tokenizer with bert-base-multilingual-cased for tr\n",
            "{'train': '../../datasets/sa_twitter_turkish-train.json', 'dev': '../../datasets/sa_twitter_turkish-dev.json', 'test': '../../datasets/sa_twitter_turkish-test.json'}\n",
            "Reading from ../../datasets/sa_twitter_turkish-train.json \n",
            "Dataset size : 1030 \n",
            "Grouping with batch size: 200\n",
            "Reading from ../../datasets/sa_twitter_turkish-dev.json \n",
            "Dataset size : 172 \n",
            "Grouping with batch size: 200\n",
            "Reading from ../../datasets/sa_twitter_turkish-test.json \n",
            "Dataset size : 514 \n",
            "Grouping with batch size: 200\n",
            "Training vocab: {'n': 0, 'p': 1}\n",
            "train number of batches: 58\n",
            "dev number of batches: 10\n",
            "test number of batches: 29\n",
            "Initializing mbert model\n",
            "200 parameter groups will be loaded in total\n",
            "Evaluation: 100% 29/29 [00:01<00:00, 17.08it/s]\n",
            "\n",
            "\n",
            "===Sentiment Analysis Test results for tr mbert twitter=== \n",
            " Acc:\t0.7529182879377432\n",
            "F1\t0.681704260651629\n",
            "Loss\t26.315239928662777\n",
            "\n",
            "\n",
            "\n",
            " Storing experiment log to ../sa_experiment_results_3103/sa_experiment_log_twitter_tr_mbert.json...\n",
            "Downloading trained Sentiment Analysis models to: ../SA_turkish_twitter_models\n",
            "\n",
            "===Downloading trained Sentiment Analysis models to replicate the result===\n",
            "\n",
            "Models for SA twitter_turkish are already downloaded.\n",
            "Content of the save model folder: ['twitter_tr_word2vec_best_sa_model_weights.pkh', 'twitter_tr_bert_best_sa_model_weights.pkh', 'twitter_tr_bert_en_best_sa_model_weights.pkh', 'twitter_tr_fastext_best_sa_model_weights.pkh', 'twitter_tr_mbert_best_sa_model_weights.pkh', 'twitter_tr_random_init_best_sa_model_weights.pkh']\n",
            "Running for  sa  turkish  bert_en\n",
            "2021-03-31 13:44:37.329815: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Using trained models to predict...\n",
            "Running for  tr bert_en \n",
            "Initializing tokenizer with bert-base-cased for tr\n",
            "{'train': '../../datasets/sa_twitter_turkish-train.json', 'dev': '../../datasets/sa_twitter_turkish-dev.json', 'test': '../../datasets/sa_twitter_turkish-test.json'}\n",
            "Reading from ../../datasets/sa_twitter_turkish-train.json \n",
            "Dataset size : 1030 \n",
            "Grouping with batch size: 200\n",
            "Reading from ../../datasets/sa_twitter_turkish-dev.json \n",
            "Dataset size : 172 \n",
            "Grouping with batch size: 200\n",
            "Reading from ../../datasets/sa_twitter_turkish-test.json \n",
            "Dataset size : 514 \n",
            "Grouping with batch size: 200\n",
            "Training vocab: {'n': 0, 'p': 1}\n",
            "train number of batches: 58\n",
            "dev number of batches: 10\n",
            "test number of batches: 29\n",
            "Initializing bert_en model\n",
            "200 parameter groups will be loaded in total\n",
            "Evaluation: 100% 29/29 [00:02<00:00, 13.20it/s]\n",
            "\n",
            "\n",
            "===Sentiment Analysis Test results for tr bert_en twitter=== \n",
            " Acc:\t0.7607003891050583\n",
            "F1\t0.7198177676537586\n",
            "Loss\t26.588940009474754\n",
            "\n",
            "\n",
            "\n",
            " Storing experiment log to ../sa_experiment_results_3103/sa_experiment_log_twitter_tr_bert_en.json...\n",
            "Downloading trained Sentiment Analysis models to: ../SA_turkish_twitter_models\n",
            "\n",
            "===Downloading trained Sentiment Analysis models to replicate the result===\n",
            "\n",
            "Models for SA twitter_turkish are already downloaded.\n",
            "Content of the save model folder: ['twitter_tr_word2vec_best_sa_model_weights.pkh', 'twitter_tr_bert_best_sa_model_weights.pkh', 'twitter_tr_bert_en_best_sa_model_weights.pkh', 'twitter_tr_fastext_best_sa_model_weights.pkh', 'twitter_tr_mbert_best_sa_model_weights.pkh', 'twitter_tr_random_init_best_sa_model_weights.pkh']\n",
            "Running for  sa  turkish  fastext\n",
            "2021-03-31 13:45:09.665991: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Using trained models to predict...\n",
            "Running for  tr fastext \n",
            "Initializing tokenizer with dbmdz/bert-base-turkish-cased for tr\n",
            "{'train': '../../datasets/sa_twitter_turkish-train.json', 'dev': '../../datasets/sa_twitter_turkish-dev.json', 'test': '../../datasets/sa_twitter_turkish-test.json'}\n",
            "Reading from ../../datasets/sa_twitter_turkish-train.json \n",
            "Dataset size : 1030 \n",
            "Grouping with batch size: 200\n",
            "Reading from ../../datasets/sa_twitter_turkish-dev.json \n",
            "Dataset size : 172 \n",
            "Grouping with batch size: 200\n",
            "Reading from ../../datasets/sa_twitter_turkish-test.json \n",
            "Dataset size : 514 \n",
            "Grouping with batch size: 200\n",
            "Training vocab: {'n': 0, 'p': 1}\n",
            "train number of batches: 58\n",
            "dev number of batches: 10\n",
            "test number of batches: 29\n",
            "27 parameter groups will be loaded in total\n",
            "Evaluation: 100% 29/29 [00:00<00:00, 127.05it/s]\n",
            "\n",
            "\n",
            "===Sentiment Analysis Test results for tr fastext twitter=== \n",
            " Acc:\t0.7392996108949417\n",
            "F1\t0.6731707317073171\n",
            "Loss\t17.223598688840866\n",
            "\n",
            "\n",
            "\n",
            " Storing experiment log to ../sa_experiment_results_3103/sa_experiment_log_twitter_tr_fastext.json...\n",
            "Downloading trained Sentiment Analysis models to: ../SA_turkish_twitter_models\n",
            "\n",
            "===Downloading trained Sentiment Analysis models to replicate the result===\n",
            "\n",
            "Models for SA twitter_turkish are already downloaded.\n",
            "Content of the save model folder: ['twitter_tr_word2vec_best_sa_model_weights.pkh', 'twitter_tr_bert_best_sa_model_weights.pkh', 'twitter_tr_bert_en_best_sa_model_weights.pkh', 'twitter_tr_fastext_best_sa_model_weights.pkh', 'twitter_tr_mbert_best_sa_model_weights.pkh', 'twitter_tr_random_init_best_sa_model_weights.pkh']\n",
            "Running for  sa  turkish  word2vec\n",
            "2021-03-31 13:45:19.134746: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Using trained models to predict...\n",
            "Running for  tr word2vec \n",
            "Initializing tokenizer with dbmdz/bert-base-turkish-cased for tr\n",
            "{'train': '../../datasets/sa_twitter_turkish-train.json', 'dev': '../../datasets/sa_twitter_turkish-dev.json', 'test': '../../datasets/sa_twitter_turkish-test.json'}\n",
            "Reading from ../../datasets/sa_twitter_turkish-train.json \n",
            "Dataset size : 1030 \n",
            "Grouping with batch size: 200\n",
            "Reading from ../../datasets/sa_twitter_turkish-dev.json \n",
            "Dataset size : 172 \n",
            "Grouping with batch size: 200\n",
            "Reading from ../../datasets/sa_twitter_turkish-test.json \n",
            "Dataset size : 514 \n",
            "Grouping with batch size: 200\n",
            "Training vocab: {'n': 0, 'p': 1}\n",
            "train number of batches: 58\n",
            "dev number of batches: 10\n",
            "test number of batches: 29\n",
            "27 parameter groups will be loaded in total\n",
            "Evaluation: 100% 29/29 [00:00<00:00, 121.51it/s]\n",
            "\n",
            "\n",
            "===Sentiment Analysis Test results for tr word2vec twitter=== \n",
            " Acc:\t0.745136186770428\n",
            "F1\t0.6960556844547564\n",
            "Loss\t32.79195508360863\n",
            "\n",
            "\n",
            "\n",
            " Storing experiment log to ../sa_experiment_results_3103/sa_experiment_log_twitter_tr_word2vec.json...\n",
            "Downloading trained Sentiment Analysis models to: ../SA_turkish_twitter_models\n",
            "\n",
            "===Downloading trained Sentiment Analysis models to replicate the result===\n",
            "\n",
            "Models for SA twitter_turkish are already downloaded.\n",
            "Content of the save model folder: ['twitter_tr_word2vec_best_sa_model_weights.pkh', 'twitter_tr_bert_best_sa_model_weights.pkh', 'twitter_tr_bert_en_best_sa_model_weights.pkh', 'twitter_tr_fastext_best_sa_model_weights.pkh', 'twitter_tr_mbert_best_sa_model_weights.pkh', 'twitter_tr_random_init_best_sa_model_weights.pkh']\n",
            "Running for  sa  turkish  random_init\n",
            "2021-03-31 13:45:28.646675: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Using trained models to predict...\n",
            "Running for  tr random_init \n",
            "Initializing tokenizer with dbmdz/bert-base-turkish-cased for tr\n",
            "{'train': '../../datasets/sa_twitter_turkish-train.json', 'dev': '../../datasets/sa_twitter_turkish-dev.json', 'test': '../../datasets/sa_twitter_turkish-test.json'}\n",
            "Reading from ../../datasets/sa_twitter_turkish-train.json \n",
            "Dataset size : 1030 \n",
            "Grouping with batch size: 200\n",
            "Reading from ../../datasets/sa_twitter_turkish-dev.json \n",
            "Dataset size : 172 \n",
            "Grouping with batch size: 200\n",
            "Reading from ../../datasets/sa_twitter_turkish-test.json \n",
            "Dataset size : 514 \n",
            "Grouping with batch size: 200\n",
            "Training vocab: {'n': 0, 'p': 1}\n",
            "train number of batches: 58\n",
            "dev number of batches: 10\n",
            "test number of batches: 29\n",
            "27 parameter groups will be loaded in total\n",
            "Evaluation: 100% 29/29 [00:00<00:00, 129.40it/s]\n",
            "\n",
            "\n",
            "===Sentiment Analysis Test results for tr random_init twitter=== \n",
            " Acc:\t0.7140077821011673\n",
            "F1\t0.6541176470588235\n",
            "Loss\t20.560567021369934\n",
            "\n",
            "\n",
            "\n",
            " Storing experiment log to ../sa_experiment_results_3103/sa_experiment_log_twitter_tr_random_init.json...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBkU9PyALxDi",
        "outputId": "e7a7051b-e899-49d7-cc07-372216112d26"
      },
      "source": [
        "sa_result_path = os.path.join(save_dir,\"sa_test_results.txt\")\n",
        "!cat $sa_result_path"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model\tAccuracy\tF1\n",
            "movie_en_bert_en\t0.833\t0.845\n",
            "movie_en_fastext\t0.746\t0.763\n",
            "movie_en_word2vec\t0.761\t0.774\n",
            "movie_en_random_init\t0.745\t0.768\n",
            "movie_en_mbert\t0.804\t0.816\n",
            "movie_en_bert_en\t0.833\t0.845\n",
            "movie_en_fastext\t0.746\t0.763\n",
            "movie_en_word2vec\t0.761\t0.774\n",
            "movie_en_random_init\t0.745\t0.768\n",
            "movie_tr_bert\t0.93\t0.895\n",
            "movie_tr_mbert\t0.819\t0.74\n",
            "movie_tr_bert_en\t0.758\t0.709\n",
            "movie_tr_fastext\t0.865\t0.805\n",
            "movie_tr_word2vec\t0.876\t0.815\n",
            "movie_tr_random_init\t0.87\t0.804\n",
            "twitter_tr_bert\t0.866\t0.849\n",
            "twitter_tr_mbert\t0.753\t0.682\n",
            "twitter_tr_bert_en\t0.761\t0.72\n",
            "twitter_tr_fastext\t0.739\t0.673\n",
            "twitter_tr_word2vec\t0.745\t0.696\n",
            "twitter_tr_random_init\t0.714\t0.654\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhaqq-UXi5eg"
      },
      "source": [
        "# Run NER and DEP Experiments\n",
        "\n",
        "**NOTE!!** To run monolingual Hungarian BERT models, you need to obtain the TensorFlow-based huBERT  from:  \n",
        " https://hlt.bme.hu/en/resources/hubert \n",
        "\n",
        "By default, the model folder should reside under \"../bert_models/hubert\".\n",
        "Use the --hubert_path to change the path accordingly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkwUu-47jNGJ"
      },
      "source": [
        "## Single Task Learning Results\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzb4kYeIpAf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "outputId": "ae4a4e0a-072e-4472-89f6-55e8f6f38a4f"
      },
      "source": [
        "import os\n",
        "import subprocess\n",
        "save_dir = \"all_stl_results_3103\"\n",
        "langs=[\"czech\", \"turkish\",  \"japanese\", \"english\", \"finnish\", \"hungarian\"]\n",
        "lang_prefs = [\"cs\",\"tr\",\"jp\",\"en\",\"fi\",\"hu\"]\n",
        "bert_embed_types = [\"bert_en\",\"mbert\",\"bert\"]\n",
        "nonbert_embed_types = [\"random_init\",\"word2vec\",\"fastext\"]\n",
        "tasks = [\"NER\",\"DEP\"]\n",
        "\n",
        "for task in tasks:\n",
        "  for word_type in bert_embed_types + nonbert_embed_types:\n",
        "    model_folder=os.path.join(\"../\",\"{}_{}_models\".format(task,word_type))\n",
        "    load_download_models(task, word_type, save_folder=model_folder)\n",
        "    for l_p, l in zip(lang_prefs,langs):\n",
        "      task_lower = task.lower()\n",
        "      load_path = os.path.join(model_folder,\"{}_{}_{}_best_{}_model.pkh\".format(task,word_type,l_p,task_lower))\n",
        "      !python jointtrainer_multilang.py --mode \"predict\" --eval_mode $task --model_type $task  --load_model 1 --load_path $load_path --word_embed_type $word_type   --lang $l_p --save_dir $save_dir\n",
        "    cmd = \"rm -r {}\".format(model_folder)\n",
        "    print(\"Removing the downloaded models to free space...\")\n",
        "    subprocess.call(cmd,shell=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "===Downloading trained Dependency Parsing models to replicate the result===\n",
            "\n",
            "../DEP_word2vec_models not found. Downloading trained models for DEP word2vec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Permission denied: https://drive.google.com/uc?id=1gF7ujIvRdmwvIKIdGw9bKBoKgstX1Mws\n",
            "Maybe you need to change permission over 'Anyone with the link'?\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-0397ee97feeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mword_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"word2vec\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"random_init\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmodel_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"{}_{}_models\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mload_download_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ml_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_prefs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlangs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0;31m# if word_type == \"word2vec\":\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-2720862fd95e>\u001b[0m in \u001b[0;36mload_download_models\u001b[0;34m(model_type, word_type, save_folder)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} not found. Downloading trained models for {} {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munzip_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mgdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquiet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0munzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munzip_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trained models are stored in {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munzip_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munzip_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-2720862fd95e>\u001b[0m in \u001b[0;36munzip\u001b[0;34m(src, dest)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0munzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzipObj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;31m# Extract all the contents of zip file in different directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mzipObj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel)\u001b[0m\n\u001b[1;32m   1238\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1240\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1241\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../DEP_word2vec_models.zip'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oj6Mzj-wb9E7"
      },
      "source": [
        "# Run Multi-Task Learning (MTL) Experiments\n",
        "\n",
        "First you need to download all the pretrained models using the link below:\n",
        "\n",
        "https://drive.google.com/drive/folders/1OL52-tDvHPWOReNnNytgjotxDB7z_187?usp=sharing\n",
        "\n",
        "The models are grouped in (Task,word_embed) fields. For example, all DEP MTL models for word2Vec-based approach for all 6 languages are stored in a single zip file.\n",
        "\n",
        " \n",
        "  - Tasks: DEP, NER\n",
        "  - Word_embed types: bert, mbert, bert_en, fastext, word2vec, random_init\n",
        "\n",
        "For each (task,word_embed) combination make sure the unzipped model folder exists under the \"root_folder\". \n",
        "\n",
        "**NOTE**: Due to the Colab space limits, you may need to unmount the folder containing the pretrained models of the previous experiments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5UahSljb8XZ"
      },
      "source": [
        "\n",
        "import os\n",
        "import subprocess\n",
        "root_folder = \"../\"\n",
        "save_dir = \"all_mtl_results_3103\"\n",
        "langs=[\"czech\", \"turkish\",  \"japanese\", \"english\", \"finnish\", \"hungarian\"]\n",
        "lang_prefs = [\"cs\",\"tr\",\"jp\",\"en\",\"fi\",\"hu\"]\n",
        "bert_embed_types = [\"bert_en\",\"mbert\",\"bert\"]\n",
        "nonbert_embed_types = [\"random_init\",\"word2vec\",\"fastext\"]\n",
        "tasks = [\"NER\",\"DEP\"]\n",
        "model_type = \"FLAT\"\n",
        "for task in tasks:\n",
        "  for word_type in bert_embed_types + nonbert_embed_types:\n",
        "    model_folder=os.path.join(root_folder,\"{}_{}_models\".format(task,word_type))\n",
        "    for l_p, l in zip(lang_prefs,langs):\n",
        "      task_lower = task.lower()\n",
        "      load_path = os.path.join(model_folder,\"{}_{}_{}_best_{}_model.pkh\".format(task,word_type,l_p,task_lower))\n",
        "      !python jointtrainer_multilang.py --mode \"predict\" --eval_mode $task --model_type $model_type  --load_model 1 --load_path $load_path --word_embed_type $word_type   --lang $l_p --save_dir $save_dir\n",
        "    cmd = \"rm -r {}\".format(model_folder)\n",
        "    print(\"Removing the downloaded models to free space...\")\n",
        "    subprocess.call(cmd,shell=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr02scmwMS1k"
      },
      "source": [
        "# Training New Models\n",
        "\n",
        "You can also use this Colab notebook to train new models.\n",
        "For word2vec models you need to obtain the pretrained embeddings to start the training.  \n",
        "You can obtain word2vec embeddings from:\n",
        "\n",
        "https://drive.google.com/drive/folders/1GqUlRknYWjSECdczeFQ0xxLzxnJAotHQ?usp=sharing\n",
        "\n",
        "Make sure to change variable names accordingly. By default, word2vec embeddings must reside under \"../word_vecs\".  \n",
        "Example\n",
        "- Hungarian Word2Vec path: \"../word_vecs/hu/hu.bin\"\n",
        "- English Word2Vec path: \"../word_vecs/en/en.txt\" \n",
        "\n",
        "**NOTE!!** To train monolingual Hungarian BERT models, you need to obtain the TensorFlow-based huBERT  from:  \n",
        " https://hlt.bme.hu/en/resources/hubert \n",
        "\n",
        "By default, the model folder should reside under \"../bert_models/hubert\".\n",
        "Use the --hubert_path to change the path accordingly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "get_3cCnMREO",
        "outputId": "a9d733a4-d8eb-4baf-b32c-a57f4bc22b73"
      },
      "source": [
        "langs = [\"cs\",\"fi\",\"en\",\"tr\",\"jp\",\"hu\"]\n",
        "bert_embed_types = [\"bert_en\",\"mbert\",\"bert\"]\n",
        "nonbert_embed_types = [\"random_init\",\"word2vec\",\"fastext\"]\n",
        "save_dir=\"../ner_turkish_mbert_1\"\n",
        "# Training NER model with mbert for Finnish\n",
        "!python jointtrainer_multilang.py --model_type NER  --word_embed_type mbert  --lang tr --save_dir $save_dir\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-31 13:59:58.500495: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "{'data_dir': 'data/depparse', 'data_folder': '../../datasets', 'wordvec_dir': '../word_vecs', 'train_file': None, 'eval_file': None, 'output_file': None, 'gold_file': None, 'ner_result_out_file': 'ner_results', 'ner_test_result_file': 'ner_test_results.txt', 'dep_test_result_file': 'dep_test_results.txt', 'log_file': 'jointtraining.log', 'ner_train_file': '../../datasets/traindev_pos.tsv', 'dep_train_file': '../../datasets/tr_imst-ud-traindev.conllu', 'ner_val_file': '../../datasets/dev_pos.tsv', 'dep_val_file': '../../datasets/tr_imst-ud-train.conllu', 'ner_test_file': '../../datasets/test_pos.tsv', 'dep_test_file': '../../datasets/tr_imst-ud-test.conllu', 'ner_output_file': 'joint_ner_out.txt', 'dep_output_file': 'joint_dep_out.txt', 'conll_file_name': 'conll_ner_output', 'config_file': 'config.json', 'mode': 'train', 'eval_mode': 'BOTH', 'load_config': 0, 'lang': 'tr', 'shorthand': None, 'lstm_hidden': 400, 'char_hidden_dim': 200, 'biaffine_hidden': 400, 'composite_deep_biaff_hidden_dim': 100, 'cap_dim': 32, 'char_emb_dim': 100, 'cap_types': 6, 'pos_dim': 64, 'dep_dim': 128, 'ner_dim': 128, 'transformed_dim': 125, 'word_embed_type': 'mbert', 'fix_embed': False, 'word_only': False, 'lstm_layers': 3, 'char_num_layers': 1, 'pretrain_max_vocab': -1, 'word_drop': 0.3, 'embed_drop': 0.3, 'lstm_drop': 0.3, 'crf_drop': 0.3, 'parser_drop': 0.3, 'rec_dropout': 0.2, 'char_rec_dropout': 0, 'char': True, 'pretrain': True, 'linearization': True, 'distance': True, 'sample_train': 1.0, 'optim': 'adam', 'ner_lr': 0.0015, 'embed_lr': 0.015, 'dep_lr': 0.0015, 'lr_decay': 0.6, 'min_lr': 2e-06, 'beta2': 0.95, 'weight_decay': 0.0005, 'max_steps': None, 'epochs': 20, 'repeat': 1, 'multiple': 0, 'early_stop': 50, 'dep_warmup': -1, 'lr_patience': 3, 'ner_warmup': -1, 'eval_interval': None, 'max_steps_before_stop': 3000, 'batch_size': 300, 'max_grad_norm': 1.0, 'max_depgrad_norm': 5.0, 'log_step': 20, 'save_dir': '../ner_turkish_mbert_1', 'hubert_path': '../bert_models/hubert', 'save_name': 'best_joint_model.pkh', 'save_ner_name': 'best_ner_model.pkh', 'save_dep_name': 'best_dep_model.pkh', 'load_model': 0, 'load_path': 'best_joint_model.pkh', 'seed': 1234, 'cuda': True, 'cpu': False, 'inner': 1, 'soft': 1, 'relu': 1, 'model_type': 'NER', 'hyper': 0, 'max_evals': 50, 'ner_only': 0, 'dep_only': 0, 'depner': 0, 'nerdep': 0, 'device': device(type='cuda', index=0)}\n",
            "tr   dbmdz/bert-base-turkish-cased\n",
            "BERT Tokenizer\n",
            "PreTrainedTokenizerFast(name_or_path='bert-base-multilingual-cased', vocab_size=119547, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n",
            "Experiment name NER_mbert_tr \n",
            "Reading from ../../datasets/myner_turkish-train.txt \n",
            "Number of sentences for ../../datasets/myner_turkish-train.txt  : 24607 \n",
            "Dataset size : 24607\n",
            "DEP dataset number of sents : 3685\n",
            "Dataset size before batching 3685 and number of words : 38082\n",
            "383 batches created for ../../datasets/dep_turkish_train.conllu.\n",
            "41767 words inside the batches\n",
            "Reading from ../../datasets/myner_turkish-dev.txt \n",
            "Number of sentences for ../../datasets/myner_turkish-dev.txt  : 2858 \n",
            "Dataset size : 2858\n",
            "DEP dataset number of sents : 975\n",
            "Dataset size before batching 975 and number of words : 10011\n",
            "101 batches created for ../../datasets/dep_turkish_dev.conllu.\n",
            "10986 words inside the batches\n",
            "Reading from ../../datasets/myner_turkish-test.txt \n",
            "Number of sentences for ../../datasets/myner_turkish-test.txt  : 2828 \n",
            "Dataset size : 2828\n",
            "DEP dataset number of sents : 975\n",
            "Dataset size before batching 975 and number of words : 10004\n",
            "101 batches created for ../../datasets/dep_turkish_test.conllu.\n",
            "10979 words inside the batches\n",
            "{'[PAD]': 0, '[SOS]': 1, '[EOS]': 2, 'O': 3, 'I-LOC': 4, 'I-PER': 5, 'I-ORG': 6}\n",
            "{'[PAD]': 0, '[SOS]': 1, '[EOS]': 2, 'O': 3, 'I-ORG': 4, 'I-LOC': 5, 'I-PER': 6}\n",
            "DEP REL VOCABS\n",
            "{'[PAD]': 0, '[SOS]': 1, '[EOS]': 2, '[UNK]': 3, 'amod': 4, 'csubj': 5, 'cop': 6, 'nmod:poss': 7, 'obj': 8, 'root': 9, 'punct': 10, 'obl': 11, 'compound': 12, 'case': 13, 'nmod': 14, 'nsubj': 15, 'conj': 16, 'det': 17, 'advmod': 18, 'acl': 19, 'cc': 20, 'flat': 21, 'compound:lvc': 22, 'nummod': 23, 'discourse': 24, 'compound:redup': 25, 'appos': 26, 'mark': 27, 'advmod:emph': 28, 'aux:q': 29, 'fixed': 30, 'parataxis': 31, 'ccomp': 32, 'dep': 33, 'vocative': 34}\n",
            "{'[PAD]': 0, '[SOS]': 1, '[EOS]': 2, '[UNK]': 3, 'amod': 4, 'csubj': 5, 'cop': 6, 'nmod:poss': 7, 'obj': 8, 'root': 9, 'punct': 10, 'obl': 11, 'compound': 12, 'case': 13, 'nmod': 14, 'nsubj': 15, 'conj': 16, 'det': 17, 'advmod': 18, 'acl': 19, 'cc': 20, 'flat': 21, 'compound:lvc': 22, 'nummod': 23, 'discourse': 24, 'compound:redup': 25, 'appos': 26, 'mark': 27, 'advmod:emph': 28, 'aux:q': 29, 'fixed': 30, 'parataxis': 31, 'ccomp': 32, 'dep': 33, 'vocative': 34}\n",
            "{'[PAD]': 0, '[SOS]': 1, '[EOS]': 2, '[UNK]': 3, 'amod': 4, 'csubj': 5, 'cop': 6, 'nmod:poss': 7, 'obj': 8, 'root': 9, 'punct': 10, 'obl': 11, 'compound': 12, 'case': 13, 'nmod': 14, 'nsubj': 15, 'conj': 16, 'det': 17, 'advmod': 18, 'acl': 19, 'cc': 20, 'flat': 21, 'compound:lvc': 22, 'nummod': 23, 'discourse': 24, 'compound:redup': 25, 'appos': 26, 'mark': 27, 'advmod:emph': 28, 'aux:q': 29, 'fixed': 30, 'parataxis': 31, 'ccomp': 32, 'dep': 33, 'vocative': 34}\n",
            "Before merging\n",
            "Dependency pos tags \n",
            "{'[PAD]': 0, '[SOS]': 1, '[EOS]': 2, '[UNK]': 3, 'ADJ': 4, 'NOUN': 5, 'AUX': 6, 'PRON': 7, 'VERB': 8, 'PUNCT': 9, 'DET': 10, 'ADP': 11, 'PROPN': 12, 'NUM': 13, 'ADV': 14, 'CCONJ': 15, 'INTJ': 16, 'X': 17}\n",
            "NER pos tags\n",
            "{'[PAD]': 0, '[SOS]': 1, '[EOS]': 2, 'Noun': 3, 'Prop': 4, 'Reflex': 5, 'Adj': 6, 'Conj': 7, 'NNum': 8, 'Punc': 9, 'Verb': 10, 'Zero': 11, 'Ness': 12, '[UNK]': 13, 'PCNom': 14, 'With': 15, 'Det': 16, 'Adverb': 17, 'Without': 18, 'Rel': 19, 'Agt': 20, 'Quant': 21, 'Demons': 22, 'Ques': 23, 'Pers': 24, 'PCDat': 25, 'PCAbl': 26, 'Neg': 27, 'Dup': 28, 'AsIf': 29, 'PCIns': 30, 'Since': 31, 'Interj': 32, 'PCGen': 33, 'Ly': 34, 'PCAcc': 35}\n",
            "After merging\n",
            "Dependency pos tags \n",
            "{'[PAD]': 0, '[SOS]': 1, '[EOS]': 2, '[UNK]': 3, 'ADJ': 4, 'NOUN': 5, 'AUX': 6, 'PRON': 7, 'VERB': 8, 'PUNCT': 9, 'DET': 10, 'ADP': 11, 'PROPN': 12, 'NUM': 13, 'ADV': 14, 'CCONJ': 15, 'INTJ': 16, 'X': 17}\n",
            "NER pos tags\n",
            "{'[PAD]': 0, '[SOS]': 1, '[EOS]': 2, 'Noun': 3, 'Prop': 4, 'Reflex': 5, 'Adj': 6, 'Conj': 7, 'NNum': 8, 'Punc': 9, 'Verb': 10, 'Zero': 11, 'Ness': 12, '[UNK]': 13, 'PCNom': 14, 'With': 15, 'Det': 16, 'Adverb': 17, 'Without': 18, 'Rel': 19, 'Agt': 20, 'Quant': 21, 'Demons': 22, 'Ques': 23, 'Pers': 24, 'PCDat': 25, 'PCAbl': 26, 'Neg': 27, 'Dup': 28, 'AsIf': 29, 'PCIns': 30, 'Since': 31, 'Interj': 32, 'PCGen': 33, 'Ly': 34, 'PCAcc': 35}\n",
            "NER vocab size 66307 \n",
            "DEP vocab size 13785 \n",
            "NER Training pos vocab : {'[PAD]': 0, '[SOS]': 1, '[EOS]': 2, 'Noun': 3, 'Prop': 4, 'Reflex': 5, 'Adj': 6, 'Conj': 7, 'NNum': 8, 'Punc': 9, 'Verb': 10, 'Zero': 11, 'Ness': 12, '[UNK]': 13, 'PCNom': 14, 'With': 15, 'Det': 16, 'Adverb': 17, 'Without': 18, 'Rel': 19, 'Agt': 20, 'Quant': 21, 'Demons': 22, 'Ques': 23, 'Pers': 24, 'PCDat': 25, 'PCAbl': 26, 'Neg': 27, 'Dup': 28, 'AsIf': 29, 'PCIns': 30, 'Since': 31, 'Interj': 32, 'PCGen': 33, 'Ly': 34, 'PCAcc': 35}\n",
            "NER Testing  pos vocab : {'[PAD]': 0, '[SOS]': 1, '[EOS]': 2, 'Noun': 3, 'Prop': 4, 'Reflex': 5, 'Adj': 6, 'Conj': 7, 'NNum': 8, 'Punc': 9, 'Verb': 10, 'Zero': 11, 'Ness': 12, '[UNK]': 13, 'PCNom': 14, 'With': 15, 'Det': 16, 'Adverb': 17, 'Without': 18, 'Rel': 19, 'Agt': 20, 'Quant': 21, 'Demons': 22, 'Ques': 23, 'Pers': 24, 'PCDat': 25, 'PCAbl': 26, 'Neg': 27, 'Dup': 28, 'AsIf': 29, 'PCIns': 30, 'Since': 31, 'Interj': 32, 'PCGen': 33, 'Ly': 34, 'PCAcc': 35}\n",
            "NER Training vocab size : 66307\n",
            "NER Val vocab size : 66307\n",
            "NER Test vocab size : 66307\n",
            "DEP Training vocab size : 13785\n",
            "DEP Validation vocab size : 13785\n",
            "DEP Test vocab size : 13785\n",
            "DEP Val  0 words not in training set \n",
            "Ner dataset contains 1164 batches\n",
            "Dep dataset contains 383  batches \n",
            "Eval interval is set to 1164 \n",
            "Word vocabulary size  : 66307\n",
            "Joint Trainer initialized on cuda:0\n",
            "\n",
            "\n",
            "STARTING NEW TRAINING\n",
            "\n",
            "\n",
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "end index : 2\n",
            "start index : 1\n",
            "Tag set size : 7\n",
            "Running for 20 epochs and evaluating after 1164 steps\n",
            "Epoch:   0% 0/20 [00:00<?, ?it/s]\n",
            "Training:   0% 0/1164 [00:00<?, ?it/s]\u001b[A\n",
            "Training:   0% 1/1164 [00:00<12:17,  1.58it/s]\u001b[A\n",
            "Training:   0% 2/1164 [00:00<10:29,  1.84it/s]\u001b[A\n",
            "Training:   0% 3/1164 [00:01<09:14,  2.10it/s]\u001b[A\n",
            "Training:   0% 4/1164 [00:01<08:20,  2.32it/s]\u001b[A\n",
            "Training:   0% 5/1164 [00:01<07:42,  2.50it/s]\u001b[A\n",
            "Training:   1% 6/1164 [00:02<07:14,  2.67it/s]\u001b[A\n",
            "Training:   1% 7/1164 [00:02<07:01,  2.74it/s]\u001b[A\n",
            "Training:   1% 8/1164 [00:02<07:13,  2.67it/s]\u001b[A\n",
            "Training:   1% 9/1164 [00:03<06:59,  2.75it/s]\u001b[A\n",
            "Training:   1% 10/1164 [00:03<07:00,  2.75it/s]\u001b[A\n",
            "Training:   1% 11/1164 [00:04<07:04,  2.72it/s]\u001b[A\n",
            "Training:   1% 12/1164 [00:04<06:44,  2.85it/s]\u001b[A\n",
            "Training:   1% 13/1164 [00:04<06:34,  2.91it/s]\u001b[A\n",
            "Training:   1% 14/1164 [00:05<06:36,  2.90it/s]\u001b[A\n",
            "Training:   1% 15/1164 [00:05<06:40,  2.87it/s]\u001b[A\n",
            "Training:   1% 16/1164 [00:05<06:49,  2.80it/s]\u001b[A\n",
            "Training:   1% 17/1164 [00:06<06:38,  2.88it/s]\u001b[A\n",
            "Training:   2% 18/1164 [00:06<06:42,  2.85it/s]\u001b[A\n",
            "Training:   2% 19/1164 [00:06<06:46,  2.82it/s]\u001b[A\n",
            "Training:   2% 20/1164 [00:07<06:34,  2.90it/s]\u001b[A\n",
            "Training:   2% 21/1164 [00:07<06:23,  2.98it/s]\u001b[A\n",
            "Training:   2% 22/1164 [00:07<06:18,  3.01it/s]\u001b[A\n",
            "Training:   2% 23/1164 [00:08<06:21,  2.99it/s]\u001b[A\n",
            "Training:   2% 24/1164 [00:08<06:25,  2.95it/s]\u001b[A\n",
            "Training:   2% 25/1164 [00:08<06:33,  2.89it/s]\u001b[A\n",
            "Training:   2% 26/1164 [00:09<06:33,  2.89it/s]\u001b[A\n",
            "Training:   2% 27/1164 [00:09<06:29,  2.92it/s]\u001b[A\n",
            "Training:   2% 28/1164 [00:09<06:29,  2.91it/s]\u001b[A\n",
            "Training:   2% 29/1164 [00:10<06:24,  2.95it/s]\u001b[A\n",
            "Training:   3% 30/1164 [00:10<06:25,  2.94it/s]\u001b[A\n",
            "Training:   3% 31/1164 [00:10<06:20,  2.98it/s]\u001b[A\n",
            "Training:   3% 32/1164 [00:11<06:27,  2.92it/s]\u001b[A\n",
            "Training:   3% 33/1164 [00:11<06:32,  2.88it/s]\u001b[A\n",
            "Training:   3% 34/1164 [00:11<06:22,  2.95it/s]\u001b[A\n",
            "Training:   3% 35/1164 [00:12<06:20,  2.97it/s]\u001b[A\n",
            "Training:   3% 36/1164 [00:12<06:20,  2.97it/s]\u001b[A\n",
            "Training:   3% 37/1164 [00:12<06:13,  3.01it/s]\u001b[A\n",
            "Training:   3% 38/1164 [00:13<06:21,  2.95it/s]\u001b[A\n",
            "Training:   3% 39/1164 [00:13<06:29,  2.89it/s]\u001b[A\n",
            "Training:   3% 40/1164 [00:14<06:46,  2.77it/s]\u001b[A\n",
            "Training:   4% 41/1164 [00:14<06:42,  2.79it/s]\u001b[A\n",
            "Training:   4% 42/1164 [00:14<06:44,  2.78it/s]\u001b[A\n",
            "Training:   4% 43/1164 [00:15<06:36,  2.83it/s]\u001b[A\n",
            "Training:   4% 44/1164 [00:15<06:25,  2.91it/s]\u001b[A\n",
            "Training:   4% 45/1164 [00:15<06:27,  2.88it/s]\u001b[A\n",
            "Training:   4% 46/1164 [00:16<06:24,  2.91it/s]\u001b[A\n",
            "Training:   4% 47/1164 [00:16<06:26,  2.89it/s]\u001b[A\n",
            "Training:   4% 48/1164 [00:16<06:36,  2.81it/s]\u001b[A\n",
            "Training:   4% 49/1164 [00:17<06:31,  2.85it/s]\u001b[A\n",
            "Training:   4% 50/1164 [00:17<06:26,  2.88it/s]\u001b[A\n",
            "Training:   4% 51/1164 [00:17<06:27,  2.87it/s]\u001b[A\n",
            "Training:   4% 52/1164 [00:18<06:29,  2.86it/s]\u001b[A\n",
            "Training:   5% 53/1164 [00:18<06:16,  2.95it/s]\u001b[A\n",
            "Training:   5% 54/1164 [00:18<06:08,  3.01it/s]\u001b[A\n",
            "Training:   5% 55/1164 [00:19<06:10,  2.99it/s]\u001b[A\n",
            "Training:   5% 56/1164 [00:19<06:08,  3.00it/s]\u001b[A\n",
            "Training:   5% 57/1164 [00:19<05:57,  3.10it/s]\u001b[A\n",
            "Training:   5% 58/1164 [00:20<06:03,  3.04it/s]\u001b[A\n",
            "Training:   5% 59/1164 [00:20<05:58,  3.08it/s]\u001b[A\n",
            "Training:   5% 60/1164 [00:20<05:59,  3.07it/s]\u001b[A\n",
            "Training:   5% 61/1164 [00:21<06:03,  3.04it/s]\u001b[A\n",
            "Training:   5% 62/1164 [00:21<05:59,  3.07it/s]\u001b[A\n",
            "Training:   5% 63/1164 [00:21<06:02,  3.04it/s]\u001b[A\n",
            "Training:   5% 64/1164 [00:22<06:05,  3.01it/s]\u001b[A\n",
            "Training:   6% 65/1164 [00:22<05:50,  3.13it/s]\u001b[A\n",
            "Training:   6% 66/1164 [00:22<05:51,  3.12it/s]\u001b[A\n",
            "Training:   6% 67/1164 [00:23<05:50,  3.13it/s]\u001b[A\n",
            "Training:   6% 68/1164 [00:23<05:41,  3.21it/s]\u001b[A\n",
            "Training:   6% 69/1164 [00:23<05:50,  3.12it/s]\u001b[A\n",
            "Training:   6% 70/1164 [00:24<06:03,  3.01it/s]\u001b[A\n",
            "Training:   6% 71/1164 [00:24<06:12,  2.93it/s]\u001b[A\n",
            "Training:   6% 72/1164 [00:24<06:14,  2.92it/s]\u001b[A\n",
            "Training:   6% 73/1164 [00:25<06:20,  2.86it/s]\u001b[A\n",
            "Training:   6% 74/1164 [00:25<06:09,  2.95it/s]\u001b[A\n",
            "Training:   6% 75/1164 [00:25<06:04,  2.99it/s]\u001b[A\n",
            "Training:   7% 76/1164 [00:26<06:03,  2.99it/s]\u001b[A\n",
            "Training:   7% 77/1164 [00:26<06:09,  2.94it/s]\u001b[A\n",
            "Training:   7% 78/1164 [00:26<06:03,  2.99it/s]\u001b[A\n",
            "Training:   7% 79/1164 [00:27<05:54,  3.06it/s]\u001b[A\n",
            "Training:   7% 80/1164 [00:27<06:10,  2.92it/s]\u001b[A\n",
            "Training:   7% 81/1164 [00:27<06:10,  2.92it/s]\u001b[A\n",
            "Training:   7% 82/1164 [00:28<06:02,  2.99it/s]\u001b[A\n",
            "Training:   7% 83/1164 [00:28<06:06,  2.95it/s]\u001b[A\n",
            "Training:   7% 84/1164 [00:28<06:21,  2.83it/s]\u001b[A\n",
            "Training:   7% 85/1164 [00:29<06:12,  2.90it/s]\u001b[A\n",
            "Training:   7% 86/1164 [00:29<06:07,  2.93it/s]\u001b[A\n",
            "Training:   7% 87/1164 [00:29<06:10,  2.91it/s]\u001b[A\n",
            "Training:   8% 88/1164 [00:30<06:13,  2.88it/s]\u001b[A\n",
            "Training:   8% 89/1164 [00:30<06:26,  2.78it/s]\u001b[A\n",
            "Training:   8% 90/1164 [00:30<06:06,  2.93it/s]\u001b[A\n",
            "Training:   8% 91/1164 [00:31<06:08,  2.91it/s]\u001b[A\n",
            "Training:   8% 92/1164 [00:31<06:16,  2.85it/s]\u001b[A\n",
            "Training:   8% 93/1164 [00:31<06:12,  2.87it/s]\u001b[A\n",
            "Training:   8% 94/1164 [00:32<06:19,  2.82it/s]\u001b[A\n",
            "Training:   8% 95/1164 [00:32<06:13,  2.86it/s]\u001b[A\n",
            "Training:   8% 96/1164 [00:32<06:09,  2.89it/s]\u001b[A\n",
            "Training:   8% 97/1164 [00:33<06:09,  2.89it/s]\u001b[A\n",
            "Training:   8% 98/1164 [00:33<06:02,  2.94it/s]\u001b[A\n",
            "Training:   9% 99/1164 [00:34<06:13,  2.85it/s]\u001b[A\n",
            "Training:   9% 100/1164 [00:34<06:05,  2.91it/s]\u001b[A\n",
            "Training:   9% 101/1164 [00:34<06:17,  2.81it/s]\u001b[A\n",
            "Training:   9% 102/1164 [00:35<06:06,  2.90it/s]\u001b[A\n",
            "Training:   9% 103/1164 [00:35<06:06,  2.89it/s]\u001b[A\n",
            "Training:   9% 104/1164 [00:35<06:27,  2.73it/s]\u001b[A\n",
            "Training:   9% 105/1164 [00:36<06:23,  2.76it/s]\u001b[A\n",
            "Training:   9% 106/1164 [00:36<06:21,  2.77it/s]\u001b[A\n",
            "Training:   9% 107/1164 [00:36<06:07,  2.88it/s]\u001b[A\n",
            "Training:   9% 108/1164 [00:37<06:19,  2.79it/s]\u001b[A\n",
            "Training:   9% 109/1164 [00:37<06:17,  2.80it/s]\u001b[A\n",
            "Training:   9% 110/1164 [00:37<06:13,  2.82it/s]\u001b[A\n",
            "Training:  10% 111/1164 [00:38<06:06,  2.87it/s]\u001b[A\n",
            "Training:  10% 112/1164 [00:38<06:00,  2.92it/s]\u001b[A\n",
            "Training:  10% 113/1164 [00:38<06:02,  2.90it/s]\u001b[A\n",
            "Training:  10% 114/1164 [00:39<06:01,  2.91it/s]\u001b[A\n",
            "Training:  10% 115/1164 [00:39<05:56,  2.94it/s]\u001b[A\n",
            "Training:  10% 116/1164 [00:39<05:53,  2.96it/s]\u001b[A\n",
            "Training:  10% 117/1164 [00:40<05:57,  2.93it/s]\u001b[A\n",
            "Training:  10% 118/1164 [00:40<05:59,  2.91it/s]\u001b[A\n",
            "Training:  10% 119/1164 [00:40<05:53,  2.95it/s]\u001b[A\n",
            "Training:  10% 120/1164 [00:41<06:06,  2.85it/s]\u001b[A\n",
            "Training:  10% 121/1164 [00:41<06:15,  2.78it/s]\u001b[A\n",
            "Training:  10% 122/1164 [00:42<06:17,  2.76it/s]\u001b[A\n",
            "Training:  11% 123/1164 [00:42<06:31,  2.66it/s]\u001b[A\n",
            "Training:  11% 124/1164 [00:42<06:27,  2.68it/s]\u001b[A\n",
            "Training:  11% 125/1164 [00:43<06:26,  2.69it/s]\u001b[A\n",
            "Training:  11% 126/1164 [00:43<06:33,  2.64it/s]\u001b[A\n",
            "Training:  11% 127/1164 [00:44<06:33,  2.63it/s]\u001b[A\n",
            "Training:  11% 128/1164 [00:44<06:27,  2.68it/s]\u001b[A\n",
            "Training:  11% 129/1164 [00:44<06:09,  2.80it/s]\u001b[A\n",
            "Training:  11% 130/1164 [00:45<05:58,  2.88it/s]\u001b[A\n",
            "Training:  11% 131/1164 [00:45<05:59,  2.88it/s]\u001b[A\n",
            "Training:  11% 132/1164 [00:45<05:56,  2.89it/s]\u001b[A\n",
            "Training:  11% 133/1164 [00:46<05:55,  2.90it/s]\u001b[A\n",
            "Training:  12% 134/1164 [00:46<06:06,  2.81it/s]\u001b[A\n",
            "Training:  12% 135/1164 [00:46<06:25,  2.67it/s]\u001b[A\n",
            "Training:  12% 136/1164 [00:47<06:21,  2.70it/s]\u001b[A\n",
            "Training:  12% 137/1164 [00:47<06:15,  2.73it/s]\u001b[A\n",
            "Training:  12% 138/1164 [00:47<06:09,  2.78it/s]\u001b[A\n",
            "Training:  12% 139/1164 [00:48<05:56,  2.87it/s]\u001b[A\n",
            "Training:  12% 140/1164 [00:48<05:53,  2.89it/s]\u001b[A\n",
            "Training:  12% 141/1164 [00:48<05:45,  2.96it/s]\u001b[A\n",
            "Training:  12% 142/1164 [00:49<05:41,  2.99it/s]\u001b[A\n",
            "Training:  12% 143/1164 [00:49<05:49,  2.92it/s]\u001b[A\n",
            "Training:  12% 144/1164 [00:49<05:48,  2.93it/s]\u001b[A\n",
            "Training:  12% 145/1164 [00:50<05:50,  2.90it/s]\u001b[A\n",
            "Training:  13% 146/1164 [00:50<05:48,  2.92it/s]\u001b[A\n",
            "Training:  13% 147/1164 [00:50<05:50,  2.90it/s]\u001b[A\n",
            "Training:  13% 148/1164 [00:51<05:58,  2.84it/s]\u001b[A\n",
            "Training:  13% 149/1164 [00:51<05:55,  2.85it/s]\u001b[A\n",
            "Training:  13% 150/1164 [00:52<06:11,  2.73it/s]\u001b[A\n",
            "Training:  13% 151/1164 [00:52<06:16,  2.69it/s]\u001b[A\n",
            "Training:  13% 152/1164 [00:52<06:24,  2.63it/s]\u001b[A\n",
            "Training:  13% 153/1164 [00:53<06:09,  2.74it/s]\u001b[A\n",
            "Training:  13% 154/1164 [00:53<05:56,  2.83it/s]\u001b[A\n",
            "Training:  13% 155/1164 [00:53<05:46,  2.91it/s]\u001b[A\n",
            "Training:  13% 156/1164 [00:54<06:22,  2.63it/s]\u001b[A\n",
            "Training:  13% 157/1164 [00:54<06:16,  2.67it/s]\u001b[A\n",
            "Training:  14% 158/1164 [00:55<06:08,  2.73it/s]\u001b[A\n",
            "Training:  14% 159/1164 [00:55<05:54,  2.83it/s]\u001b[A\n",
            "Training:  14% 160/1164 [00:55<05:48,  2.88it/s]\u001b[A\n",
            "Training:  14% 161/1164 [00:56<05:56,  2.82it/s]\u001b[A\n",
            "Training:  14% 162/1164 [00:56<05:59,  2.79it/s]\u001b[A\n",
            "Training:  14% 163/1164 [00:56<06:06,  2.73it/s]\u001b[A\n",
            "Training:  14% 164/1164 [00:57<05:55,  2.82it/s]\u001b[A\n",
            "Training:  14% 165/1164 [00:57<06:19,  2.63it/s]\u001b[A\n",
            "Training:  14% 166/1164 [00:57<06:13,  2.67it/s]\u001b[A\n",
            "Training:  14% 167/1164 [00:58<06:17,  2.64it/s]\u001b[A\n",
            "Training:  14% 168/1164 [00:58<06:05,  2.72it/s]\u001b[A\n",
            "Training:  15% 169/1164 [00:59<06:10,  2.69it/s]\u001b[A\n",
            "Training:  15% 170/1164 [00:59<06:04,  2.73it/s]\u001b[A\n",
            "Training:  15% 171/1164 [00:59<05:50,  2.83it/s]\u001b[A\n",
            "Training:  15% 172/1164 [01:00<05:43,  2.89it/s]\u001b[A\n",
            "Training:  15% 173/1164 [01:00<05:37,  2.94it/s]\u001b[A\n",
            "Training:  15% 174/1164 [01:00<05:33,  2.96it/s]\u001b[A\n",
            "Training:  15% 175/1164 [01:01<05:59,  2.75it/s]\u001b[A\n",
            "Training:  15% 176/1164 [01:01<05:54,  2.79it/s]\u001b[A\n",
            "Training:  15% 177/1164 [01:01<05:49,  2.83it/s]\u001b[A\n",
            "Training:  15% 178/1164 [01:02<05:52,  2.79it/s]\u001b[A\n",
            "Training:  15% 179/1164 [01:02<05:43,  2.87it/s]\u001b[A\n",
            "Training:  15% 180/1164 [01:02<05:37,  2.92it/s]\u001b[A\n",
            "Training:  16% 181/1164 [01:03<05:42,  2.87it/s]\u001b[A\n",
            "Training:  16% 182/1164 [01:03<05:32,  2.95it/s]\u001b[A\n",
            "Training:  16% 183/1164 [01:03<05:27,  3.00it/s]\u001b[A\n",
            "Training:  16% 184/1164 [01:04<05:32,  2.95it/s]\u001b[A\n",
            "Training:  16% 185/1164 [01:04<05:57,  2.74it/s]\u001b[A\n",
            "Training:  16% 186/1164 [01:05<06:00,  2.71it/s]\u001b[A\n",
            "Training:  16% 187/1164 [01:05<06:00,  2.71it/s]\u001b[A\n",
            "Training:  16% 188/1164 [01:05<06:07,  2.66it/s]\u001b[A\n",
            "Training:  16% 189/1164 [01:06<05:55,  2.74it/s]\u001b[A\n",
            "Training:  16% 190/1164 [01:06<05:56,  2.73it/s]\u001b[A\n",
            "Training:  16% 191/1164 [01:06<05:43,  2.83it/s]\u001b[A\n",
            "Training:  16% 192/1164 [01:07<05:49,  2.78it/s]\u001b[A\n",
            "Training:  17% 193/1164 [01:07<05:44,  2.82it/s]\u001b[A\n",
            "Training:  17% 194/1164 [01:07<05:35,  2.89it/s]\u001b[A\n",
            "Training:  17% 195/1164 [01:08<05:30,  2.93it/s]\u001b[A\n",
            "Training:  17% 196/1164 [01:08<05:44,  2.81it/s]\u001b[A\n",
            "Training:  17% 197/1164 [01:08<05:43,  2.81it/s]\u001b[A\n",
            "Training:  17% 198/1164 [01:09<05:49,  2.76it/s]\u001b[A\n",
            "Training:  17% 199/1164 [01:09<05:42,  2.82it/s]\u001b[A\n",
            "Training:  17% 200/1164 [01:09<05:28,  2.93it/s]\u001b[A\n",
            "Training:  17% 201/1164 [01:10<05:32,  2.90it/s]\u001b[A\n",
            "Training:  17% 202/1164 [01:10<05:30,  2.91it/s]\u001b[A\n",
            "Training:  17% 203/1164 [01:10<05:29,  2.92it/s]\u001b[A\n",
            "Training:  18% 204/1164 [01:11<05:34,  2.87it/s]\u001b[A\n",
            "Training:  18% 205/1164 [01:11<05:25,  2.95it/s]\u001b[A\n",
            "Training:  18% 206/1164 [01:11<05:27,  2.93it/s]\u001b[A\n",
            "Training:  18% 207/1164 [01:12<05:35,  2.85it/s]\u001b[A\n",
            "Training:  18% 208/1164 [01:12<05:34,  2.86it/s]\u001b[A\n",
            "Training:  18% 209/1164 [01:13<05:33,  2.86it/s]\u001b[A\n",
            "Training:  18% 210/1164 [01:13<05:37,  2.83it/s]\u001b[A\n",
            "Training:  18% 211/1164 [01:13<05:43,  2.78it/s]\u001b[A\n",
            "Training:  18% 212/1164 [01:14<05:40,  2.80it/s]\u001b[A\n",
            "Training:  18% 213/1164 [01:14<05:42,  2.77it/s]\u001b[A\n",
            "Training:  18% 214/1164 [01:14<05:38,  2.81it/s]\u001b[A\n",
            "Training:  18% 215/1164 [01:15<05:31,  2.86it/s]\u001b[A\n",
            "Training:  19% 216/1164 [01:15<05:33,  2.84it/s]\u001b[A\n",
            "Training:  19% 217/1164 [01:15<05:30,  2.86it/s]\u001b[A\n",
            "Training:  19% 218/1164 [01:16<05:30,  2.86it/s]\u001b[A\n",
            "Training:  19% 219/1164 [01:16<05:28,  2.88it/s]\u001b[A\n",
            "Training:  19% 220/1164 [01:16<05:25,  2.90it/s]\u001b[A\n",
            "Training:  19% 221/1164 [01:17<05:21,  2.93it/s]\u001b[A\n",
            "Training:  19% 222/1164 [01:17<05:27,  2.87it/s]\u001b[A\n",
            "Training:  19% 223/1164 [01:17<05:29,  2.86it/s]\u001b[A\n",
            "Training:  19% 224/1164 [01:18<05:35,  2.80it/s]\u001b[A\n",
            "Training:  19% 225/1164 [01:18<05:33,  2.82it/s]\u001b[A\n",
            "Training:  19% 226/1164 [01:19<05:37,  2.78it/s]\u001b[A\n",
            "Training:  20% 227/1164 [01:19<05:39,  2.76it/s]\u001b[A\n",
            "Training:  20% 228/1164 [01:19<05:36,  2.78it/s]\u001b[A\n",
            "Training:  20% 229/1164 [01:20<05:43,  2.73it/s]\u001b[A\n",
            "Training:  20% 230/1164 [01:20<05:42,  2.72it/s]\u001b[A\n",
            "Training:  20% 231/1164 [01:20<05:33,  2.80it/s]\u001b[A\n",
            "Training:  20% 232/1164 [01:21<05:27,  2.84it/s]\u001b[A\n",
            "Training:  20% 233/1164 [01:21<05:35,  2.77it/s]\u001b[A\n",
            "Training:  20% 234/1164 [01:21<05:30,  2.81it/s]\u001b[A\n",
            "Training:  20% 235/1164 [01:22<05:34,  2.78it/s]\u001b[A\n",
            "Training:  20% 236/1164 [01:22<05:32,  2.79it/s]\u001b[A\n",
            "Training:  20% 237/1164 [01:23<05:23,  2.87it/s]\u001b[A\n",
            "Training:  20% 238/1164 [01:23<05:24,  2.86it/s]\u001b[A\n",
            "Training:  21% 239/1164 [01:23<05:21,  2.88it/s]\u001b[A\n",
            "Training:  21% 240/1164 [01:24<05:29,  2.81it/s]\u001b[A\n",
            "Training:  21% 241/1164 [01:24<05:27,  2.82it/s]\u001b[A\n",
            "Training:  21% 242/1164 [01:24<05:34,  2.76it/s]\u001b[A\n",
            "Training:  21% 243/1164 [01:25<05:36,  2.73it/s]\u001b[A\n",
            "Training:  21% 244/1164 [01:25<05:24,  2.83it/s]\u001b[A\n",
            "Training:  21% 245/1164 [01:25<05:26,  2.82it/s]\u001b[A\n",
            "Training:  21% 246/1164 [01:26<05:40,  2.69it/s]\u001b[A\n",
            "Training:  21% 247/1164 [01:26<05:43,  2.67it/s]\u001b[A\n",
            "Training:  21% 248/1164 [01:27<05:35,  2.73it/s]\u001b[A\n",
            "Training:  21% 249/1164 [01:27<05:28,  2.78it/s]\u001b[A\n",
            "Training:  21% 250/1164 [01:27<05:36,  2.72it/s]\u001b[A\n",
            "Training:  22% 251/1164 [01:28<05:34,  2.73it/s]\u001b[A\n",
            "Training:  22% 252/1164 [01:28<05:33,  2.73it/s]\u001b[A\n",
            "Training:  22% 253/1164 [01:28<05:30,  2.76it/s]\u001b[A\n",
            "Training:  22% 254/1164 [01:29<05:19,  2.85it/s]\u001b[A\n",
            "Training:  22% 255/1164 [01:29<05:31,  2.74it/s]\u001b[A\n",
            "Training:  22% 256/1164 [01:29<05:28,  2.77it/s]\u001b[A\n",
            "Training:  22% 257/1164 [01:30<05:22,  2.82it/s]\u001b[A\n",
            "Training:  22% 258/1164 [01:30<05:17,  2.85it/s]\u001b[A\n",
            "Training:  22% 259/1164 [01:30<05:16,  2.86it/s]\u001b[A\n",
            "Training:  22% 260/1164 [01:31<05:12,  2.89it/s]\u001b[A\n",
            "Training:  22% 261/1164 [01:31<05:13,  2.88it/s]\u001b[A\n",
            "Training:  23% 262/1164 [01:31<05:11,  2.90it/s]\u001b[A\n",
            "Training:  23% 263/1164 [01:32<05:11,  2.89it/s]\u001b[A\n",
            "Training:  23% 264/1164 [01:32<05:15,  2.85it/s]\u001b[A\n",
            "Training:  23% 265/1164 [01:32<05:09,  2.91it/s]\u001b[A\n",
            "Training:  23% 266/1164 [01:33<05:21,  2.80it/s]\u001b[A\n",
            "Training:  23% 267/1164 [01:33<05:18,  2.82it/s]\u001b[A\n",
            "Training:  23% 268/1164 [01:34<05:13,  2.86it/s]\u001b[A\n",
            "Training:  23% 269/1164 [01:34<05:13,  2.85it/s]\u001b[A\n",
            "Training:  23% 270/1164 [01:34<05:12,  2.86it/s]\u001b[A\n",
            "Training:  23% 271/1164 [01:35<05:09,  2.89it/s]\u001b[A\n",
            "Training:  23% 272/1164 [01:35<05:06,  2.91it/s]\u001b[A\n",
            "Training:  23% 273/1164 [01:35<05:11,  2.86it/s]\u001b[A\n",
            "Training:  24% 274/1164 [01:36<05:10,  2.86it/s]\u001b[A\n",
            "Training:  24% 275/1164 [01:36<05:22,  2.75it/s]\u001b[A\n",
            "Training:  24% 276/1164 [01:36<05:26,  2.72it/s]\u001b[A\n",
            "Training:  24% 277/1164 [01:37<05:17,  2.79it/s]\u001b[A\n",
            "Training:  24% 278/1164 [01:37<05:12,  2.83it/s]\u001b[A\n",
            "Training:  24% 279/1164 [01:37<05:19,  2.77it/s]\u001b[A\n",
            "Training:  24% 280/1164 [01:38<05:18,  2.78it/s]\u001b[A\n",
            "Training:  24% 281/1164 [01:38<05:24,  2.72it/s]\u001b[A\n",
            "Training:  24% 282/1164 [01:39<05:14,  2.80it/s]\u001b[A\n",
            "Training:  24% 283/1164 [01:39<05:24,  2.71it/s]\u001b[A\n",
            "Training:  24% 284/1164 [01:39<05:46,  2.54it/s]\u001b[A\n",
            "Training:  24% 285/1164 [01:40<05:30,  2.66it/s]\u001b[A\n",
            "Training:  25% 286/1164 [01:40<05:25,  2.69it/s]\u001b[A\n",
            "Training:  25% 287/1164 [01:40<05:17,  2.76it/s]\u001b[A\n",
            "Training:  25% 288/1164 [01:41<05:07,  2.85it/s]\u001b[A\n",
            "Training:  25% 289/1164 [01:41<05:12,  2.80it/s]\u001b[A\n",
            "Training:  25% 290/1164 [01:42<05:17,  2.75it/s]\u001b[A\n",
            "Training:  25% 291/1164 [01:42<05:07,  2.84it/s]\u001b[A\n",
            "Training:  25% 292/1164 [01:42<05:17,  2.75it/s]\u001b[A\n",
            "Training:  25% 293/1164 [01:43<05:19,  2.73it/s]\u001b[A\n",
            "Training:  25% 294/1164 [01:43<05:35,  2.60it/s]\u001b[A\n",
            "Training:  25% 295/1164 [01:43<05:24,  2.68it/s]\u001b[A\n",
            "Training:  25% 296/1164 [01:44<05:25,  2.66it/s]\u001b[A\n",
            "Training:  26% 297/1164 [01:44<05:13,  2.77it/s]\u001b[A\n",
            "Training:  26% 298/1164 [01:44<05:03,  2.85it/s]\u001b[A\n",
            "Training:  26% 299/1164 [01:45<05:09,  2.80it/s]\u001b[A\n",
            "Training:  26% 300/1164 [01:45<05:00,  2.87it/s]\u001b[A\n",
            "Training:  26% 301/1164 [01:45<04:59,  2.88it/s]\u001b[A\n",
            "Training:  26% 302/1164 [01:46<04:58,  2.89it/s]\u001b[A\n",
            "Training:  26% 303/1164 [01:46<04:54,  2.93it/s]\u001b[A\n",
            "Training:  26% 304/1164 [01:46<04:50,  2.96it/s]\u001b[A\n",
            "Training:  26% 305/1164 [01:47<05:01,  2.85it/s]\u001b[A\n",
            "Training:  26% 306/1164 [01:47<05:12,  2.75it/s]\u001b[A\n",
            "Training:  26% 307/1164 [01:48<05:19,  2.68it/s]\u001b[A\n",
            "Training:  26% 308/1164 [01:48<05:07,  2.78it/s]\u001b[A\n",
            "Training:  27% 309/1164 [01:48<04:55,  2.89it/s]\u001b[A\n",
            "Training:  27% 310/1164 [01:49<04:58,  2.86it/s]\u001b[A\n",
            "Training:  27% 311/1164 [01:49<05:00,  2.84it/s]\u001b[A\n",
            "Training:  27% 312/1164 [01:49<05:03,  2.80it/s]\u001b[A\n",
            "Training:  27% 313/1164 [01:50<05:00,  2.84it/s]\u001b[A\n",
            "Training:  27% 314/1164 [01:50<04:53,  2.89it/s]\u001b[A\n",
            "Training:  27% 315/1164 [01:50<04:48,  2.94it/s]\u001b[A\n",
            "Training:  27% 316/1164 [01:51<05:00,  2.82it/s]\u001b[A\n",
            "Training:  27% 317/1164 [01:51<04:53,  2.89it/s]\u001b[A\n",
            "Training:  27% 318/1164 [01:51<04:49,  2.92it/s]\u001b[A\n",
            "Training:  27% 319/1164 [01:52<04:49,  2.92it/s]\u001b[A\n",
            "Training:  27% 320/1164 [01:52<04:46,  2.95it/s]\u001b[A\n",
            "Training:  28% 321/1164 [01:52<04:51,  2.90it/s]\u001b[A\n",
            "Training:  28% 322/1164 [01:53<04:48,  2.92it/s]\u001b[A\n",
            "Training:  28% 323/1164 [01:53<04:57,  2.82it/s]\u001b[A\n",
            "Training:  28% 324/1164 [01:54<05:02,  2.78it/s]\u001b[A\n",
            "Training:  28% 325/1164 [01:54<05:04,  2.76it/s]\u001b[A\n",
            "Training:  28% 326/1164 [01:54<04:56,  2.83it/s]\u001b[A\n",
            "Training:  28% 327/1164 [01:55<04:54,  2.85it/s]\u001b[A\n",
            "Training:  28% 328/1164 [01:55<04:57,  2.81it/s]\u001b[A\n",
            "Training:  28% 329/1164 [01:55<04:55,  2.83it/s]\u001b[A\n",
            "Training:  28% 330/1164 [01:56<04:53,  2.84it/s]\u001b[A\n",
            "Training:  28% 331/1164 [01:56<05:30,  2.52it/s]\u001b[A\n",
            "Training:  29% 332/1164 [01:56<05:18,  2.61it/s]\u001b[A\n",
            "Training:  29% 333/1164 [01:57<05:01,  2.76it/s]\u001b[A\n",
            "Training:  29% 334/1164 [01:57<04:52,  2.84it/s]\u001b[A\n",
            "Training:  29% 335/1164 [01:58<04:57,  2.79it/s]\u001b[A\n",
            "Training:  29% 336/1164 [01:58<05:01,  2.75it/s]\u001b[A\n",
            "Training:  29% 337/1164 [01:58<05:05,  2.71it/s]\u001b[A\n",
            "Training:  29% 338/1164 [01:59<05:02,  2.73it/s]\u001b[A\n",
            "Training:  29% 339/1164 [01:59<04:55,  2.79it/s]\u001b[A\n",
            "Training:  29% 340/1164 [01:59<04:46,  2.88it/s]\u001b[A\n",
            "Training:  29% 341/1164 [02:00<04:42,  2.91it/s]\u001b[A\n",
            "Training:  29% 342/1164 [02:00<04:43,  2.90it/s]\u001b[A\n",
            "Training:  29% 343/1164 [02:00<05:00,  2.73it/s]\u001b[A\n",
            "Training:  30% 344/1164 [02:01<04:54,  2.78it/s]\u001b[A\n",
            "Training:  30% 345/1164 [02:01<04:47,  2.85it/s]\u001b[A\n",
            "Training:  30% 346/1164 [02:01<04:47,  2.84it/s]\u001b[A\n",
            "Training:  30% 347/1164 [02:02<04:48,  2.83it/s]\u001b[A\n",
            "Training:  30% 348/1164 [02:02<04:40,  2.91it/s]\u001b[A\n",
            "Training:  30% 349/1164 [02:02<04:44,  2.86it/s]\u001b[A\n",
            "Training:  30% 350/1164 [02:03<04:45,  2.85it/s]\u001b[A\n",
            "Training:  30% 351/1164 [02:03<04:49,  2.81it/s]\u001b[A\n",
            "Training:  30% 352/1164 [02:04<04:56,  2.74it/s]\u001b[A\n",
            "Training:  30% 353/1164 [02:04<04:48,  2.81it/s]\u001b[A\n",
            "Training:  30% 354/1164 [02:04<04:46,  2.83it/s]\u001b[A\n",
            "Training:  30% 355/1164 [02:05<04:54,  2.75it/s]\u001b[A\n",
            "Training:  31% 356/1164 [02:05<04:54,  2.74it/s]\u001b[A\n",
            "Training:  31% 357/1164 [02:05<04:50,  2.78it/s]\u001b[A\n",
            "Training:  31% 358/1164 [02:06<04:42,  2.86it/s]\u001b[A\n",
            "Training:  31% 359/1164 [02:06<04:37,  2.90it/s]\u001b[A\n",
            "Training:  31% 360/1164 [02:06<04:38,  2.88it/s]\u001b[A\n",
            "Training:  31% 361/1164 [02:07<04:36,  2.91it/s]\u001b[A\n",
            "Training:  31% 362/1164 [02:07<04:32,  2.95it/s]\u001b[A\n",
            "Training:  31% 363/1164 [02:07<04:29,  2.98it/s]\u001b[A\n",
            "Training:  31% 364/1164 [02:08<04:27,  2.99it/s]\u001b[A\n",
            "Training:  31% 365/1164 [02:08<04:37,  2.88it/s]\u001b[A\n",
            "Training:  31% 366/1164 [02:08<04:50,  2.75it/s]\u001b[A\n",
            "Training:  32% 367/1164 [02:09<04:45,  2.79it/s]\u001b[A\n",
            "Training:  32% 368/1164 [02:09<04:49,  2.75it/s]\u001b[A\n",
            "Training:  32% 369/1164 [02:10<05:01,  2.64it/s]\u001b[A\n",
            "Training:  32% 370/1164 [02:10<04:59,  2.65it/s]\u001b[A\n",
            "Training:  32% 371/1164 [02:10<04:57,  2.67it/s]\u001b[A\n",
            "Training:  32% 372/1164 [02:11<04:51,  2.72it/s]\u001b[A\n",
            "Training:  32% 373/1164 [02:11<04:40,  2.82it/s]\u001b[A\n",
            "Training:  32% 374/1164 [02:11<04:35,  2.86it/s]\u001b[A\n",
            "Training:  32% 375/1164 [02:12<04:38,  2.84it/s]\u001b[A\n",
            "Training:  32% 376/1164 [02:12<04:33,  2.88it/s]\u001b[A\n",
            "Training:  32% 377/1164 [02:12<04:28,  2.93it/s]\u001b[A\n",
            "Training:  32% 378/1164 [02:13<04:36,  2.84it/s]\u001b[A\n",
            "Training:  33% 379/1164 [02:13<04:32,  2.88it/s]\u001b[A\n",
            "Training:  33% 380/1164 [02:13<04:41,  2.78it/s]\u001b[A\n",
            "Training:  33% 381/1164 [02:14<04:39,  2.80it/s]\u001b[A\n",
            "Training:  33% 382/1164 [02:14<04:34,  2.85it/s]\u001b[A\n",
            "Training:  33% 383/1164 [02:15<04:36,  2.83it/s]\u001b[A\n",
            "Training:  33% 384/1164 [02:15<04:31,  2.87it/s]\u001b[A\n",
            "Training:  33% 385/1164 [02:15<04:32,  2.86it/s]\u001b[A\n",
            "Training:  33% 386/1164 [02:16<04:29,  2.89it/s]\u001b[A\n",
            "Training:  33% 387/1164 [02:16<04:26,  2.92it/s]\u001b[A\n",
            "Training:  33% 388/1164 [02:16<04:31,  2.86it/s]\u001b[A\n",
            "Training:  33% 389/1164 [02:17<04:41,  2.75it/s]\u001b[A\n",
            "Training:  34% 390/1164 [02:17<04:37,  2.79it/s]\u001b[A\n",
            "Training:  34% 391/1164 [02:17<04:37,  2.79it/s]\u001b[A\n",
            "Training:  34% 392/1164 [02:18<04:30,  2.85it/s]\u001b[A\n",
            "Training:  34% 393/1164 [02:18<04:27,  2.88it/s]\u001b[A\n",
            "Training:  34% 394/1164 [02:18<04:25,  2.90it/s]\u001b[A\n",
            "Training:  34% 395/1164 [02:19<04:32,  2.82it/s]\u001b[A\n",
            "Training:  34% 396/1164 [02:19<04:38,  2.76it/s]\u001b[A\n",
            "Training:  34% 397/1164 [02:20<04:43,  2.70it/s]\u001b[A\n",
            "Training:  34% 398/1164 [02:20<04:50,  2.64it/s]\u001b[A\n",
            "Training:  34% 399/1164 [02:20<04:55,  2.59it/s]\u001b[A\n",
            "Training:  34% 400/1164 [02:21<04:41,  2.71it/s]\u001b[A\n",
            "Training:  34% 401/1164 [02:21<04:42,  2.70it/s]\u001b[A\n",
            "Training:  35% 402/1164 [02:21<04:44,  2.68it/s]\u001b[A\n",
            "Training:  35% 403/1164 [02:22<04:48,  2.64it/s]\u001b[A\n",
            "Training:  35% 404/1164 [02:22<05:01,  2.52it/s]\u001b[A\n",
            "Training:  35% 405/1164 [02:23<04:55,  2.57it/s]\u001b[A\n",
            "Training:  35% 406/1164 [02:23<04:49,  2.61it/s]\u001b[A\n",
            "Training:  35% 407/1164 [02:23<04:47,  2.64it/s]\u001b[A\n",
            "Training:  35% 408/1164 [02:24<04:39,  2.70it/s]\u001b[A\n",
            "Training:  35% 409/1164 [02:24<04:35,  2.74it/s]\u001b[A\n",
            "Training:  35% 410/1164 [02:24<04:41,  2.68it/s]\u001b[A\n",
            "Training:  35% 411/1164 [02:25<04:37,  2.72it/s]\u001b[A\n",
            "Training:  35% 412/1164 [02:25<04:41,  2.67it/s]\u001b[A\n",
            "Training:  35% 413/1164 [02:26<04:32,  2.76it/s]\u001b[A\n",
            "Training:  36% 414/1164 [02:26<04:27,  2.81it/s]\u001b[A\n",
            "Training:  36% 415/1164 [02:26<04:23,  2.85it/s]\u001b[A\n",
            "Training:  36% 416/1164 [02:27<04:22,  2.85it/s]\u001b[A\n",
            "Training:  36% 417/1164 [02:27<04:23,  2.83it/s]\u001b[A\n",
            "Training:  36% 418/1164 [02:27<04:18,  2.88it/s]\u001b[A\n",
            "Training:  36% 419/1164 [02:28<04:11,  2.96it/s]\u001b[A\n",
            "Training:  36% 420/1164 [02:28<04:09,  2.98it/s]\u001b[A\n",
            "Training:  36% 421/1164 [02:28<04:13,  2.93it/s]\u001b[A\n",
            "Training:  36% 422/1164 [02:29<04:15,  2.91it/s]\u001b[A\n",
            "Training:  36% 423/1164 [02:29<04:12,  2.94it/s]\u001b[A\n",
            "Training:  36% 424/1164 [02:29<04:16,  2.89it/s]\u001b[A\n",
            "Training:  37% 425/1164 [02:30<04:12,  2.92it/s]\u001b[A\n",
            "Training:  37% 426/1164 [02:30<04:08,  2.97it/s]\u001b[A\n",
            "Training:  37% 427/1164 [02:30<04:20,  2.83it/s]\u001b[A\n",
            "Training:  37% 428/1164 [02:31<04:27,  2.75it/s]\u001b[A\n",
            "Training:  37% 429/1164 [02:31<04:18,  2.85it/s]\u001b[A\n",
            "Training:  37% 430/1164 [02:31<04:17,  2.85it/s]\u001b[A\n",
            "Training:  37% 431/1164 [02:32<04:14,  2.87it/s]\u001b[A\n",
            "Training:  37% 432/1164 [02:32<04:05,  2.98it/s]\u001b[A\n",
            "Training:  37% 433/1164 [02:32<04:06,  2.97it/s]\u001b[A\n",
            "Training:  37% 434/1164 [02:33<04:15,  2.85it/s]\u001b[A\n",
            "Training:  37% 435/1164 [02:33<04:11,  2.90it/s]\u001b[A\n",
            "Training:  37% 436/1164 [02:33<04:18,  2.81it/s]\u001b[A\n",
            "Training:  38% 437/1164 [02:34<04:24,  2.75it/s]\u001b[A\n",
            "Training:  38% 438/1164 [02:34<04:19,  2.80it/s]\u001b[A\n",
            "Training:  38% 439/1164 [02:35<04:11,  2.88it/s]\u001b[A\n",
            "Training:  38% 440/1164 [02:35<04:07,  2.92it/s]\u001b[A\n",
            "Training:  38% 441/1164 [02:35<04:08,  2.90it/s]\u001b[A\n",
            "Training:  38% 442/1164 [02:36<04:20,  2.77it/s]\u001b[A\n",
            "Training:  38% 443/1164 [02:36<04:11,  2.86it/s]\u001b[A\n",
            "Training:  38% 444/1164 [02:36<04:18,  2.78it/s]\u001b[A\n",
            "Training:  38% 445/1164 [02:37<04:29,  2.67it/s]\u001b[A\n",
            "Training:  38% 446/1164 [02:37<04:15,  2.81it/s]\u001b[A\n",
            "Training:  38% 447/1164 [02:37<04:12,  2.84it/s]\u001b[A\n",
            "Training:  38% 448/1164 [02:38<04:11,  2.84it/s]\u001b[A\n",
            "Training:  39% 449/1164 [02:38<04:09,  2.86it/s]\u001b[A\n",
            "Training:  39% 450/1164 [02:38<04:09,  2.86it/s]\u001b[A\n",
            "Training:  39% 451/1164 [02:39<04:07,  2.89it/s]\u001b[A\n",
            "Training:  39% 452/1164 [02:39<04:03,  2.93it/s]\u001b[A\n",
            "Training:  39% 453/1164 [02:39<04:02,  2.93it/s]\u001b[A\n",
            "Training:  39% 454/1164 [02:40<04:01,  2.94it/s]\u001b[A\n",
            "Training:  39% 455/1164 [02:40<03:57,  2.99it/s]\u001b[A\n",
            "Training:  39% 456/1164 [02:40<03:55,  3.00it/s]\u001b[A\n",
            "Training:  39% 457/1164 [02:41<03:49,  3.08it/s]\u001b[A\n",
            "Training:  39% 458/1164 [02:41<04:02,  2.91it/s]\u001b[A\n",
            "Training:  39% 459/1164 [02:41<04:03,  2.90it/s]\u001b[A\n",
            "Training:  40% 460/1164 [02:42<04:03,  2.89it/s]\u001b[A\n",
            "Training:  40% 461/1164 [02:42<04:10,  2.80it/s]\u001b[A\n",
            "Training:  40% 462/1164 [02:43<04:08,  2.83it/s]\u001b[A\n",
            "Training:  40% 463/1164 [02:43<04:00,  2.91it/s]\u001b[A\n",
            "Training:  40% 464/1164 [02:43<03:59,  2.92it/s]\u001b[A\n",
            "Training:  40% 465/1164 [02:44<04:03,  2.88it/s]\u001b[A\n",
            "Training:  40% 466/1164 [02:44<04:02,  2.88it/s]\u001b[A\n",
            "Training:  40% 467/1164 [02:44<04:01,  2.88it/s]\u001b[A\n",
            "Training:  40% 468/1164 [02:45<04:02,  2.88it/s]\u001b[A\n",
            "Training:  40% 469/1164 [02:45<04:03,  2.86it/s]\u001b[A\n",
            "Training:  40% 470/1164 [02:45<04:02,  2.86it/s]\u001b[A\n",
            "Training:  40% 471/1164 [02:46<04:05,  2.82it/s]\u001b[A\n",
            "Training:  41% 472/1164 [02:46<03:59,  2.89it/s]\u001b[A\n",
            "Training:  41% 473/1164 [02:46<03:57,  2.91it/s]\u001b[A\n",
            "Training:  41% 474/1164 [02:47<03:57,  2.91it/s]\u001b[A\n",
            "Training:  41% 475/1164 [02:47<03:58,  2.89it/s]\u001b[A\n",
            "Training:  41% 476/1164 [02:47<03:54,  2.93it/s]\u001b[A\n",
            "Training:  41% 477/1164 [02:48<04:09,  2.76it/s]\u001b[A\n",
            "Training:  41% 478/1164 [02:48<04:15,  2.68it/s]\u001b[A\n",
            "Training:  41% 479/1164 [02:49<04:12,  2.71it/s]\u001b[A\n",
            "Training:  41% 480/1164 [02:49<04:07,  2.76it/s]\u001b[A\n",
            "Training:  41% 481/1164 [02:49<04:12,  2.70it/s]\u001b[A\n",
            "Training:  41% 482/1164 [02:50<04:03,  2.80it/s]\u001b[A\n",
            "Training:  41% 483/1164 [02:50<04:04,  2.79it/s]\u001b[A\n",
            "Training:  42% 484/1164 [02:50<04:03,  2.79it/s]\u001b[A\n",
            "Training:  42% 485/1164 [02:51<03:59,  2.84it/s]\u001b[A\n",
            "Training:  42% 486/1164 [02:51<04:03,  2.79it/s]\u001b[A\n",
            "Training:  42% 487/1164 [02:51<04:08,  2.72it/s]\u001b[A\n",
            "Training:  42% 488/1164 [02:52<04:03,  2.78it/s]\u001b[A\n",
            "Training:  42% 489/1164 [02:52<03:54,  2.87it/s]\u001b[A\n",
            "Training:  42% 490/1164 [02:52<03:54,  2.87it/s]\u001b[A\n",
            "Training:  42% 491/1164 [02:53<03:49,  2.94it/s]\u001b[A\n",
            "Training:  42% 492/1164 [02:53<04:09,  2.70it/s]\u001b[A\n",
            "Training:  42% 493/1164 [02:54<04:11,  2.67it/s]\u001b[A\n",
            "Training:  42% 494/1164 [02:54<04:08,  2.70it/s]\u001b[A\n",
            "Training:  43% 495/1164 [02:54<04:06,  2.71it/s]\u001b[A\n",
            "Training:  43% 496/1164 [02:55<04:01,  2.76it/s]\u001b[A\n",
            "Training:  43% 497/1164 [02:55<04:02,  2.75it/s]\u001b[A\n",
            "Training:  43% 498/1164 [02:55<03:58,  2.79it/s]\u001b[A\n",
            "Training:  43% 499/1164 [02:56<03:58,  2.79it/s]\u001b[A\n",
            "Training:  43% 500/1164 [02:56<03:55,  2.82it/s]\u001b[A\n",
            "Training:  43% 501/1164 [02:56<03:49,  2.89it/s]\u001b[A\n",
            "Training:  43% 502/1164 [02:57<03:49,  2.89it/s]\u001b[A\n",
            "Training:  43% 503/1164 [02:57<03:45,  2.93it/s]\u001b[A\n",
            "Training:  43% 504/1164 [02:57<03:48,  2.89it/s]\u001b[A\n",
            "Training:  43% 505/1164 [02:58<03:50,  2.85it/s]\u001b[A\n",
            "Training:  43% 506/1164 [02:58<03:50,  2.86it/s]\u001b[A\n",
            "Training:  44% 507/1164 [02:58<03:43,  2.94it/s]\u001b[A\n",
            "Training:  44% 508/1164 [02:59<03:54,  2.80it/s]\u001b[A\n",
            "Training:  44% 509/1164 [02:59<03:48,  2.87it/s]\u001b[A\n",
            "Training:  44% 510/1164 [03:00<03:49,  2.85it/s]\u001b[A\n",
            "Training:  44% 511/1164 [03:00<03:46,  2.88it/s]\u001b[A\n",
            "Training:  44% 512/1164 [03:00<03:48,  2.86it/s]\u001b[A\n",
            "Training:  44% 513/1164 [03:01<03:49,  2.83it/s]\u001b[A\n",
            "Training:  44% 514/1164 [03:01<03:54,  2.78it/s]\u001b[A\n",
            "Training:  44% 515/1164 [03:01<03:54,  2.77it/s]\u001b[A\n",
            "Training:  44% 516/1164 [03:02<04:10,  2.58it/s]\u001b[A\n",
            "Training:  44% 517/1164 [03:02<04:05,  2.63it/s]\u001b[A\n",
            "Training:  45% 518/1164 [03:02<04:00,  2.69it/s]\u001b[A\n",
            "Training:  45% 519/1164 [03:03<04:00,  2.68it/s]\u001b[A\n",
            "Training:  45% 520/1164 [03:03<03:50,  2.79it/s]\u001b[A\n",
            "Training:  45% 521/1164 [03:04<03:47,  2.83it/s]\u001b[A\n",
            "Training:  45% 522/1164 [03:04<04:06,  2.60it/s]\u001b[A\n",
            "Training:  45% 523/1164 [03:04<04:00,  2.66it/s]\u001b[A\n",
            "Training:  45% 524/1164 [03:05<03:58,  2.68it/s]\u001b[A\n",
            "Training:  45% 525/1164 [03:05<03:56,  2.70it/s]\u001b[A\n",
            "Training:  45% 526/1164 [03:05<03:46,  2.82it/s]\u001b[A\n",
            "Training:  45% 527/1164 [03:06<03:46,  2.82it/s]\u001b[A\n",
            "Training:  45% 528/1164 [03:06<03:44,  2.84it/s]\u001b[A\n",
            "Training:  45% 529/1164 [03:06<03:35,  2.95it/s]\u001b[A\n",
            "Training:  46% 530/1164 [03:07<03:35,  2.94it/s]\u001b[A\n",
            "Training:  46% 531/1164 [03:07<03:37,  2.92it/s]\u001b[A\n",
            "Training:  46% 532/1164 [03:07<03:38,  2.89it/s]\u001b[A\n",
            "Training:  46% 533/1164 [03:08<03:49,  2.75it/s]\u001b[A\n",
            "Training:  46% 534/1164 [03:08<03:38,  2.88it/s]\u001b[A\n",
            "Training:  46% 535/1164 [03:09<03:59,  2.63it/s]\u001b[A\n",
            "Training:  46% 536/1164 [03:09<03:53,  2.68it/s]\u001b[A\n",
            "Training:  46% 537/1164 [03:09<03:50,  2.72it/s]\u001b[A\n",
            "Training:  46% 538/1164 [03:10<03:47,  2.75it/s]\u001b[A\n",
            "Training:  46% 539/1164 [03:10<03:55,  2.66it/s]\u001b[A\n",
            "Training:  46% 540/1164 [03:10<03:47,  2.74it/s]\u001b[A\n",
            "Training:  46% 541/1164 [03:11<03:42,  2.80it/s]\u001b[A\n",
            "Training:  47% 542/1164 [03:11<03:46,  2.74it/s]\u001b[A\n",
            "Training:  47% 543/1164 [03:11<03:39,  2.83it/s]\u001b[A\n",
            "Training:  47% 544/1164 [03:12<03:40,  2.82it/s]\u001b[A\n",
            "Training:  47% 545/1164 [03:12<03:38,  2.83it/s]\u001b[A\n",
            "Training:  47% 546/1164 [03:13<03:37,  2.85it/s]\u001b[A\n",
            "Training:  47% 547/1164 [03:13<03:39,  2.81it/s]\u001b[A\n",
            "Training:  47% 548/1164 [03:13<03:31,  2.91it/s]\u001b[A\n",
            "Training:  47% 549/1164 [03:14<03:31,  2.91it/s]\u001b[A\n",
            "Training:  47% 550/1164 [03:14<03:39,  2.80it/s]\u001b[A\n",
            "Training:  47% 551/1164 [03:14<03:29,  2.93it/s]\u001b[A\n",
            "Training:  47% 552/1164 [03:15<03:29,  2.92it/s]\u001b[A\n",
            "Training:  48% 553/1164 [03:15<03:29,  2.92it/s]\u001b[A\n",
            "Training:  48% 554/1164 [03:15<03:30,  2.90it/s]\u001b[A\n",
            "Training:  48% 555/1164 [03:16<03:33,  2.85it/s]\u001b[A\n",
            "Training:  48% 556/1164 [03:16<03:35,  2.82it/s]\u001b[A\n",
            "Training:  48% 557/1164 [03:16<03:33,  2.84it/s]\u001b[A\n",
            "Training:  48% 558/1164 [03:17<03:30,  2.88it/s]\u001b[A\n",
            "Training:  48% 559/1164 [03:17<03:28,  2.91it/s]\u001b[A\n",
            "Training:  48% 560/1164 [03:17<03:30,  2.86it/s]\u001b[A\n",
            "Training:  48% 561/1164 [03:18<03:37,  2.77it/s]\u001b[A\n",
            "Training:  48% 562/1164 [03:18<03:44,  2.69it/s]\u001b[A\n",
            "Training:  48% 563/1164 [03:19<03:39,  2.74it/s]\u001b[A\n",
            "Training:  48% 564/1164 [03:19<03:31,  2.83it/s]\u001b[A\n",
            "Training:  49% 565/1164 [03:19<03:30,  2.84it/s]\u001b[A\n",
            "Training:  49% 566/1164 [03:20<03:36,  2.76it/s]\u001b[A\n",
            "Training:  49% 567/1164 [03:20<03:32,  2.81it/s]\u001b[A\n",
            "Training:  49% 568/1164 [03:20<03:38,  2.73it/s]\u001b[A\n",
            "Training:  49% 569/1164 [03:21<03:34,  2.78it/s]\u001b[A\n",
            "Training:  49% 570/1164 [03:21<03:33,  2.78it/s]\u001b[A\n",
            "Training:  49% 571/1164 [03:21<03:27,  2.86it/s]\u001b[A\n",
            "Training:  49% 572/1164 [03:22<03:29,  2.83it/s]\u001b[A\n",
            "Training:  49% 573/1164 [03:22<03:27,  2.85it/s]\u001b[A\n",
            "Training:  49% 574/1164 [03:22<03:25,  2.87it/s]\u001b[A\n",
            "Training:  49% 575/1164 [03:23<03:18,  2.96it/s]\u001b[A\n",
            "Training:  49% 576/1164 [03:23<03:16,  2.99it/s]\u001b[A\n",
            "Training:  50% 577/1164 [03:23<03:16,  2.98it/s]\u001b[A\n",
            "Training:  50% 578/1164 [03:24<03:17,  2.96it/s]\u001b[A\n",
            "Training:  50% 579/1164 [03:24<03:27,  2.82it/s]\u001b[A\n",
            "Training:  50% 580/1164 [03:24<03:29,  2.79it/s]\u001b[A\n",
            "Training:  50% 581/1164 [03:25<03:32,  2.75it/s]\u001b[A\n",
            "Training:  50% 582/1164 [03:25<03:28,  2.79it/s]\u001b[A\n",
            "Training:  50% 583/1164 [03:26<03:27,  2.81it/s]\u001b[A\n",
            "Training:  50% 584/1164 [03:26<03:30,  2.75it/s]\u001b[A\n",
            "Training:  50% 585/1164 [03:26<03:30,  2.76it/s]\u001b[A\n",
            "Training:  50% 586/1164 [03:27<03:34,  2.69it/s]\u001b[A\n",
            "Training:  50% 587/1164 [03:27<03:24,  2.82it/s]\u001b[A\n",
            "Training:  51% 588/1164 [03:27<03:24,  2.81it/s]\u001b[A\n",
            "Training:  51% 589/1164 [03:28<03:31,  2.72it/s]\u001b[A\n",
            "Training:  51% 590/1164 [03:28<03:28,  2.75it/s]\u001b[A\n",
            "Training:  51% 591/1164 [03:28<03:26,  2.77it/s]\u001b[A\n",
            "Training:  51% 592/1164 [03:29<03:20,  2.86it/s]\u001b[A\n",
            "Training:  51% 593/1164 [03:29<03:16,  2.90it/s]\u001b[A\n",
            "Training:  51% 594/1164 [03:30<03:26,  2.76it/s]\u001b[A\n",
            "Training:  51% 595/1164 [03:30<03:34,  2.66it/s]\u001b[A\n",
            "Training:  51% 596/1164 [03:30<03:26,  2.75it/s]\u001b[A\n",
            "Training:  51% 597/1164 [03:31<03:17,  2.87it/s]\u001b[A\n",
            "Training:  51% 598/1164 [03:31<03:29,  2.70it/s]\u001b[A\n",
            "Training:  51% 599/1164 [03:31<03:27,  2.72it/s]\u001b[A\n",
            "Training:  52% 600/1164 [03:32<03:25,  2.75it/s]\u001b[A\n",
            "Training:  52% 601/1164 [03:32<03:19,  2.83it/s]\u001b[A\n",
            "Training:  52% 602/1164 [03:32<03:13,  2.90it/s]\u001b[A\n",
            "Training:  52% 603/1164 [03:33<03:18,  2.82it/s]\u001b[A\n",
            "Training:  52% 604/1164 [03:33<03:16,  2.85it/s]\u001b[A\n",
            "Training:  52% 605/1164 [03:33<03:23,  2.75it/s]\u001b[A\n",
            "Training:  52% 606/1164 [03:34<03:26,  2.71it/s]\u001b[A\n",
            "Training:  52% 607/1164 [03:34<03:25,  2.71it/s]\u001b[A\n",
            "Training:  52% 608/1164 [03:35<03:19,  2.79it/s]\u001b[A\n",
            "Training:  52% 609/1164 [03:35<03:22,  2.74it/s]\u001b[A\n",
            "Training:  52% 610/1164 [03:35<03:20,  2.76it/s]\u001b[A\n",
            "Training:  52% 611/1164 [03:36<03:20,  2.75it/s]\u001b[A\n",
            "Training:  53% 612/1164 [03:36<03:16,  2.81it/s]\u001b[A\n",
            "Training:  53% 613/1164 [03:36<03:13,  2.84it/s]\u001b[A\n",
            "Training:  53% 614/1164 [03:37<03:08,  2.91it/s]\u001b[A\n",
            "Training:  53% 615/1164 [03:37<03:04,  2.97it/s]\u001b[A\n",
            "Training:  53% 616/1164 [03:37<03:07,  2.92it/s]\u001b[A\n",
            "Training:  53% 617/1164 [03:38<03:12,  2.85it/s]\u001b[A\n",
            "Training:  53% 618/1164 [03:38<03:16,  2.78it/s]\u001b[A\n",
            "Training:  53% 619/1164 [03:38<03:17,  2.75it/s]\u001b[A\n",
            "Training:  53% 620/1164 [03:39<03:19,  2.73it/s]\u001b[A\n",
            "Training:  53% 621/1164 [03:39<03:19,  2.72it/s]\u001b[A\n",
            "Training:  53% 622/1164 [03:40<03:12,  2.81it/s]\u001b[A\n",
            "Training:  54% 623/1164 [03:40<03:12,  2.81it/s]\u001b[A\n",
            "Training:  54% 624/1164 [03:40<03:09,  2.85it/s]\u001b[A\n",
            "Training:  54% 625/1164 [03:41<03:06,  2.88it/s]\u001b[A\n",
            "Training:  54% 626/1164 [03:41<03:08,  2.85it/s]\u001b[A\n",
            "Training:  54% 627/1164 [03:41<03:17,  2.72it/s]\u001b[A\n",
            "Training:  54% 628/1164 [03:42<03:18,  2.70it/s]\u001b[A\n",
            "Training:  54% 629/1164 [03:42<03:15,  2.74it/s]\u001b[A\n",
            "Training:  54% 630/1164 [03:42<03:11,  2.78it/s]\u001b[A\n",
            "Training:  54% 631/1164 [03:43<03:11,  2.78it/s]\u001b[A\n",
            "Training:  54% 632/1164 [03:43<03:22,  2.62it/s]\u001b[A\n",
            "Training:  54% 633/1164 [03:44<03:13,  2.74it/s]\u001b[A\n",
            "Training:  54% 634/1164 [03:44<03:20,  2.64it/s]\u001b[A\n",
            "Training:  55% 635/1164 [03:44<03:17,  2.68it/s]\u001b[A\n",
            "Training:  55% 636/1164 [03:45<03:11,  2.76it/s]\u001b[A\n",
            "Training:  55% 637/1164 [03:45<03:09,  2.78it/s]\u001b[A\n",
            "Training:  55% 638/1164 [03:45<03:12,  2.74it/s]\u001b[A\n",
            "Training:  55% 639/1164 [03:46<03:05,  2.84it/s]\u001b[A\n",
            "Training:  55% 640/1164 [03:46<03:05,  2.83it/s]\u001b[A\n",
            "Training:  55% 641/1164 [03:46<03:00,  2.89it/s]\u001b[A\n",
            "Training:  55% 642/1164 [03:47<03:05,  2.81it/s]\u001b[A\n",
            "Training:  55% 643/1164 [03:47<03:07,  2.78it/s]\u001b[A\n",
            "Training:  55% 644/1164 [03:47<03:04,  2.81it/s]\u001b[A\n",
            "Training:  55% 645/1164 [03:48<03:03,  2.83it/s]\u001b[A\n",
            "Training:  55% 646/1164 [03:48<03:00,  2.87it/s]\u001b[A\n",
            "Training:  56% 647/1164 [03:49<03:02,  2.84it/s]\u001b[A\n",
            "Training:  56% 648/1164 [03:49<03:02,  2.82it/s]\u001b[A\n",
            "Training:  56% 649/1164 [03:49<03:04,  2.79it/s]\u001b[A\n",
            "Training:  56% 650/1164 [03:50<03:01,  2.84it/s]\u001b[A\n",
            "Training:  56% 651/1164 [03:50<02:59,  2.86it/s]\u001b[A\n",
            "Training:  56% 652/1164 [03:50<02:59,  2.85it/s]\u001b[A\n",
            "Training:  56% 653/1164 [03:51<02:55,  2.92it/s]\u001b[A\n",
            "Training:  56% 654/1164 [03:51<03:01,  2.82it/s]\u001b[A\n",
            "Training:  56% 655/1164 [03:51<02:57,  2.86it/s]\u001b[A\n",
            "Training:  56% 656/1164 [03:52<02:57,  2.86it/s]\u001b[A\n",
            "Training:  56% 657/1164 [03:52<02:57,  2.86it/s]\u001b[A\n",
            "Training:  57% 658/1164 [03:52<03:00,  2.81it/s]\u001b[A\n",
            "Training:  57% 659/1164 [03:53<03:00,  2.79it/s]\u001b[A\n",
            "Training:  57% 660/1164 [03:53<02:59,  2.80it/s]\u001b[A\n",
            "Training:  57% 661/1164 [03:53<02:58,  2.82it/s]\u001b[A\n",
            "Training:  57% 662/1164 [03:54<02:55,  2.87it/s]\u001b[A\n",
            "Training:  57% 663/1164 [03:54<02:56,  2.84it/s]\u001b[A\n",
            "Training:  57% 664/1164 [03:55<03:03,  2.73it/s]\u001b[A\n",
            "Training:  57% 665/1164 [03:55<02:57,  2.81it/s]\u001b[A\n",
            "Training:  57% 666/1164 [03:55<02:53,  2.87it/s]\u001b[A\n",
            "Training:  57% 667/1164 [03:56<02:57,  2.79it/s]\u001b[A\n",
            "Training:  57% 668/1164 [03:56<02:55,  2.82it/s]\u001b[A\n",
            "Training:  57% 669/1164 [03:56<02:58,  2.78it/s]\u001b[A\n",
            "Training:  58% 670/1164 [03:57<02:54,  2.82it/s]\u001b[A\n",
            "Training:  58% 671/1164 [03:57<02:52,  2.86it/s]\u001b[A\n",
            "Training:  58% 672/1164 [03:57<02:47,  2.94it/s]\u001b[A\n",
            "Training:  58% 673/1164 [03:58<02:51,  2.86it/s]\u001b[A\n",
            "Training:  58% 674/1164 [03:58<02:48,  2.91it/s]\u001b[A\n",
            "Training:  58% 675/1164 [03:58<02:49,  2.88it/s]\u001b[A\n",
            "Training:  58% 676/1164 [03:59<02:51,  2.85it/s]\u001b[A\n",
            "Training:  58% 677/1164 [03:59<02:49,  2.87it/s]\u001b[A\n",
            "Training:  58% 678/1164 [03:59<02:51,  2.83it/s]\u001b[A\n",
            "Training:  58% 679/1164 [04:00<02:55,  2.76it/s]\u001b[A\n",
            "Training:  58% 680/1164 [04:00<02:49,  2.86it/s]\u001b[A\n",
            "Training:  59% 681/1164 [04:01<02:51,  2.82it/s]\u001b[A\n",
            "Training:  59% 682/1164 [04:01<02:50,  2.83it/s]\u001b[A\n",
            "Training:  59% 683/1164 [04:01<02:46,  2.89it/s]\u001b[A\n",
            "Training:  59% 684/1164 [04:02<02:53,  2.76it/s]\u001b[A\n",
            "Training:  59% 685/1164 [04:02<02:57,  2.70it/s]\u001b[A\n",
            "Training:  59% 686/1164 [04:02<02:57,  2.70it/s]\u001b[A\n",
            "Training:  59% 687/1164 [04:03<02:53,  2.75it/s]\u001b[A\n",
            "Training:  59% 688/1164 [04:03<02:49,  2.80it/s]\u001b[A\n",
            "Training:  59% 689/1164 [04:03<02:46,  2.85it/s]\u001b[A\n",
            "Training:  59% 690/1164 [04:04<02:40,  2.95it/s]\u001b[A\n",
            "Training:  59% 691/1164 [04:04<02:38,  2.99it/s]\u001b[A\n",
            "Training:  59% 692/1164 [04:04<02:39,  2.96it/s]\u001b[A\n",
            "Training:  60% 693/1164 [04:05<02:37,  3.00it/s]\u001b[A\n",
            "Training:  60% 694/1164 [04:05<02:33,  3.06it/s]\u001b[A\n",
            "Training:  60% 695/1164 [04:05<02:39,  2.95it/s]\u001b[A\n",
            "Training:  60% 696/1164 [04:06<02:38,  2.96it/s]\u001b[A\n",
            "Training:  60% 697/1164 [04:06<02:36,  2.99it/s]\u001b[A\n",
            "Training:  60% 698/1164 [04:06<02:37,  2.96it/s]\u001b[A\n",
            "Training:  60% 699/1164 [04:07<02:40,  2.89it/s]\u001b[A\n",
            "Training:  60% 700/1164 [04:07<02:38,  2.92it/s]\u001b[A\n",
            "Training:  60% 701/1164 [04:07<02:38,  2.92it/s]\u001b[A\n",
            "Training:  60% 702/1164 [04:08<02:36,  2.96it/s]\u001b[A\n",
            "Training:  60% 703/1164 [04:08<02:35,  2.97it/s]\u001b[A\n",
            "Training:  60% 704/1164 [04:08<02:40,  2.86it/s]\u001b[A\n",
            "Training:  61% 705/1164 [04:09<02:41,  2.84it/s]\u001b[A\n",
            "Training:  61% 706/1164 [04:09<02:41,  2.84it/s]\u001b[A\n",
            "Training:  61% 707/1164 [04:10<02:46,  2.74it/s]\u001b[A\n",
            "Training:  61% 708/1164 [04:10<02:47,  2.73it/s]\u001b[A\n",
            "Training:  61% 709/1164 [04:10<02:52,  2.64it/s]\u001b[A\n",
            "Training:  61% 710/1164 [04:11<02:48,  2.70it/s]\u001b[A\n",
            "Training:  61% 711/1164 [04:11<02:43,  2.76it/s]\u001b[A\n",
            "Training:  61% 712/1164 [04:11<02:41,  2.80it/s]\u001b[A\n",
            "Training:  61% 713/1164 [04:12<02:39,  2.82it/s]\u001b[A\n",
            "Training:  61% 714/1164 [04:12<02:41,  2.79it/s]\u001b[A\n",
            "Training:  61% 715/1164 [04:12<02:38,  2.83it/s]\u001b[A\n",
            "Training:  62% 716/1164 [04:13<02:50,  2.63it/s]\u001b[A\n",
            "Training:  62% 717/1164 [04:13<02:51,  2.61it/s]\u001b[A\n",
            "Training:  62% 718/1164 [04:14<02:48,  2.65it/s]\u001b[A\n",
            "Training:  62% 719/1164 [04:14<02:39,  2.79it/s]\u001b[A\n",
            "Training:  62% 720/1164 [04:14<02:40,  2.77it/s]\u001b[A\n",
            "Training:  62% 721/1164 [04:15<02:38,  2.79it/s]\u001b[A\n",
            "Training:  62% 722/1164 [04:15<02:40,  2.75it/s]\u001b[A\n",
            "Training:  62% 723/1164 [04:16<03:04,  2.39it/s]\u001b[A\n",
            "Training:  62% 724/1164 [04:16<02:58,  2.47it/s]\u001b[A\n",
            "Training:  62% 725/1164 [04:16<02:50,  2.57it/s]\u001b[A\n",
            "Training:  62% 726/1164 [04:17<02:46,  2.63it/s]\u001b[A\n",
            "Training:  62% 727/1164 [04:17<02:44,  2.66it/s]\u001b[A\n",
            "Training:  63% 728/1164 [04:17<02:38,  2.75it/s]\u001b[A\n",
            "Training:  63% 729/1164 [04:18<02:34,  2.81it/s]\u001b[A\n",
            "Training:  63% 730/1164 [04:18<02:43,  2.66it/s]\u001b[A\n",
            "Training:  63% 731/1164 [04:18<02:37,  2.75it/s]\u001b[A\n",
            "Training:  63% 732/1164 [04:19<02:39,  2.72it/s]\u001b[A\n",
            "Training:  63% 733/1164 [04:19<02:37,  2.74it/s]\u001b[A\n",
            "Training:  63% 734/1164 [04:20<02:36,  2.75it/s]\u001b[A\n",
            "Training:  63% 735/1164 [04:20<02:35,  2.75it/s]\u001b[A\n",
            "Training:  63% 736/1164 [04:20<02:38,  2.70it/s]\u001b[A\n",
            "Training:  63% 737/1164 [04:21<02:42,  2.64it/s]\u001b[A\n",
            "Training:  63% 738/1164 [04:21<02:40,  2.66it/s]\u001b[A\n",
            "Training:  63% 739/1164 [04:21<02:36,  2.72it/s]\u001b[A\n",
            "Training:  64% 740/1164 [04:22<02:36,  2.70it/s]\u001b[A\n",
            "Training:  64% 741/1164 [04:22<02:37,  2.68it/s]\u001b[A\n",
            "Training:  64% 742/1164 [04:23<02:35,  2.72it/s]\u001b[A\n",
            "Training:  64% 743/1164 [04:23<02:27,  2.86it/s]\u001b[A\n",
            "Training:  64% 744/1164 [04:23<02:27,  2.85it/s]\u001b[A\n",
            "Training:  64% 745/1164 [04:24<02:25,  2.88it/s]\u001b[A\n",
            "Training:  64% 746/1164 [04:24<02:23,  2.90it/s]\u001b[A\n",
            "Training:  64% 747/1164 [04:24<02:26,  2.85it/s]\u001b[A\n",
            "Training:  64% 748/1164 [04:25<02:23,  2.90it/s]\u001b[A\n",
            "Training:  64% 749/1164 [04:25<02:20,  2.95it/s]\u001b[A\n",
            "Training:  64% 750/1164 [04:25<02:21,  2.93it/s]\u001b[A\n",
            "Training:  65% 751/1164 [04:26<02:22,  2.90it/s]\u001b[A\n",
            "Training:  65% 752/1164 [04:26<02:26,  2.81it/s]\u001b[A\n",
            "Training:  65% 753/1164 [04:26<02:28,  2.78it/s]\u001b[A\n",
            "Training:  65% 754/1164 [04:27<02:28,  2.75it/s]\u001b[A\n",
            "Training:  65% 755/1164 [04:27<02:22,  2.87it/s]\u001b[A\n",
            "Training:  65% 756/1164 [04:27<02:23,  2.85it/s]\u001b[A\n",
            "Training:  65% 757/1164 [04:28<02:22,  2.85it/s]\u001b[A\n",
            "Training:  65% 758/1164 [04:28<02:24,  2.81it/s]\u001b[A\n",
            "Training:  65% 759/1164 [04:28<02:27,  2.74it/s]\u001b[A\n",
            "Training:  65% 760/1164 [04:29<02:24,  2.79it/s]\u001b[A\n",
            "Training:  65% 761/1164 [04:29<02:20,  2.87it/s]\u001b[A\n",
            "Training:  65% 762/1164 [04:29<02:18,  2.90it/s]\u001b[A\n",
            "Training:  66% 763/1164 [04:30<02:15,  2.97it/s]\u001b[A\n",
            "Training:  66% 764/1164 [04:30<02:17,  2.91it/s]\u001b[A\n",
            "Training:  66% 765/1164 [04:31<02:16,  2.92it/s]\u001b[A\n",
            "Training:  66% 766/1164 [04:31<02:15,  2.93it/s]\u001b[A\n",
            "Training:  66% 767/1164 [04:31<02:13,  2.97it/s]\u001b[A\n",
            "Training:  66% 768/1164 [04:32<02:12,  2.98it/s]\u001b[A\n",
            "Training:  66% 769/1164 [04:32<02:27,  2.68it/s]\u001b[A\n",
            "Training:  66% 770/1164 [04:32<02:23,  2.74it/s]\u001b[A\n",
            "Training:  66% 771/1164 [04:33<02:23,  2.74it/s]\u001b[A\n",
            "Training:  66% 772/1164 [04:33<02:20,  2.79it/s]\u001b[A\n",
            "Training:  66% 773/1164 [04:33<02:19,  2.80it/s]\u001b[A\n",
            "Training:  66% 774/1164 [04:34<02:16,  2.85it/s]\u001b[A\n",
            "Training:  67% 775/1164 [04:34<02:19,  2.80it/s]\u001b[A\n",
            "Training:  67% 776/1164 [04:34<02:16,  2.84it/s]\u001b[A\n",
            "Training:  67% 777/1164 [04:35<02:13,  2.89it/s]\u001b[A\n",
            "Training:  67% 778/1164 [04:35<02:20,  2.75it/s]\u001b[A\n",
            "Training:  67% 779/1164 [04:35<02:16,  2.82it/s]\u001b[A\n",
            "Training:  67% 780/1164 [04:36<02:15,  2.84it/s]\u001b[A\n",
            "Training:  67% 781/1164 [04:36<02:12,  2.89it/s]\u001b[A\n",
            "Training:  67% 782/1164 [04:37<02:12,  2.88it/s]\u001b[A\n",
            "Training:  67% 783/1164 [04:37<02:18,  2.74it/s]\u001b[A\n",
            "Training:  67% 784/1164 [04:37<02:14,  2.82it/s]\u001b[A\n",
            "Training:  67% 785/1164 [04:38<02:15,  2.79it/s]\u001b[A\n",
            "Training:  68% 786/1164 [04:38<02:12,  2.86it/s]\u001b[A\n",
            "Training:  68% 787/1164 [04:38<02:10,  2.90it/s]\u001b[A\n",
            "Training:  68% 788/1164 [04:39<02:12,  2.83it/s]\u001b[A\n",
            "Training:  68% 789/1164 [04:39<02:12,  2.82it/s]\u001b[A\n",
            "Training:  68% 790/1164 [04:39<02:12,  2.83it/s]\u001b[A\n",
            "Training:  68% 791/1164 [04:40<02:16,  2.72it/s]\u001b[A\n",
            "Training:  68% 792/1164 [04:40<02:12,  2.80it/s]\u001b[A\n",
            "Training:  68% 793/1164 [04:40<02:11,  2.83it/s]\u001b[A\n",
            "Training:  68% 794/1164 [04:41<02:08,  2.88it/s]\u001b[A\n",
            "Training:  68% 795/1164 [04:41<02:08,  2.88it/s]\u001b[A\n",
            "Training:  68% 796/1164 [04:41<02:04,  2.95it/s]\u001b[A\n",
            "Training:  68% 797/1164 [04:42<02:05,  2.92it/s]\u001b[A\n",
            "Training:  69% 798/1164 [04:42<02:08,  2.84it/s]\u001b[A\n",
            "Training:  69% 799/1164 [04:42<02:04,  2.92it/s]\u001b[A\n",
            "Training:  69% 800/1164 [04:43<02:10,  2.79it/s]\u001b[A\n",
            "Training:  69% 801/1164 [04:43<02:14,  2.70it/s]\u001b[A\n",
            "Training:  69% 802/1164 [04:44<02:12,  2.73it/s]\u001b[A\n",
            "Training:  69% 803/1164 [04:44<02:08,  2.82it/s]\u001b[A\n",
            "Training:  69% 804/1164 [04:44<02:04,  2.89it/s]\u001b[A\n",
            "Training:  69% 805/1164 [04:45<02:03,  2.91it/s]\u001b[A\n",
            "Training:  69% 806/1164 [04:45<02:02,  2.92it/s]\u001b[A\n",
            "Training:  69% 807/1164 [04:45<02:00,  2.96it/s]\u001b[A\n",
            "Training:  69% 808/1164 [04:46<02:05,  2.84it/s]\u001b[A\n",
            "Training:  70% 809/1164 [04:46<01:59,  2.97it/s]\u001b[A\n",
            "Training:  70% 810/1164 [04:46<01:59,  2.95it/s]\u001b[A\n",
            "Training:  70% 811/1164 [04:47<02:00,  2.93it/s]\u001b[A\n",
            "Training:  70% 812/1164 [04:47<02:02,  2.87it/s]\u001b[A\n",
            "Training:  70% 813/1164 [04:47<01:59,  2.94it/s]\u001b[A\n",
            "Training:  70% 814/1164 [04:48<02:02,  2.86it/s]\u001b[A\n",
            "Training:  70% 815/1164 [04:48<01:59,  2.92it/s]\u001b[A\n",
            "Training:  70% 816/1164 [04:48<02:00,  2.89it/s]\u001b[A\n",
            "Training:  70% 817/1164 [04:49<02:01,  2.84it/s]\u001b[A\n",
            "Training:  70% 818/1164 [04:49<01:59,  2.90it/s]\u001b[A\n",
            "Training:  70% 819/1164 [04:49<01:58,  2.90it/s]\u001b[A\n",
            "Training:  70% 820/1164 [04:50<02:02,  2.81it/s]\u001b[A\n",
            "Training:  71% 821/1164 [04:50<02:00,  2.84it/s]\u001b[A\n",
            "Training:  71% 822/1164 [04:51<01:58,  2.89it/s]\u001b[A\n",
            "Training:  71% 823/1164 [04:51<01:59,  2.86it/s]\u001b[A\n",
            "Training:  71% 824/1164 [04:51<01:56,  2.91it/s]\u001b[A\n",
            "Training:  71% 825/1164 [04:52<01:56,  2.91it/s]\u001b[A\n",
            "Training:  71% 826/1164 [04:52<01:59,  2.83it/s]\u001b[A\n",
            "Training:  71% 827/1164 [04:52<01:55,  2.91it/s]\u001b[A\n",
            "Training:  71% 828/1164 [04:53<01:52,  2.99it/s]\u001b[A\n",
            "Training:  71% 829/1164 [04:53<01:53,  2.95it/s]\u001b[A\n",
            "Training:  71% 830/1164 [04:53<01:54,  2.93it/s]\u001b[A\n",
            "Training:  71% 831/1164 [04:54<01:53,  2.92it/s]\u001b[A\n",
            "Training:  71% 832/1164 [04:54<01:51,  2.97it/s]\u001b[A\n",
            "Training:  72% 833/1164 [04:54<01:51,  2.97it/s]\u001b[A\n",
            "Training:  72% 834/1164 [04:55<01:56,  2.84it/s]\u001b[A\n",
            "Training:  72% 835/1164 [04:55<01:57,  2.80it/s]\u001b[A\n",
            "Training:  72% 836/1164 [04:55<02:01,  2.70it/s]\u001b[A\n",
            "Training:  72% 837/1164 [04:56<01:58,  2.75it/s]\u001b[A\n",
            "Training:  72% 838/1164 [04:56<01:57,  2.77it/s]\u001b[A\n",
            "Training:  72% 839/1164 [04:56<01:54,  2.83it/s]\u001b[A\n",
            "Training:  72% 840/1164 [04:57<01:56,  2.77it/s]\u001b[A\n",
            "Training:  72% 841/1164 [04:57<01:58,  2.73it/s]\u001b[A\n",
            "Training:  72% 842/1164 [04:58<01:57,  2.75it/s]\u001b[A\n",
            "Training:  72% 843/1164 [04:58<01:54,  2.80it/s]\u001b[A\n",
            "Training:  73% 844/1164 [04:58<01:56,  2.74it/s]\u001b[A\n",
            "Training:  73% 845/1164 [04:59<01:54,  2.79it/s]\u001b[A\n",
            "Training:  73% 846/1164 [04:59<01:52,  2.83it/s]\u001b[A\n",
            "Training:  73% 847/1164 [04:59<01:49,  2.90it/s]\u001b[A\n",
            "Training:  73% 848/1164 [05:00<01:48,  2.91it/s]\u001b[A\n",
            "Training:  73% 849/1164 [05:00<01:52,  2.81it/s]\u001b[A\n",
            "Training:  73% 850/1164 [05:00<01:54,  2.74it/s]\u001b[A\n",
            "Training:  73% 851/1164 [05:01<01:55,  2.71it/s]\u001b[A\n",
            "Training:  73% 852/1164 [05:01<01:50,  2.82it/s]\u001b[A\n",
            "Training:  73% 853/1164 [05:01<01:52,  2.77it/s]\u001b[A\n",
            "Training:  73% 854/1164 [05:02<01:51,  2.77it/s]\u001b[A\n",
            "Training:  73% 855/1164 [05:02<01:49,  2.83it/s]\u001b[A\n",
            "Training:  74% 856/1164 [05:03<01:48,  2.84it/s]\u001b[A\n",
            "Training:  74% 857/1164 [05:03<01:47,  2.84it/s]\u001b[A\n",
            "Training:  74% 858/1164 [05:03<01:46,  2.88it/s]\u001b[A\n",
            "Training:  74% 859/1164 [05:04<01:48,  2.80it/s]\u001b[A\n",
            "Training:  74% 860/1164 [05:04<01:45,  2.87it/s]\u001b[A\n",
            "Training:  74% 861/1164 [05:04<01:44,  2.90it/s]\u001b[A\n",
            "Training:  74% 862/1164 [05:05<01:42,  2.95it/s]\u001b[A\n",
            "Training:  74% 863/1164 [05:05<01:41,  2.98it/s]\u001b[A\n",
            "Training:  74% 864/1164 [05:05<01:42,  2.94it/s]\u001b[A\n",
            "Training:  74% 865/1164 [05:06<01:43,  2.89it/s]\u001b[A\n",
            "Training:  74% 866/1164 [05:06<01:46,  2.80it/s]\u001b[A\n",
            "Training:  74% 867/1164 [05:06<01:44,  2.84it/s]\u001b[A\n",
            "Training:  75% 868/1164 [05:07<01:41,  2.92it/s]\u001b[A\n",
            "Training:  75% 869/1164 [05:07<01:39,  2.97it/s]\u001b[A\n",
            "Training:  75% 870/1164 [05:07<01:37,  3.01it/s]\u001b[A\n",
            "Training:  75% 871/1164 [05:08<01:39,  2.93it/s]\u001b[A\n",
            "Training:  75% 872/1164 [05:08<01:37,  2.99it/s]\u001b[A\n",
            "Training:  75% 873/1164 [05:08<01:37,  2.97it/s]\u001b[A\n",
            "Training:  75% 874/1164 [05:09<01:40,  2.89it/s]\u001b[A\n",
            "Training:  75% 875/1164 [05:09<01:40,  2.87it/s]\u001b[A\n",
            "Training:  75% 876/1164 [05:09<01:44,  2.76it/s]\u001b[A\n",
            "Training:  75% 877/1164 [05:10<01:43,  2.77it/s]\u001b[A\n",
            "Training:  75% 878/1164 [05:10<01:43,  2.76it/s]\u001b[A\n",
            "Training:  76% 879/1164 [05:11<01:43,  2.75it/s]\u001b[A\n",
            "Training:  76% 880/1164 [05:11<01:43,  2.74it/s]\u001b[A\n",
            "Training:  76% 881/1164 [05:11<01:39,  2.83it/s]\u001b[A\n",
            "Training:  76% 882/1164 [05:12<01:41,  2.78it/s]\u001b[A\n",
            "Training:  76% 883/1164 [05:12<01:38,  2.84it/s]\u001b[A\n",
            "Training:  76% 884/1164 [05:12<01:37,  2.88it/s]\u001b[A\n",
            "Training:  76% 885/1164 [05:13<01:39,  2.82it/s]\u001b[A\n",
            "Training:  76% 886/1164 [05:13<01:36,  2.89it/s]\u001b[A\n",
            "Training:  76% 887/1164 [05:13<01:34,  2.92it/s]\u001b[A\n",
            "Training:  76% 888/1164 [05:14<01:33,  2.96it/s]\u001b[A\n",
            "Training:  76% 889/1164 [05:14<01:34,  2.91it/s]\u001b[A\n",
            "Training:  76% 890/1164 [05:14<01:32,  2.97it/s]\u001b[A\n",
            "Training:  77% 891/1164 [05:15<01:31,  3.00it/s]\u001b[A\n",
            "Training:  77% 892/1164 [05:15<01:32,  2.96it/s]\u001b[A\n",
            "Training:  77% 893/1164 [05:15<01:32,  2.93it/s]\u001b[A\n",
            "Training:  77% 894/1164 [05:16<01:35,  2.84it/s]\u001b[A\n",
            "Training:  77% 895/1164 [05:16<01:33,  2.89it/s]\u001b[A\n",
            "Training:  77% 896/1164 [05:16<01:32,  2.91it/s]\u001b[A\n",
            "Training:  77% 897/1164 [05:17<01:31,  2.92it/s]\u001b[A\n",
            "Training:  77% 898/1164 [05:17<01:31,  2.91it/s]\u001b[A\n",
            "Training:  77% 899/1164 [05:17<01:32,  2.87it/s]\u001b[A\n",
            "Training:  77% 900/1164 [05:18<01:32,  2.84it/s]\u001b[A\n",
            "Training:  77% 901/1164 [05:18<01:29,  2.94it/s]\u001b[A\n",
            "Training:  77% 902/1164 [05:18<01:30,  2.88it/s]\u001b[A\n",
            "Training:  78% 903/1164 [05:19<01:29,  2.92it/s]\u001b[A\n",
            "Training:  78% 904/1164 [05:19<01:28,  2.94it/s]\u001b[A\n",
            "Training:  78% 905/1164 [05:19<01:27,  2.95it/s]\u001b[A\n",
            "Training:  78% 906/1164 [05:20<01:27,  2.96it/s]\u001b[A\n",
            "Training:  78% 907/1164 [05:20<01:26,  2.98it/s]\u001b[A\n",
            "Training:  78% 908/1164 [05:21<01:27,  2.93it/s]\u001b[A\n",
            "Training:  78% 909/1164 [05:21<01:25,  2.99it/s]\u001b[A\n",
            "Training:  78% 910/1164 [05:21<01:25,  2.96it/s]\u001b[A\n",
            "Training:  78% 911/1164 [05:22<01:26,  2.93it/s]\u001b[A\n",
            "Training:  78% 912/1164 [05:22<01:27,  2.87it/s]\u001b[A\n",
            "Training:  78% 913/1164 [05:22<01:28,  2.83it/s]\u001b[A\n",
            "Training:  79% 914/1164 [05:23<01:28,  2.81it/s]\u001b[A\n",
            "Training:  79% 915/1164 [05:23<01:27,  2.83it/s]\u001b[A\n",
            "Training:  79% 916/1164 [05:23<01:27,  2.83it/s]\u001b[A\n",
            "Training:  79% 917/1164 [05:24<01:25,  2.88it/s]\u001b[A\n",
            "Training:  79% 918/1164 [05:24<01:26,  2.86it/s]\u001b[A\n",
            "Training:  79% 919/1164 [05:24<01:28,  2.78it/s]\u001b[A\n",
            "Training:  79% 920/1164 [05:25<01:27,  2.80it/s]\u001b[A\n",
            "Training:  79% 921/1164 [05:25<01:24,  2.88it/s]\u001b[A\n",
            "Training:  79% 922/1164 [05:25<01:22,  2.94it/s]\u001b[A\n",
            "Training:  79% 923/1164 [05:26<01:22,  2.91it/s]\u001b[A\n",
            "Training:  79% 924/1164 [05:26<01:21,  2.94it/s]\u001b[A\n",
            "Training:  79% 925/1164 [05:26<01:20,  2.97it/s]\u001b[A\n",
            "Training:  80% 926/1164 [05:27<01:21,  2.94it/s]\u001b[A\n",
            "Training:  80% 927/1164 [05:27<01:20,  2.94it/s]\u001b[A\n",
            "Training:  80% 928/1164 [05:27<01:20,  2.94it/s]\u001b[A\n",
            "Training:  80% 929/1164 [05:28<01:20,  2.93it/s]\u001b[A\n",
            "Training:  80% 930/1164 [05:28<01:19,  2.94it/s]\u001b[A\n",
            "Training:  80% 931/1164 [05:28<01:17,  3.00it/s]\u001b[A\n",
            "Training:  80% 932/1164 [05:29<01:19,  2.93it/s]\u001b[A\n",
            "Training:  80% 933/1164 [05:29<01:19,  2.92it/s]\u001b[A\n",
            "Training:  80% 934/1164 [05:29<01:17,  2.96it/s]\u001b[A\n",
            "Training:  80% 935/1164 [05:30<01:16,  2.98it/s]\u001b[A\n",
            "Training:  80% 936/1164 [05:30<01:16,  2.97it/s]\u001b[A\n",
            "Training:  80% 937/1164 [05:30<01:18,  2.89it/s]\u001b[A\n",
            "Training:  81% 938/1164 [05:31<01:17,  2.92it/s]\u001b[A\n",
            "Training:  81% 939/1164 [05:31<01:17,  2.90it/s]\u001b[A\n",
            "Training:  81% 940/1164 [05:32<01:18,  2.84it/s]\u001b[A\n",
            "Training:  81% 941/1164 [05:32<01:18,  2.84it/s]\u001b[A\n",
            "Training:  81% 942/1164 [05:32<01:21,  2.74it/s]\u001b[A\n",
            "Training:  81% 943/1164 [05:33<01:21,  2.71it/s]\u001b[A\n",
            "Training:  81% 944/1164 [05:33<01:18,  2.80it/s]\u001b[A\n",
            "Training:  81% 945/1164 [05:33<01:15,  2.90it/s]\u001b[A\n",
            "Training:  81% 946/1164 [05:34<01:16,  2.85it/s]\u001b[A\n",
            "Training:  81% 947/1164 [05:34<01:16,  2.85it/s]\u001b[A\n",
            "Training:  81% 948/1164 [05:34<01:14,  2.89it/s]\u001b[A\n",
            "Training:  82% 949/1164 [05:35<01:16,  2.81it/s]\u001b[A\n",
            "Training:  82% 950/1164 [05:35<01:16,  2.79it/s]\u001b[A\n",
            "Training:  82% 951/1164 [05:35<01:15,  2.84it/s]\u001b[A\n",
            "Training:  82% 952/1164 [05:36<01:17,  2.73it/s]\u001b[A\n",
            "Training:  82% 953/1164 [05:36<01:18,  2.70it/s]\u001b[A\n",
            "Training:  82% 954/1164 [05:37<01:15,  2.77it/s]\u001b[A\n",
            "Training:  82% 955/1164 [05:37<01:16,  2.75it/s]\u001b[A\n",
            "Training:  82% 956/1164 [05:37<01:16,  2.72it/s]\u001b[A\n",
            "Training:  82% 957/1164 [05:38<01:15,  2.74it/s]\u001b[A\n",
            "Training:  82% 958/1164 [05:38<01:15,  2.71it/s]\u001b[A\n",
            "Training:  82% 959/1164 [05:38<01:12,  2.81it/s]\u001b[A\n",
            "Training:  82% 960/1164 [05:39<01:12,  2.81it/s]\u001b[A\n",
            "Training:  83% 961/1164 [05:39<01:10,  2.87it/s]\u001b[A\n",
            "Training:  83% 962/1164 [05:39<01:08,  2.93it/s]\u001b[A\n",
            "Training:  83% 963/1164 [05:40<01:09,  2.89it/s]\u001b[A\n",
            "Training:  83% 964/1164 [05:40<01:08,  2.92it/s]\u001b[A\n",
            "Training:  83% 965/1164 [05:40<01:06,  3.00it/s]\u001b[A\n",
            "Training:  83% 966/1164 [05:41<01:07,  2.91it/s]\u001b[A\n",
            "Training:  83% 967/1164 [05:41<01:11,  2.74it/s]\u001b[A\n",
            "Training:  83% 968/1164 [05:42<01:10,  2.76it/s]\u001b[A\n",
            "Training:  83% 969/1164 [05:42<01:09,  2.81it/s]\u001b[A\n",
            "Training:  83% 970/1164 [05:42<01:11,  2.73it/s]\u001b[A\n",
            "Training:  83% 971/1164 [05:43<01:10,  2.74it/s]\u001b[A\n",
            "Training:  84% 972/1164 [05:43<01:09,  2.78it/s]\u001b[A\n",
            "Training:  84% 973/1164 [05:43<01:09,  2.76it/s]\u001b[A\n",
            "Training:  84% 974/1164 [05:44<01:08,  2.78it/s]\u001b[A\n",
            "Training:  84% 975/1164 [05:44<01:05,  2.86it/s]\u001b[A\n",
            "Training:  84% 976/1164 [05:44<01:08,  2.76it/s]\u001b[A\n",
            "Training:  84% 977/1164 [05:45<01:07,  2.77it/s]\u001b[A\n",
            "Training:  84% 978/1164 [05:45<01:06,  2.80it/s]\u001b[A\n",
            "Training:  84% 979/1164 [05:45<01:04,  2.87it/s]\u001b[A\n",
            "Training:  84% 980/1164 [05:46<01:01,  2.99it/s]\u001b[A\n",
            "Training:  84% 981/1164 [05:46<01:01,  2.97it/s]\u001b[A\n",
            "Training:  84% 982/1164 [05:46<01:01,  2.97it/s]\u001b[A\n",
            "Training:  84% 983/1164 [05:47<00:59,  3.04it/s]\u001b[A\n",
            "Training:  85% 984/1164 [05:47<00:58,  3.05it/s]\u001b[A\n",
            "Training:  85% 985/1164 [05:47<00:58,  3.04it/s]\u001b[A\n",
            "Training:  85% 986/1164 [05:48<00:59,  2.97it/s]\u001b[A\n",
            "Training:  85% 987/1164 [05:48<00:59,  2.97it/s]\u001b[A\n",
            "Training:  85% 988/1164 [05:48<00:58,  3.00it/s]\u001b[A\n",
            "Training:  85% 989/1164 [05:49<00:59,  2.95it/s]\u001b[A\n",
            "Training:  85% 990/1164 [05:49<00:59,  2.94it/s]\u001b[A\n",
            "Training:  85% 991/1164 [05:49<01:00,  2.85it/s]\u001b[A\n",
            "Training:  85% 992/1164 [05:50<00:58,  2.95it/s]\u001b[A\n",
            "Training:  85% 993/1164 [05:50<00:57,  2.97it/s]\u001b[A\n",
            "Training:  85% 994/1164 [05:50<00:56,  2.99it/s]\u001b[A\n",
            "Training:  85% 995/1164 [05:51<00:58,  2.90it/s]\u001b[A\n",
            "Training:  86% 996/1164 [05:51<00:59,  2.82it/s]\u001b[A\n",
            "Training:  86% 997/1164 [05:52<00:59,  2.81it/s]\u001b[A\n",
            "Training:  86% 998/1164 [05:52<01:00,  2.74it/s]\u001b[A\n",
            "Training:  86% 999/1164 [05:52<01:00,  2.71it/s]\u001b[A\n",
            "Training:  86% 1000/1164 [05:53<01:00,  2.71it/s]\u001b[A\n",
            "Training:  86% 1001/1164 [05:53<00:56,  2.87it/s]\u001b[A\n",
            "Training:  86% 1002/1164 [05:53<00:56,  2.85it/s]\u001b[A\n",
            "Training:  86% 1003/1164 [05:54<00:55,  2.88it/s]\u001b[A\n",
            "Training:  86% 1004/1164 [05:54<00:54,  2.94it/s]\u001b[A\n",
            "Training:  86% 1005/1164 [05:54<00:53,  2.95it/s]\u001b[A\n",
            "Training:  86% 1006/1164 [05:55<00:53,  2.97it/s]\u001b[A\n",
            "Training:  87% 1007/1164 [05:55<00:52,  2.96it/s]\u001b[A\n",
            "Training:  87% 1008/1164 [05:55<00:52,  2.99it/s]\u001b[A\n",
            "Training:  87% 1009/1164 [05:56<00:52,  2.96it/s]\u001b[A\n",
            "Training:  87% 1010/1164 [05:56<00:52,  2.92it/s]\u001b[A\n",
            "Training:  87% 1011/1164 [05:56<00:52,  2.89it/s]\u001b[A\n",
            "Training:  87% 1012/1164 [05:57<00:52,  2.89it/s]\u001b[A\n",
            "Training:  87% 1013/1164 [05:57<00:53,  2.83it/s]\u001b[A\n",
            "Training:  87% 1014/1164 [05:58<00:54,  2.73it/s]\u001b[A\n",
            "Training:  87% 1015/1164 [05:58<00:53,  2.78it/s]\u001b[A\n",
            "Training:  87% 1016/1164 [05:58<00:52,  2.80it/s]\u001b[A\n",
            "Training:  87% 1017/1164 [05:59<00:51,  2.87it/s]\u001b[A\n",
            "Training:  87% 1018/1164 [05:59<00:50,  2.91it/s]\u001b[A\n",
            "Training:  88% 1019/1164 [05:59<00:48,  2.99it/s]\u001b[A\n",
            "Training:  88% 1020/1164 [06:00<00:48,  2.95it/s]\u001b[A\n",
            "Training:  88% 1021/1164 [06:00<00:48,  2.97it/s]\u001b[A\n",
            "Training:  88% 1022/1164 [06:00<00:48,  2.91it/s]\u001b[A\n",
            "Training:  88% 1023/1164 [06:01<00:49,  2.87it/s]\u001b[A\n",
            "Training:  88% 1024/1164 [06:01<00:48,  2.90it/s]\u001b[A\n",
            "Training:  88% 1025/1164 [06:01<00:47,  2.92it/s]\u001b[A\n",
            "Training:  88% 1026/1164 [06:02<00:47,  2.88it/s]\u001b[A\n",
            "Training:  88% 1027/1164 [06:02<00:47,  2.90it/s]\u001b[A\n",
            "Training:  88% 1028/1164 [06:02<00:46,  2.94it/s]\u001b[A\n",
            "Training:  88% 1029/1164 [06:03<00:46,  2.92it/s]\u001b[A\n",
            "Training:  88% 1030/1164 [06:03<00:46,  2.91it/s]\u001b[A\n",
            "Training:  89% 1031/1164 [06:03<00:45,  2.94it/s]\u001b[A\n",
            "Training:  89% 1032/1164 [06:04<00:44,  2.95it/s]\u001b[A\n",
            "Training:  89% 1033/1164 [06:04<00:47,  2.78it/s]\u001b[A\n",
            "Training:  89% 1034/1164 [06:04<00:46,  2.77it/s]\u001b[A\n",
            "Training:  89% 1035/1164 [06:05<00:45,  2.84it/s]\u001b[A\n",
            "Training:  89% 1036/1164 [06:05<00:43,  2.93it/s]\u001b[A\n",
            "Training:  89% 1037/1164 [06:05<00:43,  2.91it/s]\u001b[A\n",
            "Training:  89% 1038/1164 [06:06<00:44,  2.84it/s]\u001b[A\n",
            "Training:  89% 1039/1164 [06:06<00:43,  2.90it/s]\u001b[A\n",
            "Training:  89% 1040/1164 [06:07<00:45,  2.70it/s]\u001b[A\n",
            "Training:  89% 1041/1164 [06:07<00:44,  2.77it/s]\u001b[A\n",
            "Training:  90% 1042/1164 [06:07<00:43,  2.79it/s]\u001b[A\n",
            "Training:  90% 1043/1164 [06:08<00:43,  2.79it/s]\u001b[A\n",
            "Training:  90% 1044/1164 [06:08<00:43,  2.78it/s]\u001b[A\n",
            "Training:  90% 1045/1164 [06:08<00:42,  2.81it/s]\u001b[A\n",
            "Training:  90% 1046/1164 [06:09<00:42,  2.76it/s]\u001b[A\n",
            "Training:  90% 1047/1164 [06:09<00:41,  2.79it/s]\u001b[A\n",
            "Training:  90% 1048/1164 [06:09<00:40,  2.85it/s]\u001b[A\n",
            "Training:  90% 1049/1164 [06:10<00:40,  2.86it/s]\u001b[A\n",
            "Training:  90% 1050/1164 [06:10<00:41,  2.75it/s]\u001b[A\n",
            "Training:  90% 1051/1164 [06:10<00:41,  2.73it/s]\u001b[A\n",
            "Training:  90% 1052/1164 [06:11<00:40,  2.80it/s]\u001b[A\n",
            "Training:  90% 1053/1164 [06:11<00:39,  2.82it/s]\u001b[A\n",
            "Training:  91% 1054/1164 [06:11<00:38,  2.86it/s]\u001b[A\n",
            "Training:  91% 1055/1164 [06:12<00:39,  2.73it/s]\u001b[A\n",
            "Training:  91% 1056/1164 [06:12<00:38,  2.83it/s]\u001b[A\n",
            "Training:  91% 1057/1164 [06:13<00:36,  2.91it/s]\u001b[A\n",
            "Training:  91% 1058/1164 [06:13<00:36,  2.89it/s]\u001b[A\n",
            "Training:  91% 1059/1164 [06:13<00:36,  2.88it/s]\u001b[A\n",
            "Training:  91% 1060/1164 [06:14<00:36,  2.84it/s]\u001b[A\n",
            "Training:  91% 1061/1164 [06:14<00:36,  2.84it/s]\u001b[A\n",
            "Training:  91% 1062/1164 [06:14<00:36,  2.78it/s]\u001b[A\n",
            "Training:  91% 1063/1164 [06:15<00:36,  2.74it/s]\u001b[A\n",
            "Training:  91% 1064/1164 [06:15<00:36,  2.71it/s]\u001b[A\n",
            "Training:  91% 1065/1164 [06:15<00:35,  2.78it/s]\u001b[A\n",
            "Training:  92% 1066/1164 [06:16<00:35,  2.77it/s]\u001b[A\n",
            "Training:  92% 1067/1164 [06:16<00:34,  2.85it/s]\u001b[A\n",
            "Training:  92% 1068/1164 [06:16<00:33,  2.89it/s]\u001b[A\n",
            "Training:  92% 1069/1164 [06:17<00:34,  2.74it/s]\u001b[A\n",
            "Training:  92% 1070/1164 [06:17<00:35,  2.68it/s]\u001b[A\n",
            "Training:  92% 1071/1164 [06:18<00:34,  2.73it/s]\u001b[A\n",
            "Training:  92% 1072/1164 [06:18<00:33,  2.74it/s]\u001b[A\n",
            "Training:  92% 1073/1164 [06:18<00:32,  2.78it/s]\u001b[A\n",
            "Training:  92% 1074/1164 [06:19<00:32,  2.75it/s]\u001b[A\n",
            "Training:  92% 1075/1164 [06:19<00:32,  2.73it/s]\u001b[A\n",
            "Training:  92% 1076/1164 [06:19<00:31,  2.80it/s]\u001b[A\n",
            "Training:  93% 1077/1164 [06:20<00:31,  2.73it/s]\u001b[A\n",
            "Training:  93% 1078/1164 [06:20<00:30,  2.80it/s]\u001b[A\n",
            "Training:  93% 1079/1164 [06:20<00:30,  2.82it/s]\u001b[A\n",
            "Training:  93% 1080/1164 [06:21<00:29,  2.88it/s]\u001b[A\n",
            "Training:  93% 1081/1164 [06:21<00:29,  2.84it/s]\u001b[A\n",
            "Training:  93% 1082/1164 [06:22<00:28,  2.84it/s]\u001b[A\n",
            "Training:  93% 1083/1164 [06:22<00:27,  2.94it/s]\u001b[A\n",
            "Training:  93% 1084/1164 [06:22<00:26,  3.02it/s]\u001b[A\n",
            "Training:  93% 1085/1164 [06:22<00:26,  3.00it/s]\u001b[A\n",
            "Training:  93% 1086/1164 [06:23<00:26,  2.95it/s]\u001b[A\n",
            "Training:  93% 1087/1164 [06:23<00:26,  2.94it/s]\u001b[A\n",
            "Training:  93% 1088/1164 [06:23<00:25,  3.01it/s]\u001b[A\n",
            "Training:  94% 1089/1164 [06:24<00:25,  2.96it/s]\u001b[A\n",
            "Training:  94% 1090/1164 [06:24<00:25,  2.88it/s]\u001b[A\n",
            "Training:  94% 1091/1164 [06:25<00:24,  2.92it/s]\u001b[A\n",
            "Training:  94% 1092/1164 [06:25<00:24,  2.97it/s]\u001b[A\n",
            "Training:  94% 1093/1164 [06:25<00:24,  2.90it/s]\u001b[A\n",
            "Training:  94% 1094/1164 [06:26<00:25,  2.79it/s]\u001b[A\n",
            "Training:  94% 1095/1164 [06:26<00:24,  2.85it/s]\u001b[A\n",
            "Training:  94% 1096/1164 [06:26<00:23,  2.88it/s]\u001b[A\n",
            "Training:  94% 1097/1164 [06:27<00:23,  2.89it/s]\u001b[A\n",
            "Training:  94% 1098/1164 [06:27<00:22,  2.96it/s]\u001b[A\n",
            "Training:  94% 1099/1164 [06:27<00:22,  2.95it/s]\u001b[A\n",
            "Training:  95% 1100/1164 [06:28<00:21,  2.92it/s]\u001b[A\n",
            "Training:  95% 1101/1164 [06:28<00:21,  2.94it/s]\u001b[A\n",
            "Training:  95% 1102/1164 [06:28<00:20,  2.97it/s]\u001b[A\n",
            "Training:  95% 1103/1164 [06:29<00:21,  2.90it/s]\u001b[A\n",
            "Training:  95% 1104/1164 [06:29<00:20,  2.99it/s]\u001b[A\n",
            "Training:  95% 1105/1164 [06:29<00:20,  2.95it/s]\u001b[A\n",
            "Training:  95% 1106/1164 [06:30<00:19,  2.92it/s]\u001b[A\n",
            "Training:  95% 1107/1164 [06:30<00:19,  2.89it/s]\u001b[A\n",
            "Training:  95% 1108/1164 [06:30<00:19,  2.91it/s]\u001b[A\n",
            "Training:  95% 1109/1164 [06:31<00:19,  2.88it/s]\u001b[A\n",
            "Training:  95% 1110/1164 [06:31<00:18,  2.86it/s]\u001b[A\n",
            "Training:  95% 1111/1164 [06:31<00:18,  2.83it/s]\u001b[A\n",
            "Training:  96% 1112/1164 [06:32<00:18,  2.86it/s]\u001b[A\n",
            "Training:  96% 1113/1164 [06:32<00:18,  2.83it/s]\u001b[A\n",
            "Training:  96% 1114/1164 [06:32<00:17,  2.89it/s]\u001b[A\n",
            "Training:  96% 1115/1164 [06:33<00:16,  2.96it/s]\u001b[A\n",
            "Training:  96% 1116/1164 [06:33<00:16,  2.91it/s]\u001b[A\n",
            "Training:  96% 1117/1164 [06:34<00:16,  2.87it/s]\u001b[A\n",
            "Training:  96% 1118/1164 [06:34<00:16,  2.73it/s]\u001b[A\n",
            "Training:  96% 1119/1164 [06:34<00:16,  2.70it/s]\u001b[A\n",
            "Training:  96% 1120/1164 [06:35<00:16,  2.73it/s]\u001b[A\n",
            "Training:  96% 1121/1164 [06:35<00:15,  2.79it/s]\u001b[A\n",
            "Training:  96% 1122/1164 [06:35<00:15,  2.72it/s]\u001b[A\n",
            "Training:  96% 1123/1164 [06:36<00:15,  2.73it/s]\u001b[A\n",
            "Training:  97% 1124/1164 [06:36<00:14,  2.82it/s]\u001b[A\n",
            "Training:  97% 1125/1164 [06:36<00:13,  2.85it/s]\u001b[A\n",
            "Training:  97% 1126/1164 [06:37<00:13,  2.86it/s]\u001b[A\n",
            "Training:  97% 1127/1164 [06:37<00:12,  2.86it/s]\u001b[A\n",
            "Training:  97% 1128/1164 [06:37<00:12,  2.87it/s]\u001b[A\n",
            "Training:  97% 1129/1164 [06:38<00:12,  2.88it/s]\u001b[A\n",
            "Training:  97% 1130/1164 [06:38<00:11,  2.85it/s]\u001b[A\n",
            "Training:  97% 1131/1164 [06:39<00:11,  2.88it/s]\u001b[A\n",
            "Training:  97% 1132/1164 [06:39<00:11,  2.90it/s]\u001b[A\n",
            "Training:  97% 1133/1164 [06:39<00:10,  2.95it/s]\u001b[A\n",
            "Training:  97% 1134/1164 [06:39<00:10,  2.99it/s]\u001b[A\n",
            "Training:  98% 1135/1164 [06:40<00:09,  2.96it/s]\u001b[A\n",
            "Training:  98% 1136/1164 [06:40<00:09,  2.91it/s]\u001b[A\n",
            "Training:  98% 1137/1164 [06:41<00:09,  2.78it/s]\u001b[A\n",
            "Training:  98% 1138/1164 [06:41<00:09,  2.85it/s]\u001b[A\n",
            "Training:  98% 1139/1164 [06:41<00:08,  2.86it/s]\u001b[A\n",
            "Training:  98% 1140/1164 [06:42<00:08,  2.67it/s]\u001b[A\n",
            "Training:  98% 1141/1164 [06:42<00:08,  2.74it/s]\u001b[A\n",
            "Training:  98% 1142/1164 [06:42<00:08,  2.73it/s]\u001b[A\n",
            "Training:  98% 1143/1164 [06:43<00:07,  2.83it/s]\u001b[A\n",
            "Training:  98% 1144/1164 [06:43<00:07,  2.80it/s]\u001b[A\n",
            "Training:  98% 1145/1164 [06:43<00:06,  2.75it/s]\u001b[A\n",
            "Training:  98% 1146/1164 [06:44<00:06,  2.82it/s]\u001b[A\n",
            "Training:  99% 1147/1164 [06:44<00:05,  2.91it/s]\u001b[A\n",
            "Training:  99% 1148/1164 [06:45<00:05,  2.78it/s]\u001b[A\n",
            "Training:  99% 1149/1164 [06:45<00:05,  2.81it/s]\u001b[A\n",
            "Training:  99% 1150/1164 [06:45<00:04,  2.82it/s]\u001b[A\n",
            "Training:  99% 1151/1164 [06:46<00:04,  2.70it/s]\u001b[A\n",
            "Training:  99% 1152/1164 [06:46<00:04,  2.74it/s]\u001b[A\n",
            "Training:  99% 1153/1164 [06:46<00:04,  2.67it/s]\u001b[A\n",
            "Training:  99% 1154/1164 [06:47<00:03,  2.73it/s]\u001b[A\n",
            "Training:  99% 1155/1164 [06:47<00:03,  2.69it/s]\u001b[A\n",
            "Training:  99% 1156/1164 [06:47<00:02,  2.76it/s]\u001b[A\n",
            "Training:  99% 1157/1164 [06:48<00:02,  2.66it/s]\u001b[A\n",
            "Training:  99% 1158/1164 [06:48<00:02,  2.60it/s]\u001b[A\n",
            "Training: 100% 1159/1164 [06:49<00:01,  2.69it/s]\u001b[A\n",
            "Training: 100% 1160/1164 [06:49<00:01,  2.81it/s]\u001b[A\n",
            "Training: 100% 1161/1164 [06:49<00:01,  2.87it/s]\u001b[A\n",
            "Training: 100% 1162/1164 [06:50<00:00,  2.85it/s]\u001b[A\n",
            "Training: 100% 1163/1164 [06:50<00:00,  2.86it/s]\u001b[A\n",
            "Training: 100% 1164/1164 [06:50<00:00,  2.83it/s]\n",
            "\n",
            "  0% 0/144 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 1/144 [00:00<00:20,  7.06it/s]\u001b[A\n",
            "  1% 2/144 [00:00<00:19,  7.22it/s]\u001b[A\n",
            "  2% 3/144 [00:00<00:19,  7.35it/s]\u001b[A\n",
            "  3% 4/144 [00:00<00:18,  7.65it/s]\u001b[A\n",
            "  3% 5/144 [00:00<00:17,  7.90it/s]\u001b[A\n",
            "  4% 6/144 [00:00<00:16,  8.19it/s]\u001b[A\n",
            "  5% 7/144 [00:00<00:16,  8.10it/s]\u001b[A\n",
            "  6% 8/144 [00:01<00:17,  7.80it/s]\u001b[A\n",
            "  6% 9/144 [00:01<00:17,  7.68it/s]\u001b[A\n",
            "  7% 10/144 [00:01<00:16,  7.91it/s]\u001b[A\n",
            "  8% 11/144 [00:01<00:17,  7.75it/s]\u001b[A\n",
            "  8% 12/144 [00:01<00:16,  7.95it/s]\u001b[A\n",
            "  9% 13/144 [00:01<00:16,  8.16it/s]\u001b[A\n",
            " 10% 14/144 [00:01<00:16,  7.94it/s]\u001b[A\n",
            " 10% 15/144 [00:01<00:15,  8.17it/s]\u001b[A\n",
            " 11% 16/144 [00:02<00:15,  8.23it/s]\u001b[A\n",
            " 12% 17/144 [00:02<00:15,  8.13it/s]\u001b[A\n",
            " 12% 18/144 [00:02<00:15,  8.18it/s]\u001b[A\n",
            " 13% 19/144 [00:02<00:15,  8.29it/s]\u001b[A\n",
            " 14% 20/144 [00:02<00:14,  8.36it/s]\u001b[A\n",
            " 15% 21/144 [00:02<00:14,  8.55it/s]\u001b[A\n",
            " 15% 22/144 [00:02<00:14,  8.41it/s]\u001b[A\n",
            " 16% 23/144 [00:02<00:14,  8.20it/s]\u001b[A\n",
            " 17% 24/144 [00:02<00:14,  8.08it/s]\u001b[A\n",
            " 17% 25/144 [00:03<00:15,  7.92it/s]\u001b[A\n",
            " 18% 26/144 [00:03<00:15,  7.58it/s]\u001b[A\n",
            " 19% 27/144 [00:03<00:15,  7.40it/s]\u001b[A\n",
            " 19% 28/144 [00:03<00:15,  7.40it/s]\u001b[A\n",
            " 20% 29/144 [00:03<00:15,  7.64it/s]\u001b[A\n",
            " 21% 30/144 [00:03<00:14,  7.78it/s]\u001b[A\n",
            " 22% 31/144 [00:03<00:14,  7.77it/s]\u001b[A\n",
            " 22% 32/144 [00:04<00:15,  7.42it/s]\u001b[A\n",
            " 23% 33/144 [00:04<00:15,  7.35it/s]\u001b[A\n",
            " 24% 34/144 [00:04<00:14,  7.56it/s]\u001b[A\n",
            " 24% 35/144 [00:04<00:14,  7.74it/s]\u001b[A\n",
            " 25% 36/144 [00:04<00:13,  8.00it/s]\u001b[A\n",
            " 26% 37/144 [00:04<00:13,  7.90it/s]\u001b[A\n",
            " 26% 38/144 [00:04<00:13,  8.03it/s]\u001b[A\n",
            " 27% 39/144 [00:04<00:12,  8.25it/s]\u001b[A\n",
            " 28% 40/144 [00:05<00:13,  7.75it/s]\u001b[A\n",
            " 28% 41/144 [00:05<00:13,  7.70it/s]\u001b[A\n",
            " 29% 42/144 [00:05<00:12,  8.09it/s]\u001b[A\n",
            " 30% 43/144 [00:05<00:12,  8.26it/s]\u001b[A\n",
            " 31% 44/144 [00:05<00:11,  8.57it/s]\u001b[A\n",
            " 31% 45/144 [00:05<00:12,  8.21it/s]\u001b[A\n",
            " 32% 46/144 [00:05<00:12,  8.05it/s]\u001b[A\n",
            " 33% 47/144 [00:05<00:11,  8.14it/s]\u001b[A\n",
            " 33% 48/144 [00:06<00:11,  8.24it/s]\u001b[A\n",
            " 34% 49/144 [00:06<00:11,  7.97it/s]\u001b[A\n",
            " 35% 50/144 [00:06<00:11,  7.99it/s]\u001b[A\n",
            " 35% 51/144 [00:06<00:11,  8.05it/s]\u001b[A\n",
            " 36% 52/144 [00:06<00:11,  8.33it/s]\u001b[A\n",
            " 37% 53/144 [00:06<00:10,  8.38it/s]\u001b[A\n",
            " 38% 54/144 [00:06<00:10,  8.20it/s]\u001b[A\n",
            " 38% 55/144 [00:06<00:10,  8.33it/s]\u001b[A\n",
            " 39% 56/144 [00:07<00:10,  8.07it/s]\u001b[A\n",
            " 40% 57/144 [00:07<00:10,  7.97it/s]\u001b[A\n",
            " 40% 58/144 [00:07<00:10,  8.02it/s]\u001b[A\n",
            " 41% 59/144 [00:07<00:10,  8.20it/s]\u001b[A\n",
            " 42% 60/144 [00:07<00:10,  8.34it/s]\u001b[A\n",
            " 42% 61/144 [00:07<00:09,  8.36it/s]\u001b[A\n",
            " 43% 62/144 [00:07<00:09,  8.50it/s]\u001b[A\n",
            " 44% 63/144 [00:07<00:09,  8.21it/s]\u001b[A\n",
            " 44% 64/144 [00:07<00:09,  8.07it/s]\u001b[A\n",
            " 45% 65/144 [00:08<00:09,  8.17it/s]\u001b[A\n",
            " 46% 66/144 [00:08<00:09,  8.26it/s]\u001b[A\n",
            " 47% 67/144 [00:08<00:09,  8.34it/s]\u001b[A\n",
            " 47% 68/144 [00:08<00:09,  8.18it/s]\u001b[A\n",
            " 48% 69/144 [00:08<00:09,  8.22it/s]\u001b[A\n",
            " 49% 70/144 [00:08<00:09,  8.07it/s]\u001b[A\n",
            " 49% 71/144 [00:08<00:08,  8.38it/s]\u001b[A\n",
            " 50% 72/144 [00:08<00:08,  8.59it/s]\u001b[A\n",
            " 51% 73/144 [00:09<00:08,  8.48it/s]\u001b[A\n",
            " 51% 74/144 [00:09<00:08,  8.52it/s]\u001b[A\n",
            " 52% 75/144 [00:09<00:07,  8.71it/s]\u001b[A\n",
            " 53% 76/144 [00:09<00:07,  8.70it/s]\u001b[A\n",
            " 53% 77/144 [00:09<00:07,  8.71it/s]\u001b[A\n",
            " 54% 78/144 [00:09<00:07,  8.58it/s]\u001b[A\n",
            " 55% 79/144 [00:09<00:07,  8.64it/s]\u001b[A\n",
            " 56% 80/144 [00:09<00:07,  8.68it/s]\u001b[A\n",
            " 56% 81/144 [00:10<00:07,  8.01it/s]\u001b[A\n",
            " 57% 82/144 [00:10<00:07,  7.87it/s]\u001b[A\n",
            " 58% 83/144 [00:10<00:07,  7.64it/s]\u001b[A\n",
            " 58% 84/144 [00:10<00:07,  7.51it/s]\u001b[A\n",
            " 59% 85/144 [00:10<00:08,  7.10it/s]\u001b[A\n",
            " 60% 86/144 [00:10<00:08,  7.19it/s]\u001b[A\n",
            " 60% 87/144 [00:10<00:07,  7.38it/s]\u001b[A\n",
            " 61% 88/144 [00:10<00:07,  7.46it/s]\u001b[A\n",
            " 62% 89/144 [00:11<00:07,  7.53it/s]\u001b[A\n",
            " 62% 90/144 [00:11<00:06,  7.83it/s]\u001b[A\n",
            " 63% 91/144 [00:11<00:06,  7.92it/s]\u001b[A\n",
            " 64% 92/144 [00:11<00:06,  8.12it/s]\u001b[A\n",
            " 65% 93/144 [00:11<00:06,  8.03it/s]\u001b[A\n",
            " 65% 94/144 [00:11<00:06,  7.98it/s]\u001b[A\n",
            " 66% 95/144 [00:11<00:06,  8.05it/s]\u001b[A\n",
            " 67% 96/144 [00:11<00:05,  8.10it/s]\u001b[A\n",
            " 67% 97/144 [00:12<00:05,  8.09it/s]\u001b[A\n",
            " 68% 98/144 [00:12<00:06,  7.51it/s]\u001b[A\n",
            " 69% 99/144 [00:12<00:05,  7.51it/s]\u001b[A\n",
            " 69% 100/144 [00:12<00:06,  7.01it/s]\u001b[A\n",
            " 70% 101/144 [00:12<00:06,  7.08it/s]\u001b[A\n",
            " 71% 102/144 [00:13<00:11,  3.66it/s]\u001b[A\n",
            " 72% 103/144 [00:13<00:09,  4.19it/s]\u001b[A\n",
            " 72% 104/144 [00:13<00:08,  4.77it/s]\u001b[A\n",
            " 73% 105/144 [00:13<00:07,  5.30it/s]\u001b[A\n",
            " 74% 106/144 [00:13<00:06,  5.74it/s]\u001b[A\n",
            " 74% 107/144 [00:13<00:05,  6.20it/s]\u001b[A\n",
            " 75% 108/144 [00:14<00:05,  6.45it/s]\u001b[A\n",
            " 76% 109/144 [00:14<00:05,  6.93it/s]\u001b[A\n",
            " 76% 110/144 [00:14<00:04,  7.11it/s]\u001b[A\n",
            " 77% 111/144 [00:14<00:04,  7.34it/s]\u001b[A\n",
            " 78% 112/144 [00:14<00:04,  7.50it/s]\u001b[A\n",
            " 78% 113/144 [00:14<00:04,  7.57it/s]\u001b[A\n",
            " 79% 114/144 [00:14<00:03,  7.78it/s]\u001b[A\n",
            " 80% 115/144 [00:15<00:03,  7.49it/s]\u001b[A\n",
            " 81% 116/144 [00:15<00:03,  7.59it/s]\u001b[A\n",
            " 81% 117/144 [00:15<00:03,  7.58it/s]\u001b[A\n",
            " 82% 118/144 [00:15<00:03,  7.63it/s]\u001b[A\n",
            " 83% 119/144 [00:15<00:03,  7.76it/s]\u001b[A\n",
            " 83% 120/144 [00:15<00:03,  7.80it/s]\u001b[A\n",
            " 84% 121/144 [00:15<00:02,  7.89it/s]\u001b[A\n",
            " 85% 122/144 [00:15<00:02,  7.99it/s]\u001b[A\n",
            " 85% 123/144 [00:16<00:02,  7.80it/s]\u001b[A\n",
            " 86% 124/144 [00:16<00:02,  7.92it/s]\u001b[A\n",
            " 87% 125/144 [00:16<00:02,  8.06it/s]\u001b[A\n",
            " 88% 126/144 [00:16<00:02,  8.05it/s]\u001b[A\n",
            " 88% 127/144 [00:16<00:02,  8.15it/s]\u001b[A\n",
            " 89% 128/144 [00:16<00:01,  8.19it/s]\u001b[A\n",
            " 90% 129/144 [00:16<00:01,  8.05it/s]\u001b[A\n",
            " 90% 130/144 [00:16<00:01,  7.91it/s]\u001b[A\n",
            " 91% 131/144 [00:17<00:01,  7.63it/s]\u001b[A\n",
            " 92% 132/144 [00:17<00:01,  7.57it/s]\u001b[A\n",
            " 92% 133/144 [00:17<00:01,  7.16it/s]\u001b[A\n",
            " 93% 134/144 [00:17<00:01,  7.02it/s]\u001b[A\n",
            " 94% 135/144 [00:17<00:01,  6.92it/s]\u001b[A\n",
            " 94% 136/144 [00:17<00:01,  6.88it/s]\u001b[A\n",
            " 95% 137/144 [00:17<00:00,  7.10it/s]\u001b[A\n",
            " 96% 138/144 [00:18<00:00,  6.74it/s]\u001b[A\n",
            " 97% 139/144 [00:18<00:00,  6.86it/s]\u001b[A\n",
            " 97% 140/144 [00:18<00:00,  6.99it/s]\u001b[A\n",
            " 98% 141/144 [00:18<00:00,  6.77it/s]\u001b[A\n",
            " 99% 142/144 [00:18<00:00,  6.67it/s]\u001b[A\n",
            "100% 144/144 [00:18<00:00,  7.62it/s]\n",
            "Ner output will be written to ../ner_turkish_mbert_1/NER_mbert_tr_joint_ner_out.txt\n",
            "processed 41949 tokens with 3019 phrases; found: 3381 phrases; correct: 2765.\n",
            "accuracy:  95.02%; (non-O)\n",
            "accuracy:  98.12%; precision:  81.78%; recall:  91.59%; FB1:  86.41\n",
            "              LOC: precision:  84.15%; recall:  90.80%; FB1:  87.35  833\n",
            "              ORG: precision:  87.79%; recall:  88.00%; FB1:  87.89  827\n",
            "              PER: precision:  77.75%; recall:  94.09%; FB1:  85.14  1721\n",
            "Saving best ner model to NER_mbert_tr_best_ner_model.pkh \n",
            "Epoch:   5% 1/20 [07:26<2:21:28, 446.74s/it]\n",
            "Training:   0% 0/1164 [00:00<?, ?it/s]\u001b[A\n",
            "Training:   0% 1/1164 [00:00<11:37,  1.67it/s]\u001b[A\n",
            "Training:   0% 2/1164 [00:01<10:42,  1.81it/s]\u001b[A\n",
            "Training:   0% 3/1164 [00:01<10:13,  1.89it/s]\u001b[A\n",
            "Training:   0% 4/1164 [00:01<09:42,  1.99it/s]\u001b[A\n",
            "Training:   0% 5/1164 [00:02<08:54,  2.17it/s]\u001b[A\n",
            "Training:   1% 6/1164 [00:02<08:37,  2.24it/s]\u001b[A\n",
            "Training:   1% 7/1164 [00:03<08:07,  2.37it/s]\u001b[A\n",
            "Training:   1% 8/1164 [00:03<08:05,  2.38it/s]\u001b[A\n",
            "Training:   1% 9/1164 [00:03<08:12,  2.35it/s]\u001b[A\n",
            "Training:   1% 10/1164 [00:04<08:01,  2.40it/s]\u001b[A\n",
            "Training:   1% 11/1164 [00:04<08:01,  2.39it/s]\u001b[A\n",
            "Training:   1% 12/1164 [00:05<07:45,  2.47it/s]\u001b[A\n",
            "Training:   1% 13/1164 [00:05<08:11,  2.34it/s]\u001b[A\n",
            "Training:   1% 14/1164 [00:06<08:00,  2.39it/s]\u001b[A\n",
            "Training:   1% 15/1164 [00:06<08:28,  2.26it/s]\u001b[A\n",
            "Training:   1% 16/1164 [00:06<08:19,  2.30it/s]\u001b[A\n",
            "Training:   1% 17/1164 [00:07<08:53,  2.15it/s]\u001b[A\n",
            "Training:   2% 18/1164 [00:07<08:46,  2.18it/s]\u001b[A\n",
            "Training:   2% 19/1164 [00:08<08:45,  2.18it/s]\u001b[A\n",
            "Training:   2% 20/1164 [00:08<08:37,  2.21it/s]\u001b[A\n",
            "Training:   2% 21/1164 [00:09<08:40,  2.20it/s]\u001b[A\n",
            "Training:   2% 22/1164 [00:09<08:46,  2.17it/s]\u001b[A\n",
            "Training:   2% 23/1164 [00:10<09:06,  2.09it/s]\u001b[A\n",
            "Training:   2% 24/1164 [00:10<09:18,  2.04it/s]\u001b[A\n",
            "Training:   2% 25/1164 [00:11<09:05,  2.09it/s]\u001b[A\n",
            "Training:   2% 26/1164 [00:11<09:10,  2.07it/s]\u001b[A\n",
            "Training:   2% 27/1164 [00:12<09:05,  2.09it/s]\u001b[A\n",
            "Training:   2% 28/1164 [00:12<09:11,  2.06it/s]\u001b[A\n",
            "Training:   2% 29/1164 [00:13<09:06,  2.08it/s]\u001b[A\n",
            "Training:   3% 30/1164 [00:13<09:30,  1.99it/s]\u001b[A\n",
            "Training:   3% 31/1164 [00:14<09:10,  2.06it/s]\u001b[A\n",
            "Training:   3% 32/1164 [00:14<09:20,  2.02it/s]\u001b[A\n",
            "Training:   3% 33/1164 [00:15<09:23,  2.01it/s]\u001b[A\n",
            "Training:   3% 34/1164 [00:15<08:51,  2.13it/s]\u001b[A\n",
            "Training:   3% 35/1164 [00:15<08:19,  2.26it/s]\u001b[A\n",
            "Training:   3% 36/1164 [00:16<07:59,  2.35it/s]\u001b[A\n",
            "Training:   3% 37/1164 [00:16<07:51,  2.39it/s]\u001b[A\n",
            "Training:   3% 38/1164 [00:17<08:02,  2.33it/s]\u001b[A\n",
            "Training:   3% 39/1164 [00:17<07:56,  2.36it/s]\u001b[A\n",
            "Training:   3% 40/1164 [00:18<07:40,  2.44it/s]\u001b[A\n",
            "Training:   4% 41/1164 [00:18<07:50,  2.39it/s]\u001b[A\n",
            "Training:   4% 42/1164 [00:18<07:25,  2.52it/s]\u001b[A\n",
            "Training:   4% 43/1164 [00:19<07:03,  2.65it/s]\u001b[A\n",
            "Training:   4% 44/1164 [00:19<07:02,  2.65it/s]\u001b[A\n",
            "Training:   4% 45/1164 [00:19<06:52,  2.71it/s]\u001b[A\n",
            "Training:   4% 46/1164 [00:20<06:41,  2.78it/s]\u001b[A\n",
            "Training:   4% 47/1164 [00:20<06:35,  2.83it/s]\u001b[A\n",
            "Training:   4% 48/1164 [00:20<06:51,  2.71it/s]\u001b[A\n",
            "Training:   4% 49/1164 [00:21<06:34,  2.83it/s]\u001b[A\n",
            "Training:   4% 50/1164 [00:21<06:35,  2.82it/s]\u001b[A\n",
            "Training:   4% 51/1164 [00:21<06:37,  2.80it/s]\u001b[A\n",
            "Training:   4% 52/1164 [00:22<06:34,  2.82it/s]\u001b[A\n",
            "Training:   5% 53/1164 [00:22<07:04,  2.62it/s]\u001b[A\n",
            "Training:   5% 54/1164 [00:23<06:57,  2.66it/s]\u001b[A\n",
            "Training:   5% 55/1164 [00:23<07:16,  2.54it/s]\u001b[A\n",
            "Training:   5% 56/1164 [00:23<07:17,  2.53it/s]\u001b[A\n",
            "Training:   5% 57/1164 [00:24<07:06,  2.59it/s]\u001b[A\n",
            "Training:   5% 58/1164 [00:24<06:50,  2.70it/s]\u001b[A\n",
            "Training:   5% 59/1164 [00:24<06:38,  2.77it/s]\u001b[A\n",
            "Training:   5% 60/1164 [00:25<06:29,  2.83it/s]\u001b[A\n",
            "Training:   5% 61/1164 [00:25<06:25,  2.86it/s]\u001b[A\n",
            "Training:   5% 62/1164 [00:26<06:28,  2.83it/s]\u001b[A\n",
            "Training:   5% 63/1164 [00:26<06:27,  2.84it/s]\u001b[A\n",
            "Training:   5% 64/1164 [00:26<06:30,  2.82it/s]\u001b[A\n",
            "Training:   6% 65/1164 [00:27<06:22,  2.87it/s]\u001b[A\n",
            "Training:   6% 66/1164 [00:27<06:27,  2.84it/s]\u001b[A\n",
            "Training:   6% 67/1164 [00:27<06:27,  2.83it/s]\u001b[A\n",
            "Training:   6% 68/1164 [00:28<06:25,  2.84it/s]\u001b[A\n",
            "Training:   6% 69/1164 [00:28<06:16,  2.91it/s]\u001b[A\n",
            "Training:   6% 70/1164 [00:28<06:32,  2.79it/s]\u001b[A\n",
            "Training:   6% 71/1164 [00:29<06:39,  2.74it/s]\u001b[A\n",
            "Training:   6% 72/1164 [00:29<06:28,  2.81it/s]\u001b[A\n",
            "Training:   6% 73/1164 [00:29<06:28,  2.81it/s]\u001b[A\n",
            "Training:   6% 74/1164 [00:30<06:16,  2.89it/s]\u001b[A\n",
            "Training:   6% 75/1164 [00:30<06:44,  2.69it/s]\u001b[A\n",
            "Training:   7% 76/1164 [00:31<06:47,  2.67it/s]\u001b[A\n",
            "Training:   7% 77/1164 [00:31<06:39,  2.72it/s]\u001b[A\n",
            "Training:   7% 78/1164 [00:31<06:41,  2.71it/s]\u001b[A\n",
            "Training:   7% 79/1164 [00:32<06:38,  2.72it/s]\u001b[A\n",
            "Training:   7% 80/1164 [00:32<06:43,  2.69it/s]\u001b[A\n",
            "Training:   7% 81/1164 [00:32<06:47,  2.66it/s]\u001b[A\n",
            "Training:   7% 82/1164 [00:33<06:40,  2.70it/s]\u001b[A\n",
            "Training:   7% 83/1164 [00:33<06:26,  2.80it/s]\u001b[A\n",
            "Training:   7% 84/1164 [00:33<06:20,  2.84it/s]\u001b[A\n",
            "Training:   7% 85/1164 [00:34<06:25,  2.80it/s]\u001b[A\n",
            "Training:   7% 86/1164 [00:34<06:35,  2.72it/s]\u001b[A\n",
            "Training:   7% 87/1164 [00:35<06:54,  2.60it/s]\u001b[A\n",
            "Training:   8% 88/1164 [00:35<06:56,  2.59it/s]\u001b[A\n",
            "Training:   8% 89/1164 [00:35<06:51,  2.61it/s]\u001b[A\n",
            "Training:   8% 90/1164 [00:36<06:22,  2.81it/s]\u001b[A\n",
            "Training:   8% 91/1164 [00:36<06:28,  2.76it/s]\u001b[A\n",
            "Training:   8% 92/1164 [00:36<06:25,  2.78it/s]\u001b[A\n",
            "Training:   8% 93/1164 [00:37<06:25,  2.78it/s]\u001b[A\n",
            "Training:   8% 94/1164 [00:37<06:32,  2.72it/s]\u001b[A\n",
            "Training:   8% 95/1164 [00:38<06:41,  2.66it/s]\u001b[A\n",
            "Training:   8% 96/1164 [00:38<06:43,  2.65it/s]\u001b[A\n",
            "Training:   8% 97/1164 [00:38<06:38,  2.68it/s]\u001b[A\n",
            "Training:   8% 98/1164 [00:39<06:32,  2.72it/s]\u001b[A\n",
            "Training:   9% 99/1164 [00:39<06:28,  2.74it/s]\u001b[A\n",
            "Training:   9% 100/1164 [00:39<06:19,  2.80it/s]\u001b[A\n",
            "Training:   9% 101/1164 [00:40<06:09,  2.88it/s]\u001b[A\n",
            "Training:   9% 102/1164 [00:40<06:04,  2.92it/s]\u001b[A\n",
            "Training:   9% 103/1164 [00:40<05:59,  2.95it/s]\u001b[A\n",
            "Training:   9% 104/1164 [00:41<06:00,  2.94it/s]\u001b[A\n",
            "Training:   9% 105/1164 [00:41<06:02,  2.92it/s]\u001b[A\n",
            "Training:   9% 106/1164 [00:41<06:10,  2.85it/s]\u001b[A\n",
            "Training:   9% 107/1164 [00:42<06:12,  2.84it/s]\u001b[A\n",
            "Training:   9% 108/1164 [00:42<06:03,  2.90it/s]\u001b[A\n",
            "Training:   9% 109/1164 [00:42<05:58,  2.94it/s]\u001b[A\n",
            "Training:   9% 110/1164 [00:43<06:12,  2.83it/s]\u001b[A\n",
            "Training:  10% 111/1164 [00:43<06:09,  2.85it/s]\u001b[A\n",
            "Training:  10% 112/1164 [00:44<06:09,  2.85it/s]\u001b[A\n",
            "Training:  10% 113/1164 [00:44<06:11,  2.83it/s]\u001b[A\n",
            "Training:  10% 114/1164 [00:44<06:07,  2.86it/s]\u001b[A\n",
            "Training:  10% 115/1164 [00:45<06:21,  2.75it/s]\u001b[A\n",
            "Training:  10% 116/1164 [00:45<06:28,  2.70it/s]\u001b[A\n",
            "Training:  10% 117/1164 [00:45<06:30,  2.68it/s]\u001b[A\n",
            "Training:  10% 118/1164 [00:46<06:19,  2.75it/s]\u001b[A\n",
            "Training:  10% 119/1164 [00:46<06:11,  2.81it/s]\u001b[A\n",
            "Training:  10% 120/1164 [00:46<06:03,  2.87it/s]\u001b[A\n",
            "Training:  10% 121/1164 [00:47<06:16,  2.77it/s]\u001b[A\n",
            "Training:  10% 122/1164 [00:47<06:24,  2.71it/s]\u001b[A\n",
            "Training:  11% 123/1164 [00:48<06:23,  2.71it/s]\u001b[A\n",
            "Training:  11% 124/1164 [00:48<06:15,  2.77it/s]\u001b[A\n",
            "Training:  11% 125/1164 [00:48<06:13,  2.78it/s]\u001b[A\n",
            "Training:  11% 126/1164 [00:49<06:00,  2.88it/s]\u001b[A\n",
            "Training:  11% 127/1164 [00:49<05:56,  2.91it/s]\u001b[A\n",
            "Training:  11% 128/1164 [00:49<06:01,  2.86it/s]\u001b[A\n",
            "Training:  11% 129/1164 [00:50<05:52,  2.94it/s]\u001b[A\n",
            "Training:  11% 130/1164 [00:50<06:06,  2.82it/s]\u001b[A\n",
            "Training:  11% 131/1164 [00:50<05:57,  2.89it/s]\u001b[A\n",
            "Training:  11% 132/1164 [00:51<06:00,  2.86it/s]\u001b[A\n",
            "Training:  11% 133/1164 [00:51<06:06,  2.82it/s]\u001b[A\n",
            "Training:  12% 134/1164 [00:51<06:11,  2.77it/s]\u001b[A\n",
            "Training:  12% 135/1164 [00:52<06:10,  2.78it/s]\u001b[A\n",
            "Training:  12% 136/1164 [00:52<06:06,  2.80it/s]\u001b[A\n",
            "Training:  12% 137/1164 [00:52<06:00,  2.85it/s]\u001b[A\n",
            "Training:  12% 138/1164 [00:53<05:58,  2.86it/s]\u001b[A\n",
            "Training:  12% 139/1164 [00:53<06:21,  2.68it/s]\u001b[A\n",
            "Training:  12% 140/1164 [00:54<06:06,  2.80it/s]\u001b[A\n",
            "Training:  12% 141/1164 [00:54<05:54,  2.89it/s]\u001b[A\n",
            "Training:  12% 142/1164 [00:54<05:55,  2.87it/s]\u001b[A\n",
            "Training:  12% 143/1164 [00:55<05:56,  2.86it/s]\u001b[A\n",
            "Training:  12% 144/1164 [00:55<05:58,  2.85it/s]\u001b[A\n",
            "Training:  12% 145/1164 [00:55<06:06,  2.78it/s]\u001b[A\n",
            "Training:  13% 146/1164 [00:56<05:57,  2.85it/s]\u001b[A\n",
            "Training:  13% 147/1164 [00:56<06:00,  2.82it/s]\u001b[A\n",
            "Training:  13% 148/1164 [00:56<05:58,  2.83it/s]\u001b[A\n",
            "Training:  13% 149/1164 [00:57<05:58,  2.83it/s]\u001b[A\n",
            "Training:  13% 150/1164 [00:57<05:59,  2.82it/s]\u001b[A\n",
            "Training:  13% 151/1164 [00:57<05:50,  2.89it/s]\u001b[A\n",
            "Training:  13% 152/1164 [00:58<05:40,  2.98it/s]\u001b[A\n",
            "Training:  13% 153/1164 [00:58<05:42,  2.95it/s]\u001b[A\n",
            "Training:  13% 154/1164 [00:58<05:43,  2.94it/s]\u001b[A\n",
            "Training:  13% 155/1164 [00:59<05:43,  2.94it/s]\u001b[A\n",
            "Training:  13% 156/1164 [00:59<05:42,  2.94it/s]\u001b[A\n",
            "Training:  13% 157/1164 [00:59<05:38,  2.98it/s]\u001b[A\n",
            "Training:  14% 158/1164 [01:00<05:41,  2.94it/s]\u001b[A\n",
            "Training:  14% 159/1164 [01:00<05:36,  2.99it/s]\u001b[A\n",
            "Training:  14% 160/1164 [01:00<05:26,  3.07it/s]\u001b[A\n",
            "Training:  14% 161/1164 [01:01<05:34,  3.00it/s]\u001b[A\n",
            "Training:  14% 162/1164 [01:01<05:52,  2.84it/s]\u001b[A\n",
            "Training:  14% 163/1164 [01:01<05:43,  2.91it/s]\u001b[A\n",
            "Training:  14% 164/1164 [01:02<05:48,  2.87it/s]\u001b[A\n",
            "Training:  14% 165/1164 [01:02<05:48,  2.87it/s]\u001b[A\n",
            "Training:  14% 166/1164 [01:02<05:57,  2.80it/s]\u001b[A\n",
            "Training:  14% 167/1164 [01:03<06:00,  2.77it/s]\u001b[A\n",
            "Training:  14% 168/1164 [01:03<05:49,  2.85it/s]\u001b[A\n",
            "Training:  15% 169/1164 [01:04<06:04,  2.73it/s]\u001b[A\n",
            "Training:  15% 170/1164 [01:04<06:12,  2.67it/s]\u001b[A\n",
            "Training:  15% 171/1164 [01:04<06:15,  2.65it/s]\u001b[A\n",
            "Training:  15% 172/1164 [01:05<06:12,  2.66it/s]\u001b[A\n",
            "Training:  15% 173/1164 [01:05<06:01,  2.74it/s]\u001b[A\n",
            "Training:  15% 174/1164 [01:05<06:12,  2.66it/s]\u001b[A\n",
            "Training:  15% 175/1164 [01:06<05:50,  2.82it/s]\u001b[A\n",
            "Training:  15% 176/1164 [01:06<05:41,  2.89it/s]\u001b[A\n",
            "Training:  15% 177/1164 [01:06<05:46,  2.85it/s]\u001b[A\n",
            "Training:  15% 178/1164 [01:07<05:48,  2.83it/s]\u001b[A\n",
            "Training:  15% 179/1164 [01:07<05:46,  2.84it/s]\u001b[A\n",
            "Training:  15% 180/1164 [01:08<05:46,  2.84it/s]\u001b[A\n",
            "Training:  16% 181/1164 [01:08<05:37,  2.91it/s]\u001b[A\n",
            "Training:  16% 182/1164 [01:08<05:27,  2.99it/s]\u001b[A\n",
            "Training:  16% 183/1164 [01:09<05:27,  2.99it/s]\u001b[A\n",
            "Training:  16% 184/1164 [01:09<05:28,  2.99it/s]\u001b[A\n",
            "Training:  16% 185/1164 [01:09<05:43,  2.85it/s]\u001b[A\n",
            "Training:  16% 186/1164 [01:10<05:52,  2.77it/s]\u001b[A\n",
            "Training:  16% 187/1164 [01:10<05:42,  2.85it/s]\u001b[A\n",
            "Training:  16% 188/1164 [01:10<05:32,  2.94it/s]\u001b[A\n",
            "Training:  16% 189/1164 [01:11<05:31,  2.94it/s]\u001b[A\n",
            "Training:  16% 190/1164 [01:11<05:35,  2.90it/s]\u001b[A\n",
            "Training:  16% 191/1164 [01:11<05:37,  2.88it/s]\u001b[A\n",
            "Training:  16% 192/1164 [01:12<05:50,  2.78it/s]\u001b[A\n",
            "Training:  17% 193/1164 [01:12<05:50,  2.77it/s]\u001b[A\n",
            "Training:  17% 194/1164 [01:12<05:43,  2.82it/s]\u001b[A\n",
            "Training:  17% 195/1164 [01:13<05:43,  2.82it/s]\u001b[A\n",
            "Training:  17% 196/1164 [01:13<05:42,  2.83it/s]\u001b[A\n",
            "Training:  17% 197/1164 [01:13<05:39,  2.85it/s]\u001b[A\n",
            "Training:  17% 198/1164 [01:14<05:28,  2.94it/s]\u001b[A\n",
            "Training:  17% 199/1164 [01:14<05:32,  2.91it/s]\u001b[A\n",
            "Training:  17% 200/1164 [01:14<05:30,  2.91it/s]\u001b[A\n",
            "Training:  17% 201/1164 [01:15<05:24,  2.97it/s]\u001b[A\n",
            "Training:  17% 202/1164 [01:15<05:32,  2.90it/s]\u001b[A\n",
            "Training:  17% 203/1164 [01:16<05:35,  2.86it/s]\u001b[A\n",
            "Training:  18% 204/1164 [01:16<05:28,  2.92it/s]\u001b[A\n",
            "Training:  18% 205/1164 [01:16<05:23,  2.96it/s]\u001b[A\n",
            "Training:  18% 206/1164 [01:17<05:28,  2.91it/s]\u001b[A\n",
            "Training:  18% 207/1164 [01:17<05:27,  2.93it/s]\u001b[A\n",
            "Training:  18% 208/1164 [01:17<05:46,  2.76it/s]\u001b[A\n",
            "Training:  18% 209/1164 [01:18<05:45,  2.76it/s]\u001b[A\n",
            "Training:  18% 210/1164 [01:18<05:41,  2.79it/s]\u001b[A\n",
            "Training:  18% 211/1164 [01:18<05:37,  2.82it/s]\u001b[A\n",
            "Training:  18% 212/1164 [01:19<05:34,  2.85it/s]\u001b[A\n",
            "Training:  18% 213/1164 [01:19<05:22,  2.95it/s]\u001b[A\n",
            "Training:  18% 214/1164 [01:19<05:22,  2.94it/s]\u001b[A\n",
            "Training:  18% 215/1164 [01:20<05:37,  2.81it/s]\u001b[A\n",
            "Training:  19% 216/1164 [01:20<05:39,  2.79it/s]\u001b[A\n",
            "Training:  19% 217/1164 [01:20<05:26,  2.90it/s]\u001b[A\n",
            "Training:  19% 218/1164 [01:21<05:24,  2.91it/s]\u001b[A\n",
            "Training:  19% 219/1164 [01:21<05:39,  2.78it/s]\u001b[A\n",
            "Training:  19% 220/1164 [01:21<05:39,  2.78it/s]\u001b[A\n",
            "Training:  19% 221/1164 [01:22<05:35,  2.81it/s]\u001b[A\n",
            "Training:  19% 222/1164 [01:22<05:32,  2.84it/s]\u001b[A\n",
            "Training:  19% 223/1164 [01:23<05:27,  2.88it/s]\u001b[A\n",
            "Training:  19% 224/1164 [01:23<05:22,  2.91it/s]\u001b[A\n",
            "Training:  19% 225/1164 [01:23<05:36,  2.79it/s]\u001b[A\n",
            "Training:  19% 226/1164 [01:24<05:57,  2.62it/s]\u001b[A\n",
            "Training:  20% 227/1164 [01:24<05:39,  2.76it/s]\u001b[A\n",
            "Training:  20% 228/1164 [01:24<05:35,  2.79it/s]\u001b[A\n",
            "Training:  20% 229/1164 [01:25<05:29,  2.84it/s]\u001b[A\n",
            "Training:  20% 230/1164 [01:25<05:24,  2.88it/s]\u001b[A\n",
            "Training:  20% 231/1164 [01:25<05:22,  2.89it/s]\u001b[A\n",
            "Training:  20% 232/1164 [01:26<05:20,  2.91it/s]\u001b[A\n",
            "Training:  20% 233/1164 [01:26<05:17,  2.93it/s]\u001b[A\n",
            "Training:  20% 234/1164 [01:26<05:12,  2.98it/s]\u001b[A\n",
            "Training:  20% 235/1164 [01:27<05:16,  2.94it/s]\u001b[A\n",
            "Training:  20% 236/1164 [01:27<05:22,  2.88it/s]\u001b[A\n",
            "Training:  20% 237/1164 [01:27<05:16,  2.93it/s]\u001b[A\n",
            "Training:  20% 238/1164 [01:28<05:16,  2.93it/s]\u001b[A\n",
            "Training:  21% 239/1164 [01:28<05:07,  3.01it/s]\u001b[A\n",
            "Training:  21% 240/1164 [01:28<05:13,  2.95it/s]\u001b[A\n",
            "Training:  21% 241/1164 [01:29<05:19,  2.89it/s]\u001b[A\n",
            "Training:  21% 242/1164 [01:29<05:26,  2.82it/s]\u001b[A\n",
            "Training:  21% 243/1164 [01:29<05:19,  2.88it/s]\u001b[A\n",
            "Training:  21% 244/1164 [01:30<05:18,  2.89it/s]\u001b[A\n",
            "Training:  21% 245/1164 [01:30<05:22,  2.85it/s]\u001b[A\n",
            "Training:  21% 246/1164 [01:31<05:19,  2.87it/s]\u001b[A\n",
            "Training:  21% 247/1164 [01:31<05:16,  2.90it/s]\u001b[A\n",
            "Training:  21% 248/1164 [01:31<05:13,  2.92it/s]\u001b[A\n",
            "Training:  21% 249/1164 [01:32<05:20,  2.86it/s]\u001b[A\n",
            "Training:  21% 250/1164 [01:32<05:17,  2.87it/s]\u001b[A\n",
            "Training:  22% 251/1164 [01:32<05:24,  2.81it/s]\u001b[A\n",
            "Training:  22% 252/1164 [01:33<05:21,  2.84it/s]\u001b[A\n",
            "Training:  22% 253/1164 [01:33<05:47,  2.62it/s]\u001b[A\n",
            "Training:  22% 254/1164 [01:33<05:40,  2.67it/s]\u001b[A\n",
            "Training:  22% 255/1164 [01:34<05:34,  2.72it/s]\u001b[A\n",
            "Training:  22% 256/1164 [01:34<05:22,  2.82it/s]\u001b[A\n",
            "Training:  22% 257/1164 [01:34<05:19,  2.84it/s]\u001b[A\n",
            "Training:  22% 258/1164 [01:35<05:13,  2.89it/s]\u001b[A\n",
            "Training:  22% 259/1164 [01:35<05:13,  2.89it/s]\u001b[A\n",
            "Training:  22% 260/1164 [01:36<05:21,  2.81it/s]\u001b[A\n",
            "Training:  22% 261/1164 [01:36<05:16,  2.85it/s]\u001b[A\n",
            "Training:  23% 262/1164 [01:36<05:08,  2.93it/s]\u001b[A\n",
            "Training:  23% 263/1164 [01:37<05:13,  2.87it/s]\u001b[A\n",
            "Training:  23% 264/1164 [01:37<05:09,  2.91it/s]\u001b[A\n",
            "Training:  23% 265/1164 [01:37<05:06,  2.93it/s]\u001b[A\n",
            "Training:  23% 266/1164 [01:38<05:05,  2.94it/s]\u001b[A\n",
            "Training:  23% 267/1164 [01:38<05:17,  2.82it/s]\u001b[A\n",
            "Training:  23% 268/1164 [01:38<05:08,  2.90it/s]\u001b[A\n",
            "Training:  23% 269/1164 [01:39<05:03,  2.95it/s]\u001b[A\n",
            "Training:  23% 270/1164 [01:39<05:06,  2.92it/s]\u001b[A\n",
            "Training:  23% 271/1164 [01:39<05:17,  2.81it/s]\u001b[A\n",
            "Training:  23% 272/1164 [01:40<05:15,  2.83it/s]\u001b[A\n",
            "Training:  23% 273/1164 [01:40<05:15,  2.83it/s]\u001b[A\n",
            "Training:  24% 274/1164 [01:40<05:23,  2.75it/s]\u001b[A\n",
            "Training:  24% 275/1164 [01:41<05:19,  2.78it/s]\u001b[A\n",
            "Training:  24% 276/1164 [01:41<05:08,  2.88it/s]\u001b[A\n",
            "Training:  24% 277/1164 [01:41<05:07,  2.88it/s]\u001b[A\n",
            "Training:  24% 278/1164 [01:42<05:08,  2.87it/s]\u001b[A\n",
            "Training:  24% 279/1164 [01:42<05:12,  2.83it/s]\u001b[A\n",
            "Training:  24% 280/1164 [01:43<05:30,  2.68it/s]\u001b[A\n",
            "Training:  24% 281/1164 [01:43<05:29,  2.68it/s]\u001b[A\n",
            "Training:  24% 282/1164 [01:43<05:15,  2.80it/s]\u001b[A\n",
            "Training:  24% 283/1164 [01:44<05:11,  2.83it/s]\u001b[A\n",
            "Training:  24% 284/1164 [01:44<05:26,  2.69it/s]\u001b[A\n",
            "Training:  24% 285/1164 [01:44<05:26,  2.69it/s]\u001b[A\n",
            "Training:  25% 286/1164 [01:45<05:16,  2.77it/s]\u001b[A\n",
            "Training:  25% 287/1164 [01:45<05:14,  2.79it/s]\u001b[A\n",
            "Training:  25% 288/1164 [01:45<05:18,  2.75it/s]\u001b[A\n",
            "Training:  25% 289/1164 [01:46<05:26,  2.68it/s]\u001b[A\n",
            "Training:  25% 290/1164 [01:46<05:17,  2.75it/s]\u001b[A\n",
            "Training:  25% 291/1164 [01:47<05:07,  2.84it/s]\u001b[A\n",
            "Training:  25% 292/1164 [01:47<05:08,  2.83it/s]\u001b[A\n",
            "Training:  25% 293/1164 [01:47<05:06,  2.84it/s]\u001b[A\n",
            "Training:  25% 294/1164 [01:48<05:12,  2.78it/s]\u001b[A\n",
            "Training:  25% 295/1164 [01:48<05:06,  2.83it/s]\u001b[A\n",
            "Training:  25% 296/1164 [01:48<04:55,  2.94it/s]\u001b[A\n",
            "Training:  26% 297/1164 [01:49<04:56,  2.92it/s]\u001b[A\n",
            "Training:  26% 298/1164 [01:49<04:55,  2.93it/s]\u001b[A\n",
            "Training:  26% 299/1164 [01:49<05:18,  2.72it/s]\u001b[A\n",
            "Training:  26% 300/1164 [01:50<05:21,  2.69it/s]\u001b[A\n",
            "Training:  26% 301/1164 [01:50<05:26,  2.64it/s]\u001b[A\n",
            "Training:  26% 302/1164 [01:50<05:22,  2.68it/s]\u001b[A\n",
            "Training:  26% 303/1164 [01:51<05:21,  2.68it/s]\u001b[A\n",
            "Training:  26% 304/1164 [01:51<05:17,  2.71it/s]\u001b[A\n",
            "Training:  26% 305/1164 [01:52<05:15,  2.72it/s]\u001b[A\n",
            "Training:  26% 306/1164 [01:52<05:12,  2.75it/s]\u001b[A\n",
            "Training:  26% 307/1164 [01:52<05:14,  2.73it/s]\u001b[A\n",
            "Training:  26% 308/1164 [01:53<05:12,  2.74it/s]\u001b[A\n",
            "Training:  27% 309/1164 [01:53<05:09,  2.76it/s]\u001b[A\n",
            "Training:  27% 310/1164 [01:53<05:15,  2.70it/s]\u001b[A\n",
            "Training:  27% 311/1164 [01:54<05:09,  2.76it/s]\u001b[A\n",
            "Training:  27% 312/1164 [01:54<05:12,  2.73it/s]\u001b[A\n",
            "Training:  27% 313/1164 [01:55<05:20,  2.66it/s]\u001b[A\n",
            "Training:  27% 314/1164 [01:55<05:07,  2.76it/s]\u001b[A\n",
            "Training:  27% 315/1164 [01:55<05:02,  2.81it/s]\u001b[A\n",
            "Training:  27% 316/1164 [01:56<05:06,  2.76it/s]\u001b[A\n",
            "Training:  27% 317/1164 [01:56<05:09,  2.74it/s]\u001b[A\n",
            "Training:  27% 318/1164 [01:56<05:02,  2.79it/s]\u001b[A\n",
            "Training:  27% 319/1164 [01:57<05:02,  2.79it/s]\u001b[A\n",
            "Training:  27% 320/1164 [01:57<05:01,  2.80it/s]\u001b[A\n",
            "Training:  28% 321/1164 [01:57<05:10,  2.71it/s]\u001b[A\n",
            "Training:  28% 322/1164 [01:58<05:09,  2.72it/s]\u001b[A\n",
            "Training:  28% 323/1164 [01:58<05:04,  2.76it/s]\u001b[A\n",
            "Training:  28% 324/1164 [01:58<05:06,  2.74it/s]\u001b[A\n",
            "Training:  28% 325/1164 [01:59<05:00,  2.80it/s]\u001b[A\n",
            "Training:  28% 326/1164 [01:59<04:54,  2.85it/s]\u001b[A\n",
            "Training:  28% 327/1164 [02:00<04:56,  2.83it/s]\u001b[A\n",
            "Training:  28% 328/1164 [02:00<04:49,  2.89it/s]\u001b[A\n",
            "Training:  28% 329/1164 [02:00<04:50,  2.88it/s]\u001b[A\n",
            "Training:  28% 330/1164 [02:01<04:43,  2.94it/s]\u001b[A\n",
            "Training:  28% 331/1164 [02:01<04:44,  2.93it/s]\u001b[A\n",
            "Training:  29% 332/1164 [02:01<04:43,  2.93it/s]\u001b[A\n",
            "Training:  29% 333/1164 [02:02<04:38,  2.99it/s]\u001b[A\n",
            "Training:  29% 334/1164 [02:02<04:36,  3.00it/s]\u001b[A\n",
            "Training:  29% 335/1164 [02:02<04:49,  2.86it/s]\u001b[A\n",
            "Training:  29% 336/1164 [02:03<04:45,  2.90it/s]\u001b[A"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRGat4IGNo_N",
        "outputId": "e0313c2d-e1cd-42c7-dc83-b449f2c5c7c1"
      },
      "source": [
        "save_dir=\"../dep_turkish_bert_en_1\"\n",
        "# Training DEP model with bert_en for Turkish\n",
        "!python jointtrainer_multilang.py --model_type DEP  --word_embed_type mbert  --lang tr --save_dir $save_dir"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training:  19% 72/383 [00:13<00:57,  5.45it/s]\u001b[A\n",
            "Training:  19% 73/383 [00:13<00:55,  5.54it/s]\u001b[A\n",
            "Training:  19% 74/383 [00:13<00:55,  5.54it/s]\u001b[A\n",
            "Training:  20% 75/383 [00:14<00:56,  5.48it/s]\u001b[A\n",
            "Training:  20% 76/383 [00:14<00:56,  5.40it/s]\u001b[A\n",
            "Training:  20% 77/383 [00:14<00:56,  5.45it/s]\u001b[A\n",
            "Training:  20% 78/383 [00:14<00:55,  5.49it/s]\u001b[A\n",
            "Training:  21% 79/383 [00:14<00:54,  5.55it/s]\u001b[A\n",
            "Training:  21% 80/383 [00:15<00:55,  5.43it/s]\u001b[A\n",
            "Training:  21% 81/383 [00:15<00:56,  5.39it/s]\u001b[A\n",
            "Training:  21% 82/383 [00:15<00:55,  5.46it/s]\u001b[A\n",
            "Training:  22% 83/383 [00:15<00:54,  5.53it/s]\u001b[A\n",
            "Training:  22% 84/383 [00:15<00:54,  5.48it/s]\u001b[A\n",
            "Training:  22% 85/383 [00:15<00:55,  5.39it/s]\u001b[A\n",
            "Training:  22% 86/383 [00:16<00:54,  5.48it/s]\u001b[A\n",
            "Training:  23% 87/383 [00:16<00:53,  5.49it/s]\u001b[A\n",
            "Training:  23% 88/383 [00:16<00:53,  5.53it/s]\u001b[ATraceback (most recent call last):\n",
            "  File \"jointtrainer_multilang.py\", line 1603, in <module>\n",
            "    main(args)\n",
            "  File \"jointtrainer_multilang.py\", line 1586, in main\n",
            "    ner_f1, dep_f1, exp_log = jointtrainer.train2()\n",
            "  File \"jointtrainer_multilang.py\", line 1267, in train2\n",
            "    ner_loss, dep_loss = model_func(i, e)\n",
            "  File \"jointtrainer_multilang.py\", line 1118, in dep_update_caller\n",
            "    dep_loss, deprel_loss, depind_loss, acc, uas = self.dep_update(dep_batch)\n",
            "  File \"jointtrainer_multilang.py\", line 1130, in dep_update\n",
            "    dep_loss.backward()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/tensor.py\", line 118, in backward\n",
            "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\", line 93, in backward\n",
            "    allow_unreachable=True)  # allow_unreachable flag\n",
            "KeyboardInterrupt\n",
            "\n",
            "Epoch:   0% 0/20 [00:16<?, ?it/s]\n",
            "Training:  23% 88/383 [00:16<00:55,  5.29it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5j_X70SvPA77",
        "outputId": "68a3d5fa-ec66-41e0-c14f-bb45da01ba73"
      },
      "source": [
        "save_dir=\"../mtl_czech_fastext_1\"\n",
        "# Training DEP model with bert_en for Turkish\n",
        "!python jointtrainer_multilang.py --model_type FLAT  --word_embed_type fastext  --lang cs --save_dir $save_dir"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-31 12:49:05.251035: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "{'data_dir': 'data/depparse', 'data_folder': '../../datasets', 'wordvec_dir': '../word_vecs', 'train_file': None, 'eval_file': None, 'output_file': None, 'gold_file': None, 'ner_result_out_file': 'ner_results', 'ner_test_result_file': 'ner_test_results.txt', 'dep_test_result_file': 'dep_test_results.txt', 'log_file': 'jointtraining.log', 'ner_train_file': '../../datasets/traindev_pos.tsv', 'dep_train_file': '../../datasets/tr_imst-ud-traindev.conllu', 'ner_val_file': '../../datasets/dev_pos.tsv', 'dep_val_file': '../../datasets/tr_imst-ud-train.conllu', 'ner_test_file': '../../datasets/test_pos.tsv', 'dep_test_file': '../../datasets/tr_imst-ud-test.conllu', 'ner_output_file': 'joint_ner_out.txt', 'dep_output_file': 'joint_dep_out.txt', 'conll_file_name': 'conll_ner_output', 'config_file': 'config.json', 'mode': 'train', 'eval_mode': 'BOTH', 'load_config': 0, 'lang': 'cs', 'shorthand': None, 'lstm_hidden': 400, 'char_hidden_dim': 200, 'biaffine_hidden': 400, 'composite_deep_biaff_hidden_dim': 100, 'cap_dim': 32, 'char_emb_dim': 100, 'cap_types': 6, 'pos_dim': 64, 'dep_dim': 128, 'ner_dim': 128, 'transformed_dim': 125, 'word_embed_type': 'fastext', 'fix_embed': False, 'word_only': False, 'lstm_layers': 3, 'char_num_layers': 1, 'pretrain_max_vocab': -1, 'word_drop': 0.3, 'embed_drop': 0.3, 'lstm_drop': 0.3, 'crf_drop': 0.3, 'parser_drop': 0.3, 'rec_dropout': 0.2, 'char_rec_dropout': 0, 'char': True, 'pretrain': True, 'linearization': True, 'distance': True, 'sample_train': 1.0, 'optim': 'adam', 'ner_lr': 0.0015, 'embed_lr': 0.015, 'dep_lr': 0.0015, 'lr_decay': 0.6, 'min_lr': 2e-06, 'beta2': 0.95, 'weight_decay': 0.0005, 'max_steps': None, 'epochs': 20, 'repeat': 1, 'multiple': 0, 'early_stop': 50, 'dep_warmup': -1, 'lr_patience': 3, 'ner_warmup': -1, 'eval_interval': None, 'max_steps_before_stop': 3000, 'batch_size': 300, 'max_grad_norm': 1.0, 'max_depgrad_norm': 5.0, 'log_step': 20, 'save_dir': '../mtl_czech_fastext_1', 'save_name': 'best_joint_model.pkh', 'save_ner_name': 'best_ner_model.pkh', 'save_dep_name': 'best_dep_model.pkh', 'load_model': 0, 'load_path': 'best_joint_model.pkh', 'seed': 1234, 'cuda': True, 'cpu': False, 'inner': 1, 'soft': 1, 'relu': 1, 'model_type': 'FLAT', 'hyper': 0, 'max_evals': 50, 'ner_only': 0, 'dep_only': 0, 'depner': 0, 'nerdep': 0, 'device': device(type='cuda', index=0)}\n",
            "cs   DeepPavlov/bert-base-bg-cs-pl-ru-cased\n",
            "BERT Tokenizer\n",
            "PreTrainedTokenizerFast(name_or_path='DeepPavlov/bert-base-bg-cs-pl-ru-cased', vocab_size=119547, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n",
            "Experiment name FLAT_fastext_cs \n",
            "Reading from ../../datasets/myner_czech-train.txt \n",
            "Number of sentences for ../../datasets/myner_czech-train.txt  : 7142 \n",
            "Dataset size : 7142\n",
            "DEP dataset number of sents : 23449\n",
            "Dataset size before batching 23449 and number of words : 472609\n",
            "4294 batches created for ../../datasets/dep_czech_train.conllu.\n",
            "491588 words inside the batches\n",
            "Reading from ../../datasets/myner_czech-dev.txt \n",
            "Number of sentences for ../../datasets/myner_czech-dev.txt  : 885 \n",
            "Dataset size : 885\n",
            "DEP dataset number of sents : 602\n",
            "Dataset size before batching 602 and number of words : 10912\n",
            "102 batches created for ../../datasets/dep_czech_dev.conllu.\n",
            "11411 words inside the batches\n",
            "Reading from ../../datasets/myner_czech-test.txt \n",
            "Number of sentences for ../../datasets/myner_czech-test.txt  : 890 \n",
            "Dataset size : 890\n",
            "DEP dataset number of sents : 628\n",
            "Dataset size before batching 628 and number of words : 10862\n",
            "102 batches created for ../../datasets/dep_czech_test.conllu.\n",
            "11490 words inside the batches\n",
            "{'[PAD]': 0, '[SOS]': 1, '[EOS]': 2, 'O': 3, 'B-P': 4, 'I-P': 5, 'B-G': 6, 'B-I': 7, 'I-I': 8, 'B-T': 9, 'I-T': 10, 'B-O': 11, 'I-O': 12, 'B-M': 13, 'I-G': 14, 'B-A': 15, 'I-M': 16, 'I-A': 17}\n",
            "{'[PAD]': 0, '[SOS]': 1, '[EOS]': 2, 'O': 3, 'B-G': 4, 'B-I': 5, 'B-P': 6, 'I-I': 7, 'B-O': 8, 'I-O': 9, 'I-P': 10, 'I-G': 11, 'B-T': 12, 'I-T': 13, 'B-M': 14, 'I-M': 15, 'B-A': 16, 'I-A': 17}\n",
            "DEP REL VOCABS\n",
            "{'[PAD]': 0, '[SOS]': 1, '[EOS]': 2, '[UNK]': 3, 'amod': 4, 'nsubj': 5, 'nmod': 6, 'root': 7, 'case': 8, 'obl': 9, 'punct': 10, 'conj': 11, 'cc': 12, 'det': 13, 'advmod': 14, 'obj': 15, 'acl:relcl': 16, 'obl:arg': 17, 'acl': 18, 'advmod:emph': 19, 'cop': 20, 'nummod': 21, 'dep': 22, 'expl:pass': 23, 'nsubj:pass': 24, 'appos': 25, 'parataxis': 26, 'xcomp': 27, 'expl:pv': 28, 'aux:pass': 29, 'mark': 30, 'advcl': 31, 'fixed': 32, 'aux': 33, 'ccomp': 34, 'orphan': 35, 'csubj': 36, 'csubj:pass': 37, 'nummod:gov': 38, 'iobj': 39, 'det:nummod': 40, 'det:numgov': 41, 'flat': 42, 'vocative': 43, 'flat:foreign': 44, 'discourse': 45, 'compound': 46}\n",
            "{'[PAD]': 0, '[SOS]': 1, '[EOS]': 2, '[UNK]': 3, 'amod': 4, 'nsubj': 5, 'nmod': 6, 'root': 7, 'case': 8, 'obl': 9, 'punct': 10, 'conj': 11, 'cc': 12, 'det': 13, 'advmod': 14, 'obj': 15, 'acl:relcl': 16, 'obl:arg': 17, 'acl': 18, 'advmod:emph': 19, 'cop': 20, 'nummod': 21, 'dep': 22, 'expl:pass': 23, 'nsubj:pass': 24, 'appos': 25, 'parataxis': 26, 'xcomp': 27, 'expl:pv': 28, 'aux:pass': 29, 'mark': 30, 'advcl': 31, 'fixed': 32, 'aux': 33, 'ccomp': 34, 'orphan': 35, 'csubj': 36, 'csubj:pass': 37, 'nummod:gov': 38, 'iobj': 39, 'det:nummod': 40, 'det:numgov': 41, 'flat': 42, 'vocative': 43, 'flat:foreign': 44, 'discourse': 45, 'compound': 46}\n",
            "{'[PAD]': 0, '[SOS]': 1, '[EOS]': 2, '[UNK]': 3, 'amod': 4, 'nsubj': 5, 'nmod': 6, 'root': 7, 'case': 8, 'obl': 9, 'punct': 10, 'conj': 11, 'cc': 12, 'det': 13, 'advmod': 14, 'obj': 15, 'acl:relcl': 16, 'obl:arg': 17, 'acl': 18, 'advmod:emph': 19, 'cop': 20, 'nummod': 21, 'dep': 22, 'expl:pass': 23, 'nsubj:pass': 24, 'appos': 25, 'parataxis': 26, 'xcomp': 27, 'expl:pv': 28, 'aux:pass': 29, 'mark': 30, 'advcl': 31, 'fixed': 32, 'aux': 33, 'ccomp': 34, 'orphan': 35, 'csubj': 36, 'csubj:pass': 37, 'nummod:gov': 38, 'iobj': 39, 'det:nummod': 40, 'det:numgov': 41, 'flat': 42, 'vocative': 43, 'flat:foreign': 44, 'discourse': 45, 'compound': 46}\n",
            "Before merging\n",
            "Dependency pos tags \n",
            "{'[PAD]': 0, '[SOS]': 1, '[EOS]': 2, '[UNK]': 3, 'ADJ': 4, 'NOUN': 5, 'VERB': 6, 'ADP': 7, 'PUNCT': 8, 'CCONJ': 9, 'SCONJ': 10, 'DET': 11, 'ADV': 12, 'PRON': 13, 'INTJ': 14, 'AUX': 15, 'NUM': 16, 'SYM': 17, 'PART': 18, 'PROPN': 19}\n",
            "NER pos tags\n",
            "{'[PAD]': 0, '[SOS]': 1, '[EOS]': 2, 'AUX': 3, 'NOUN': 4, 'ADJ': 5, 'CCONJ': 6, 'VERB': 7, 'ADV': 8, 'SCONJ': 9, 'PRON': 10, 'PUNCT': 11, 'ADP': 12, 'PROPN': 13, 'DET': 14, 'PART': 15, 'NUM': 16, 'SYM': 17, 'INTJ': 18}\n",
            "After merging\n",
            "Dependency pos tags \n",
            "{'[PAD]': 0, '[SOS]': 1, '[EOS]': 2, 'AUX': 3, 'NOUN': 4, 'ADJ': 5, 'CCONJ': 6, 'VERB': 7, 'ADV': 8, 'SCONJ': 9, 'PRON': 10, 'PUNCT': 11, 'ADP': 12, 'PROPN': 13, 'DET': 14, 'PART': 15, 'NUM': 16, 'SYM': 17, 'INTJ': 18, '[UNK]': 19}\n",
            "NER pos tags\n",
            "{'[PAD]': 0, '[SOS]': 1, '[EOS]': 2, 'AUX': 3, 'NOUN': 4, 'ADJ': 5, 'CCONJ': 6, 'VERB': 7, 'ADV': 8, 'SCONJ': 9, 'PRON': 10, 'PUNCT': 11, 'ADP': 12, 'PROPN': 13, 'DET': 14, 'PART': 15, 'NUM': 16, 'SYM': 17, 'INTJ': 18, '[UNK]': 19}\n",
            "NER vocab size 97586 \n",
            "DEP vocab size 97586 \n",
            "NER Training pos vocab : {'[PAD]': 0, '[SOS]': 1, '[EOS]': 2, 'AUX': 3, 'NOUN': 4, 'ADJ': 5, 'CCONJ': 6, 'VERB': 7, 'ADV': 8, 'SCONJ': 9, 'PRON': 10, 'PUNCT': 11, 'ADP': 12, 'PROPN': 13, 'DET': 14, 'PART': 15, 'NUM': 16, 'SYM': 17, 'INTJ': 18, '[UNK]': 19}\n",
            "NER Testing  pos vocab : {'[PAD]': 0, '[SOS]': 1, '[EOS]': 2, 'AUX': 3, 'NOUN': 4, 'ADJ': 5, 'CCONJ': 6, 'VERB': 7, 'ADV': 8, 'SCONJ': 9, 'PRON': 10, 'PUNCT': 11, 'ADP': 12, 'PROPN': 13, 'DET': 14, 'PART': 15, 'NUM': 16, 'SYM': 17, 'INTJ': 18, '[UNK]': 19}\n",
            "NER Training vocab size : 97586\n",
            "NER Val vocab size : 97586\n",
            "NER Test vocab size : 97586\n",
            "DEP Training vocab size : 97586\n",
            "DEP Validation vocab size : 97586\n",
            "DEP Test vocab size : 97586\n",
            "DEP Val  0 words not in training set \n",
            "Ner dataset contains 548 batches\n",
            "Dep dataset contains 4294  batches \n",
            "Eval interval is set to 548 \n",
            "Word vocabulary size  : 97586\n",
            "Joint Trainer initialized on cuda:0\n",
            "\n",
            "\n",
            "STARTING NEW TRAINING\n",
            "\n",
            "\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at DeepPavlov/bert-base-bg-cs-pl-ru-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Whole vocab size 97586\n",
            "Getting pretrained embeddings with dim:300\n",
            "Generating embedding from fasttext model (not .txt file)\n",
            "Loading raw fastext vectors\n",
            "Downloading the fasttext model ../word_vecs/cs/cc.cs.300.bin...\n",
            "Downloaded the fasttext model ../word_vecs/cs/cc.cs.300.bin!\n",
            "\n",
            "\n",
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
            "tcmalloc: large alloc 4800004096 bytes == 0x558f91f88000 @  0x7f957cdb2887 0x7f9509e3d163 0x7f9509e2a44f 0x7f9509e2aa61 0x7f9509dec933 0x7f9509e10fec 0x558f1444ac38 0x558f144be63d 0x558f144b8b0e 0x558f1444c02c 0x558f1448cd39 0x558f14489c84 0x558f1444a8e9 0x558f144beade 0x558f1444b69a 0x558f144bde50 0x558f144b8b0e 0x558f1444b77a 0x558f144ba86a 0x558f144b8e0d 0x558f1444be11 0x558f1448cd39 0x558f14489c84 0x558f1444a7f2 0x558f144bdd75 0x558f144b8e0d 0x558f1444be11 0x558f1448cd39 0x558f14489c84 0x558f1444a7f2 0x558f144bdd75\n",
            "tcmalloc: large alloc 2400002048 bytes == 0x5590b092c000 @  0x7f957cdb2887 0x7f9509e3d163 0x7f9509e2a49e 0x7f9509e2aa61 0x7f9509dec933 0x7f9509e10fec 0x558f1444ac38 0x558f144be63d 0x558f144b8b0e 0x558f1444c02c 0x558f1448cd39 0x558f14489c84 0x558f1444a8e9 0x558f144beade 0x558f1444b69a 0x558f144bde50 0x558f144b8b0e 0x558f1444b77a 0x558f144ba86a 0x558f144b8e0d 0x558f1444be11 0x558f1448cd39 0x558f14489c84 0x558f1444a7f2 0x558f144bdd75 0x558f144b8e0d 0x558f1444be11 0x558f1448cd39 0x558f14489c84 0x558f1444a7f2 0x558f144bdd75\n",
            "Traceback (most recent call last):\n",
            "  File \"jointtrainer_multilang.py\", line 1603, in <module>\n",
            "    main(args)\n",
            "  File \"jointtrainer_multilang.py\", line 1583, in main\n",
            "    jointtrainer.init_models()\n",
            "  File \"jointtrainer_multilang.py\", line 815, in init_models\n",
            "    self.jointmodel = JointModel(self.args, self.bert_tokenizer)\n",
            "  File \"jointtrainer_multilang.py\", line 430, in __init__\n",
            "    self.base_model = BaseModel(self.args, tokenizer)\n",
            "  File \"jointtrainer_multilang.py\", line 526, in __init__\n",
            "    load_model=self.args[\"load_model\"] == 1)\n",
            "  File \"jointtrainer_multilang.py\", line 227, in get_pretrained_word_embeddings\n",
            "    subprocess.call(cmd, shell=True)\n",
            "NameError: name 'subprocess' is not defined\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6_olY7APNF4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}